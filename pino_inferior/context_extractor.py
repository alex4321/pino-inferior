# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/08_context_extractor.ipynb.

# %% auto 0
__all__ = ['CONTEXT_PROMPT_DIR', 'CONTEXT_INPUT_TEXT', 'CONTEXT_INPUT_POST_TIME', 'CONTEXT_INPUT_GOALS',
           'CONTEXT_INPUT_CURRENT_TIME', 'CONTEXT_INPUT_USERNAME', 'CONTEXT_INPUT_CHARACTER',
           'CONTEXT_INTERMEDIATE_POST_CUTTEN', 'CONTEXT_INTERMEDIATE_POST_TIME_STR',
           'CONTEXT_INTERMEDIATE_CURRENT_TIME_STR', 'CONTEXT_INTERMEDIATE_LLM_OUTPUT', 'CONTEXT_OUTPUT_CONTEXT',
           'context_system_prompt', 'context_instruction_prompt', 'context_llm_prompt', 'LengthConfig',
           'PromptMarkupConfig', 'build_context_extractor_chain']

# %% ../nbs/08_context_extractor.ipynb 1
import os
from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate
from langchain.chains import TransformChain
from langchain.chat_models.base import BaseChatModel
from langchain.chat_models.openai import ChatOpenAI
from .core import PROMPTS_DIR, OPENAI_API_KEY
from datetime import datetime
from typing import Callable
from langchain.schema import AIMessage
from langchain.schema.runnable import RunnableSequence
import tiktoken
from dataclasses import dataclass

# %% ../nbs/08_context_extractor.ipynb 2
CONTEXT_PROMPT_DIR = os.path.join(PROMPTS_DIR, "context")

# %% ../nbs/08_context_extractor.ipynb 3
CONTEXT_INPUT_TEXT = "text"
CONTEXT_INPUT_POST_TIME = "post_datetime"
CONTEXT_INPUT_GOALS = "goals"
CONTEXT_INPUT_CURRENT_TIME = "current_datetime"
CONTEXT_INPUT_USERNAME = "name"
CONTEXT_INPUT_CHARACTER = "character"

CONTEXT_INTERMEDIATE_POST_CUTTEN = "post_cutten"
CONTEXT_INTERMEDIATE_POST_TIME_STR = "post_datetime_str"
CONTEXT_INTERMEDIATE_CURRENT_TIME_STR = "current_datetime_str"
CONTEXT_INTERMEDIATE_LLM_OUTPUT = "llm_output"

CONTEXT_OUTPUT_CONTEXT = "context"

# %% ../nbs/08_context_extractor.ipynb 4
def _read_file(fname: str) -> str:
    with open(fname, "r", encoding="utf-8") as src:
        return src.read()

# %% ../nbs/08_context_extractor.ipynb 5
context_system_prompt = SystemMessagePromptTemplate.from_template(_read_file(
    os.path.join(CONTEXT_PROMPT_DIR, "system.txt")
))
context_instruction_prompt = HumanMessagePromptTemplate.from_template(_read_file(
    os.path.join(CONTEXT_PROMPT_DIR, "instruction.txt")
))
context_llm_prompt = ChatPromptTemplate.from_messages([context_system_prompt, context_instruction_prompt])

# %% ../nbs/08_context_extractor.ipynb 6
def _build_preprocess_conversion(length_function: Callable[[str], int],
                                 cut_function: Callable[[str, int], int],
                                 max_goals_length: int,
                                 max_name_length: int,
                                 max_character_length: int,
                                 max_post_length: int,) -> Callable[[dict], dict]:
    def _func(row: dict) -> dict:
        post_time: datetime = row[CONTEXT_INPUT_POST_TIME]
        current_time: datetime = row[CONTEXT_INPUT_CURRENT_TIME]
        goals: str = row[CONTEXT_INPUT_GOALS]
        name: str = row[CONTEXT_INPUT_USERNAME]
        character: str = row[CONTEXT_INPUT_CHARACTER]
        assert length_function(goals) <= max_goals_length
        assert length_function(name) <= max_name_length
        assert length_function(character) <= max_character_length
        post: str = row[CONTEXT_INPUT_TEXT]
        if length_function(post) > max_post_length:
            post = cut_function(post, max_post_length)
        return {
            CONTEXT_INTERMEDIATE_CURRENT_TIME_STR: current_time.strftime("%d %b %Y %H:%M"),
            CONTEXT_INTERMEDIATE_POST_TIME_STR: post_time.strftime("%d %b %Y %H:%M"),
            CONTEXT_INTERMEDIATE_POST_CUTTEN: post,
        }
    
    async def _afunc(row: dict) -> dict:
        return _func(row)
    
    return TransformChain(transform=_func,
                          atransform=_afunc,
                          input_variables=[CONTEXT_INPUT_CURRENT_TIME,
                                           CONTEXT_INPUT_POST_TIME,
                                           CONTEXT_INPUT_GOALS,
                                           CONTEXT_INPUT_TEXT],
                          output_variables=[CONTEXT_INTERMEDIATE_POST_TIME_STR,
                                            CONTEXT_INTERMEDIATE_CURRENT_TIME_STR,
                                            CONTEXT_INTERMEDIATE_POST_CUTTEN])

# %% ../nbs/08_context_extractor.ipynb 7
def _build_llm_output_parser(tags_open_marker: str, tags_close_marker: str, 
                             summary_open_marker: str, summary_close_marker: str) -> TransformChain:
    def _func(response: dict) -> dict:
        ai_message: AIMessage = response[CONTEXT_INTERMEDIATE_LLM_OUTPUT]
        tags = None
        if tags_open_marker in ai_message.content and tags_close_marker in ai_message.content:
            tags = ai_message.content.split(tags_open_marker)[-1].split(tags_close_marker)[0]
        summary = ai_message.content.split(summary_open_marker)[-1].split(summary_close_marker)[0]
        return {CONTEXT_OUTPUT_CONTEXT: f"[tags]{tags}[/tags]\n{summary}"}
    
    async def _afunc(response: AIMessage) -> dict:
        return _func(response)
    
    return TransformChain(transform=_func, atransform=_afunc,
                          input_variables=[CONTEXT_INTERMEDIATE_LLM_OUTPUT],
                          output_variables=[CONTEXT_OUTPUT_CONTEXT])

# %% ../nbs/08_context_extractor.ipynb 8
def _build_context_extractor(llm: BaseChatModel,
                             length_function: Callable[[str], int],
                             cut_function: Callable[[str, int], int],
                             max_goals_length: int,
                             max_name_length: int,
                             max_character_length: int,
                             max_post_length: int,
                             tags_open_marker: str,
                             tags_close_marker: str, 
                             summary_open_marker: str,
                             summary_close_marker: str) -> RunnableSequence:
    stringify = _build_preprocess_conversion(
        length_function,
        cut_function,
        max_goals_length,
        max_name_length,
        max_character_length,
        max_post_length,
    )
    output_parser = _build_llm_output_parser(
        tags_open_marker,
        tags_close_marker,
        summary_open_marker,
        summary_close_marker,
    )
    return stringify | context_llm_prompt | llm | output_parser

# %% ../nbs/08_context_extractor.ipynb 9
@dataclass
class LengthConfig:
    cut_function: Callable[[str, int], str]
    length_function: Callable[[str], int]
    max_goals_length: int = 256
    max_name_length: int = 10
    max_character_length: int = 256
    max_post_length: int = 2048


@dataclass
class PromptMarkupConfig:
    tags_open_sequence: str = "[tags]"
    tags_close_sequence: str = "[/tags]"
    summary_open_sequence: str = "[tags]"
    summary_close_sequence: str = "[/tags]"


def build_context_extractor_chain(llm: BaseChatModel, lengths: LengthConfig, prompts: PromptMarkupConfig) -> RunnableSequence:
    return _build_context_extractor(
        llm,
        length_function=lengths.length_function,
        cut_function=lengths.cut_function,
        max_goals_length=lengths.max_goals_length,
        max_name_length=lengths.max_name_length,
        max_character_length=lengths.max_character_length,
        max_post_length=lengths.max_post_length,
        tags_open_marker=prompts.tags_open_sequence,
        tags_close_marker=prompts.tags_close_sequence,
        summary_open_marker=prompts.summary_open_sequence,
        summary_close_marker=prompts.summary_close_sequence,
    )
