# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_fallacies.ipynb.

# %% auto 0
__all__ = ['FALLACIES_FNAME', 'FALLACIES_PROMPT_DIR', 'INPUT_FALLACIES', 'INPUT_HISTORY', 'INPUT_CONTEXT', 'INPUT_QUERY',
           'INTERMEDIATE_FALLACIES_STR', 'INTERMEDIATE_HISTORY_STR', 'INTERMEDIATE_LAST_AUTHOR', 'OUTPUT_LLM_OUTPUT',
           'OUTPUT_SHORT_ANSWER', 'LLM_OUTPUT_MARKER', 'system_prompt', 'instruction_prompt', 'chat_prompt',
           'FallacyExample', 'Fallacy', 'read_fallacies', 'stringify', 'astringify', 'extract_last_user',
           'aextract_last_user', 'extract_answer_from_cot', 'aextract_answer_from_cot', 'build_fallacy_detection_chain']

# %% ../nbs/02_fallacies.ipynb 3
from .core import DATA_DIR, PROMPTS_DIR, OPENAI_API_KEY
from langchain.schema.runnable import RunnableSequence
from langchain.callbacks.base import BaseCallbackHandler
from langchain.chains import LLMChain, SequentialChain
from langchain.llms.openai import BaseLLM
from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.chains.transform import TransformChain
from langchain.schema.messages import AIMessage, AIMessageChunk
import os
from dataclasses import dataclass
from typing import Union, List
import json
from datetime import datetime
from .message import Message

# %% ../nbs/02_fallacies.ipynb 4
FALLACIES_FNAME = os.path.join(DATA_DIR, "fallacies.json")
FALLACIES_PROMPT_DIR = os.path.join(PROMPTS_DIR, "fallacies")

# %% ../nbs/02_fallacies.ipynb 5
INPUT_FALLACIES = "fallacies"
INPUT_HISTORY = "history"
INPUT_CONTEXT = "context"
INPUT_QUERY = "query"

INTERMEDIATE_FALLACIES_STR = "fallacies_str"
INTERMEDIATE_HISTORY_STR = "history_str"
INTERMEDIATE_LAST_AUTHOR = "last_message_author"

OUTPUT_LLM_OUTPUT = "llm_output"
OUTPUT_SHORT_ANSWER = "answer"

LLM_OUTPUT_MARKER = "Therefore"

# %% ../nbs/02_fallacies.ipynb 7
@dataclass
class FallacyExample:
    text: str
    response: str

    def __str__(self) -> str:
        return f"Example: {self.text}\nExample Response: {self.response}"


@dataclass
class Fallacy:
    name: str
    description: str
    example: Union[FallacyExample, None]

    def __str__(self):
        result = f"# {self.name}\n\n{self.description}"
        if self.example:
            result += "\n\n" + str(self.example)
        return result
    

def read_fallacies(fname: str) -> List[Fallacy]:
    with open(fname, "r", encoding="utf-8") as src:
        data = json.load(src)
    result = []
    for item in data:
        if item.get("example"):
            example = FallacyExample(**item["example"])
        else:
            example = None
        fallacy = Fallacy(name=item["name"], description=item["description"], example=example)
        result.append(fallacy)
    return result

# %% ../nbs/02_fallacies.ipynb 9
system_prompt = SystemMessagePromptTemplate.from_template_file(
    os.path.join(FALLACIES_PROMPT_DIR, "system.txt"),
    input_variables=[]
)
instruction_prompt = HumanMessagePromptTemplate.from_template_file(
    os.path.join(FALLACIES_PROMPT_DIR, "instruction.txt"),
    input_variables=[INTERMEDIATE_FALLACIES_STR,
                     INTERMEDIATE_HISTORY_STR,
                     INTERMEDIATE_LAST_AUTHOR,
                     INPUT_CONTEXT,
                     INPUT_QUERY]
)
chat_prompt = ChatPromptTemplate.from_messages([system_prompt, instruction_prompt])

# %% ../nbs/02_fallacies.ipynb 14
def stringify(row):
    fallacies: List[Fallacy] = row[INPUT_FALLACIES]
    history: List[Message] = row[INPUT_HISTORY] # TODO: cut
    return {
        INTERMEDIATE_FALLACIES_STR: "\n\n".join(map(str, fallacies)),
        INTERMEDIATE_HISTORY_STR: "\n\n".join(map(str, history))
    }

async def astringify(row):
    return stringify(row)

# %% ../nbs/02_fallacies.ipynb 17
def extract_last_user(row):
    history: List[Message] = row[INPUT_HISTORY]
    assert len(history) > 0
    return {
        INTERMEDIATE_LAST_AUTHOR: history[-1].author
    }

async def aextract_last_user(row):
    return extract_last_user(row)

# %% ../nbs/02_fallacies.ipynb 20
def extract_answer_from_cot(row):
    response: Union[AIMessage, AIMessageChunk] = row[OUTPUT_LLM_OUTPUT]
    text: str = response.content
    text = text.split(LLM_OUTPUT_MARKER)[-1]
    text = text.split(":", maxsplit=1)[-1]
    text = text.strip()
    return {
        OUTPUT_SHORT_ANSWER: text
    }

async def aextract_answer_from_cot(row):
    return extract_answer_from_cot(row)

# %% ../nbs/02_fallacies.ipynb 22
def build_fallacy_detection_chain(llm: BaseLLM) -> RunnableSequence:
    stringify_transform = TransformChain(
        input_variables=[INPUT_FALLACIES, INPUT_HISTORY],
        output_variables=[INTERMEDIATE_FALLACIES_STR, INTERMEDIATE_HISTORY_STR],
        transform=stringify,
        atransform=astringify,
    )
    extract_last_user_transform = TransformChain(
        input_variables=[INPUT_HISTORY],
        output_variables=[INTERMEDIATE_LAST_AUTHOR],
        transform=extract_last_user,
        atransform=aextract_last_user,
    )
    extract_answer_transform = TransformChain(
        input_variables=[OUTPUT_LLM_OUTPUT],
        output_variables=[OUTPUT_SHORT_ANSWER],
        transform=extract_answer_from_cot,
        atransform=aextract_answer_from_cot,
    )
    return stringify_transform | \
        extract_last_user_transform | \
        chat_prompt | \
        llm | \
        extract_answer_transform
