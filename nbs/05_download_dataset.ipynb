{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pino_inferior.core import DATA_DIR\n",
    "from sqlalchemy import Column, String, Text, DateTime\n",
    "from datapipe.compute import Catalog, Table, Pipeline, run_pipeline\n",
    "from datapipe.step.batch_transform import BatchTransform\n",
    "from datapipe.datatable import DataStore\n",
    "from datapipe.store.database import DBConn, TableStoreDB\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from goose3 import Goose\n",
    "from datetime import datetime\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from pino_inferior.core import OPENAI_API_KEY, VECTOR_DB, VECTOR_DB_PARAMS, MEMORY_PARAMS\n",
    "from pino_inferior.models import aengine\n",
    "from pino_inferior.memory import Memory, SequentialSplitter, ParagraphSplitter, MarkdownHeaderTextSplitter\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\Projects\\\\pino-inferior\\\\pino_inferior\\\\data\\\\05_dump_data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXPERIMENT_DATA_DIRECTORY = os.path.join(DATA_DIR, \"05_dump_data\")\n",
    "EXPERIMENT_DATA_DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://zona.media/news/2023/10/20/msk-spb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://zona.media/news/2023/10/20/prekratili-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://zona.media/news/2023/10/20/prigovor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://zona.media/news/2023/10/20/eu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://zona.media/news/2023/10/20/openart</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link\n",
       "0         https://zona.media/news/2023/10/20/msk-spb\n",
       "1  https://zona.media/news/2023/10/20/prekratili-...\n",
       "2        https://zona.media/news/2023/10/20/prigovor\n",
       "3              https://zona.media/news/2023/10/20/eu\n",
       "4         https://zona.media/news/2023/10/20/openart"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _get_links():\n",
    "    link_files = [item for item in os.listdir(EXPERIMENT_DATA_DIRECTORY) if item.lower().endswith(\".json\")]\n",
    "    links = []\n",
    "    for item in link_files:\n",
    "        with open(os.path.join(EXPERIMENT_DATA_DIRECTORY, item), 'r') as src:\n",
    "            links += json.load(src)\n",
    "    return pd.DataFrame({\"link\": links})\n",
    "\n",
    "\n",
    "df_links = _get_links()\n",
    "df_links.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datapipe.store.database.DBConn at 0x1b80a3999d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_FNAME = os.path.join(EXPERIMENT_DATA_DIRECTORY, 'experiment.sqlite3')\n",
    "\n",
    "dbconn = DBConn(f\"sqlite:///{DB_FNAME}\")\n",
    "dbconn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = Catalog({\n",
    "    \"links\": Table(\n",
    "        TableStoreDB(\n",
    "            dbconn=dbconn,\n",
    "            name=\"links\",\n",
    "            create_table=True,\n",
    "            data_sql_schema=[\n",
    "                Column(\"link\", String(2048), primary_key=True)\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "    \"html\": Table(\n",
    "        TableStoreDB(\n",
    "            dbconn=dbconn,\n",
    "            name=\"html\",\n",
    "            create_table=True,\n",
    "            data_sql_schema=[\n",
    "                Column(\"link\", String(2048), primary_key=True),\n",
    "                Column(\"html\", Text, primary_key=True),\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "    \"parsed\": Table(\n",
    "        TableStoreDB(\n",
    "            dbconn=dbconn,\n",
    "            name=\"parsed\",\n",
    "            create_table=True,\n",
    "            data_sql_schema=[\n",
    "                Column(\"link\", String(2048), primary_key=True),\n",
    "                Column(\"title\", String(2048)),\n",
    "                Column(\"meta_description\", String(2048)),\n",
    "                Column(\"datetime_string\", String(128)),\n",
    "                Column(\"text\", Text),\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "    \"parsed_datetime\": Table(\n",
    "        TableStoreDB(\n",
    "            dbconn=dbconn,\n",
    "            name=\"parsed_datetime\",\n",
    "            create_table=True,\n",
    "            data_sql_schema=[\n",
    "                Column(\"link\", String(2048), primary_key=True),\n",
    "                Column(\"datetime\", DateTime),\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "    \"saved_documents\": Table(\n",
    "        TableStoreDB(\n",
    "            dbconn=dbconn,\n",
    "            name=\"saved_documents\",\n",
    "            create_table=True,\n",
    "            data_sql_schema=[\n",
    "                Column(\"link\", String(2048), primary_key=True),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DataStore(dbconn, create_meta_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.init_all_tables(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch_size = 1000\n",
    "input_batch_count = int(math.ceil(df_links.shape[0] / input_batch_size))\n",
    "for i in range(input_batch_count):\n",
    "    catalog.get_datatable(ds, \"links\").store_chunk(\n",
    "        df_links.iloc[i * input_batch_size : (i + 1) * input_batch_size]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEOUT = 1.0\n",
    "\n",
    "\n",
    "def _read_link(url: str) -> str:\n",
    "    response = requests.get(url)\n",
    "    assert response.status_code == 200\n",
    "    return response.content.decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def read_links(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        link = row[\"link\"]\n",
    "        try:\n",
    "            html = _read_link(link)\n",
    "            records.append({\"link\": link, \"html\": html})\n",
    "        finally:\n",
    "            time.sleep(TIMEOUT)\n",
    "    return pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    goose = Goose()\n",
    "    result = []\n",
    "    for _, row in df.iterrows():\n",
    "        link = row[\"link\"]\n",
    "        html = row[\"html\"]\n",
    "        try:\n",
    "            article = goose.extract(raw_html=html)\n",
    "            record = {\n",
    "                \"link\": link,\n",
    "                \"title\": article.title,\n",
    "                \"meta_description\": article.meta_description,\n",
    "                \"datetime_string\": article.publish_date,\n",
    "                \"text\": article.cleaned_text,\n",
    "            }\n",
    "            result.append(record)\n",
    "        except:\n",
    "            pass\n",
    "    df = pd.DataFrame.from_records(result)\n",
    "    df[\"datetime_string\"] = df[\"datetime_string\"].fillna(\n",
    "        str(datetime.now()),\n",
    "    )\n",
    "    return df\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing date / time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATETIME_MONTHS_CONVERSION = {\n",
    "    \"январ.*\\s\": \"january \",\n",
    "    \"феврал.*\\s\": \"february \",\n",
    "    \"март.*\\s\": \"march \",\n",
    "    \"апрел.*\\s\": \"april \",\n",
    "    \"май\\s\": \"may \",\n",
    "    \"июн.*\\s\": \"june \",\n",
    "    \"июл.*\\s\": \"july \",\n",
    "    \"август.*\\s\": \"august \",\n",
    "    \"сентябр.*\\s\": \"september \",\n",
    "    \"октябр.*\\s\": \"october \",\n",
    "    \"ноябр.*\\s\": \"november \",\n",
    "    \"декабр.*\\s\": \"december \",\n",
    "}\n",
    "\n",
    "\n",
    "def convert_datetime(df):\n",
    "    df = df.copy()\n",
    "    for key, value in DATETIME_MONTHS_CONVERSION.items():\n",
    "        df[\"datetime_string\"] = df[\"datetime_string\"].str.replace(key, value, regex=True)\n",
    "    datetimes = []\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            datetimes.append(pd.to_datetime(row[\"datetime_string\"]))\n",
    "        except:\n",
    "            datetimes.append(datetime.now())\n",
    "    df[\"datetime\"] = datetimes\n",
    "    return df[[\"link\", \"datetime\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving document to retriever database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoder = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model=\"text-embedding-ada-002\",\n",
    ")\n",
    "memory = Memory(\n",
    "    engine=aengine,\n",
    "    vector_db=VECTOR_DB(\n",
    "        embedding_function=OpenAIEmbeddings(\n",
    "            openai_api_key=OPENAI_API_KEY,\n",
    "            model=\"text-embedding-ada-002\",\n",
    "        ),\n",
    "        **VECTOR_DB_PARAMS\n",
    "    ),\n",
    "    **MEMORY_PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_documents(titles: List[str],\n",
    "                   links: List[str],\n",
    "                   publication_datetimes: List[datetime],\n",
    "                   meta_descriptions: List[str],\n",
    "                   texts: List[str],\n",
    "                   memory: Memory):\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header1\"),\n",
    "        (\"##\", \"Header2\"),\n",
    "        (\"###\", \"Header3\"),\n",
    "    ]\n",
    "    paragraph_splitter = SequentialSplitter(\n",
    "        [\n",
    "            MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on),\n",
    "            ParagraphSplitter(),\n",
    "        ]\n",
    "    )\n",
    "    paragraphs = []\n",
    "    for title, link, publication_datetime, meta_description, text in zip(titles,\n",
    "                                                                         links,\n",
    "                                                                         publication_datetimes,\n",
    "                                                                         meta_descriptions,\n",
    "                                                                         texts):\n",
    "        md = f\"# {title}\\n\\n{meta_description}\\n\\n{text}\"\n",
    "        paragraphs += paragraph_splitter.split_text(md)\n",
    "    memory.store(paragraphs)\n",
    "\n",
    "\n",
    "def build_add_document_conversion(memory: Memory):\n",
    "    def _func(df_source: pd.DataFrame, df_parsed_datetime: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df_source.merge(df_parsed_datetime, left_on=[\"link\"], right_on=[\"link\"])\n",
    "        _add_documents(\n",
    "            titles=df[\"title\"].tolist(),\n",
    "            links=df[\"link\"].tolist(),\n",
    "            publication_datetimes=df[\"datetime\"].tolist(),\n",
    "            meta_descriptions=df[\"meta_description\"].tolist(),\n",
    "            texts=df[\"text\"].tolist(),\n",
    "            memory=memory,\n",
    "        )\n",
    "        return df[[\"link\"]]\n",
    "    \n",
    "    return _func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    BatchTransform(\n",
    "        read_links,\n",
    "        inputs=[\"links\"],\n",
    "        outputs=[\"html\"],\n",
    "        chunk_size=1,\n",
    "    ),\n",
    "    BatchTransform(\n",
    "        parse_page,\n",
    "        inputs=[\"html\"],\n",
    "        outputs=[\"parsed\"],\n",
    "        chunk_size=1,\n",
    "    ),\n",
    "    BatchTransform(\n",
    "        convert_datetime,\n",
    "        inputs=[\"parsed\"],\n",
    "        outputs=[\"parsed_datetime\"],\n",
    "        chunk_size=1,\n",
    "    ),\n",
    "    BatchTransform(\n",
    "        build_add_document_conversion(memory),\n",
    "        inputs=[\"parsed\", \"parsed_datetime\"],\n",
    "        outputs=[\"saved_documents\"],\n",
    "        chunk_size=10,\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_pipeline(ds, catalog, pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
