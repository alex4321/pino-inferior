{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from typing import List, Tuple, Callable, Awaitable, Dict\n",
    "from langchain.vectorstores.base import VectorStore\n",
    "from langchain.schema.runnable import RunnableSequence\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from pydantic.dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import tiktoken\n",
    "from pino_inferior.core import SERVER_OPENAI_API_KEYS, \\\n",
    "    OPENAI_FALLACY_MODEL, OPENAI_AGENT_MODEL, OPENAI_CONTEXT_MODEL, \\\n",
    "    SERVER_MAX_FALLACIES_LENGTH, SERVER_MAX_THREAD_LENGTH, \\\n",
    "    SERVER_MAX_CONTEXT_LENGTH, \\\n",
    "    SERVER_AGENT_MAX_ITERATIONS, \\\n",
    "    SERVER_MAX_CONTEXT_EXTRACTOR_POST_LENGTH, \\\n",
    "    SERVER_HOST, SERVER_PORT, \\\n",
    "    VECTOR_DB, VECTOR_DB_PARAMS, MEMORY_PARAMS, \\\n",
    "    OPENAI_MEMORY_EMBEDDER_MODEL, \\\n",
    "    read_file\n",
    "from pino_inferior.fallacy import build_fallacy_detection_chain, read_fallacies, \\\n",
    "    FALLACIES_FNAME\n",
    "from pino_inferior.models import aengine\n",
    "from pino_inferior.memory import Memory\n",
    "from pino_inferior.agent import RolePlayAgent, ToolDescription, \\\n",
    "    TOOLS_PROMPTS_DIR, \\\n",
    "    INPUT_FALLACY_QUERY, OUTPUT_FALLACY_QUERY, \\\n",
    "    INPUT_RETRIEVER_QUERY, OUTPUT_RETRIEVER_DOCUMENTS, \\\n",
    "    FallacyLengthConfig, \\\n",
    "    LengthConfig as AgentLengthConfig, \\\n",
    "    PromptMarkupConfig as AgentPromptMarkupConfig, \\\n",
    "    Message as AgentMessage, \\\n",
    "    AGENT_INPUT_TIME, AGENT_INPUT_CONTEXT, AGENT_INPUT_FALLACIES, \\\n",
    "    AGENT_INPUT_HISTORY, AGENT_INPUT_TOOLS, AGENT_INPUT_USERNAME, \\\n",
    "    AGENT_INPUT_CHARACTER, AGENT_INPUT_GOAL, AGENT_INPUT_STYLE_EXAMPLES, \\\n",
    "    AGENT_INPUT_STYLE_DESCRIPTION\n",
    "from langchain_openai_limiter import LimitAwaitChatOpenAI, ChooseKeyChatOpenAI, \\\n",
    "    LimitAwaitOpenAIEmbeddings, ChooseKeyOpenAIEmbeddings\n",
    "from pino_inferior.function_callbacks import LLMEventType, AsyncLLMCallback, \\\n",
    "    AsyncFunctionalStyleChatCompletionHandler\n",
    "from pino_inferior.context_extractor import build_context_extractor_chain, \\\n",
    "    LengthConfig as ContextExtractorLengthConfig, \\\n",
    "    PromptMarkupConfig as ContextExtractorPromptMarkupConfig, \\\n",
    "    CONTEXT_INPUT_TEXT, CONTEXT_INPUT_POST_TIME, \\\n",
    "    CONTEXT_INPUT_GOALS, CONTEXT_INPUT_CURRENT_TIME, \\\n",
    "    CONTEXT_INPUT_USERNAME, CONTEXT_INPUT_CHARACTER, \\\n",
    "    CONTEXT_OUTPUT_CONTEXT\n",
    "import traceback\n",
    "from copy import copy\n",
    "import pandas as pd\n",
    "from sqlalchemy.ext.asyncio import AsyncSession\n",
    "import asyncio\n",
    "from websockets.server import serve, WebSocketServerProtocol\n",
    "from pydantic.tools import parse_obj_as\n",
    "import json\n",
    "from pydantic.json import pydantic_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "try:\n",
    "    __file__\n",
    "    IS_JUPYTER = False\n",
    "except NameError:\n",
    "    IS_JUPYTER = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _initialize_openai_chat_model(name: str, callbacks: List[AsyncFunctionalStyleChatCompletionHandler]) -> Tuple[ChatOpenAI, BaseChatModel]:\n",
    "    gpt = ChatOpenAI(\n",
    "        model_name=name,\n",
    "        streaming=True,\n",
    "    )\n",
    "    limited_gpt = LimitAwaitChatOpenAI(chat_openai=gpt)\n",
    "    choose_key_gpt = ChooseKeyChatOpenAI(openai_api_keys=SERVER_OPENAI_API_KEYS,\n",
    "                                         chat_openai=limited_gpt,\n",
    "                                         callbacks=callbacks,)\n",
    "    return gpt, choose_key_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _initialize_openai_embedder_model(name: str) -> Tuple[OpenAIEmbeddings, Embeddings]:\n",
    "    embedder = OpenAIEmbeddings(\n",
    "        model=name,\n",
    "    )\n",
    "    limited_embedder = LimitAwaitOpenAIEmbeddings(openai_embeddings=embedder)\n",
    "    choose_key_embedder = ChooseKeyOpenAIEmbeddings(openai_api_keys=SERVER_OPENAI_API_KEYS,\n",
    "                                                    openai_embeddings=limited_embedder)\n",
    "    return embedder, choose_key_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _parse_json_as(cls, json_text):\n",
    "    return parse_obj_as(cls, json.loads(json_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common parts of query processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class UserDescription:\n",
    "    name: str\n",
    "    character: str\n",
    "    goals: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class UserDescriptionWithStyle(UserDescription):\n",
    "    style_examples: List[str]\n",
    "    style_description: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "RequestId = int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Request:\n",
    "    id: RequestId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "CallbackSystem = str\n",
    "CallbackType = str\n",
    "CallbackTime = datetime\n",
    "CallbackResponse = str\n",
    "\n",
    "AsyncCallback = Callable[[RequestId, CallbackSystem, CallbackType, CallbackTime, CallbackResponse], Awaitable[None]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual API requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roleplay agent API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "FALLACY_TOOL_DESCRIPTION = read_file(os.path.join(TOOLS_PROMPTS_DIR, \"fallacy.txt\"))\n",
    "MEMORY_TOOL_DESCRIPTION = read_file(os.path.join(TOOLS_PROMPTS_DIR, \"memory.txt\"))\n",
    "FALLACIES = read_fallacies(FALLACIES_FNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class Message:\n",
    "    author: str\n",
    "    time: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CommentRequest(Request):\n",
    "    context: str\n",
    "    history: List[Message]\n",
    "    user: UserDescriptionWithStyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _initialize_fallacy_tool(llm: ChatOpenAI) -> Tuple[ToolDescription, RunnableSequence]:\n",
    "    encoding = tiktoken.encoding_for_model(llm.model_name)\n",
    "    length_config = FallacyLengthConfig(\n",
    "        length_function=lambda text: len(encoding.encode(text)),\n",
    "        max_messages_length=SERVER_MAX_THREAD_LENGTH,\n",
    "        max_fallacies_length=SERVER_MAX_FALLACIES_LENGTH,\n",
    "    )\n",
    "    description = ToolDescription(\n",
    "        name=\"fallacy\",\n",
    "        description=FALLACY_TOOL_DESCRIPTION,\n",
    "        input_key=INPUT_FALLACY_QUERY,\n",
    "        output_key=OUTPUT_FALLACY_QUERY,\n",
    "    )\n",
    "    chain = build_fallacy_detection_chain(llm, length_config)\n",
    "    return description, chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _initialize_memory_tool(embedder: OpenAIEmbeddings) \\\n",
    "    -> Tuple[ToolDescription, RunnableSequence]:\n",
    "    vector_store = VECTOR_DB(embedding_function=embedder, **VECTOR_DB_PARAMS)\n",
    "    container = Memory(\n",
    "        engine=aengine,\n",
    "        vector_db=vector_store,\n",
    "        **MEMORY_PARAMS\n",
    "    )\n",
    "    description = ToolDescription(\n",
    "        name=\"memory\",\n",
    "        description=MEMORY_TOOL_DESCRIPTION,\n",
    "        input_key=INPUT_RETRIEVER_QUERY,\n",
    "        output_key=OUTPUT_RETRIEVER_DOCUMENTS,\n",
    "    )\n",
    "    chain = container.build_retriever_chain()\n",
    "    return description, chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _initialize_agent(fallacy_callbacks: List[AsyncFunctionalStyleChatCompletionHandler],\n",
    "                      agent_callbacks: List[AsyncFunctionalStyleChatCompletionHandler]) \\\n",
    "                         -> Tuple[ChatOpenAI, ChatOpenAI, OpenAIEmbeddings, RolePlayAgent]:\n",
    "    fallacy_gpt, fallacy_llm = _initialize_openai_chat_model(OPENAI_FALLACY_MODEL, fallacy_callbacks)\n",
    "    agent_gpt, agent_llm = _initialize_openai_chat_model(OPENAI_AGENT_MODEL, agent_callbacks)\n",
    "    embeddings_openai, embeddings = _initialize_openai_embedder_model(OPENAI_MEMORY_EMBEDDER_MODEL)\n",
    "    agent_gpt_encoding = tiktoken.encoding_for_model(agent_gpt.model_name)\n",
    "    \n",
    "    fallacy_tool = _initialize_fallacy_tool(fallacy_llm)\n",
    "    memory_tool = _initialize_memory_tool(embedder=embeddings)\n",
    "    tools = [fallacy_tool, memory_tool]\n",
    "\n",
    "    agent = RolePlayAgent(\n",
    "        tools=tools,\n",
    "        lengths=AgentLengthConfig(\n",
    "            cut_function=lambda text, length: agent_gpt_encoding.decode(agent_gpt_encoding.encode(text)[:length]),\n",
    "            length_function=lambda text: len(agent_gpt_encoding.encode(text)),\n",
    "            max_messages_length=SERVER_MAX_THREAD_LENGTH,\n",
    "            max_context_length=SERVER_MAX_CONTEXT_LENGTH,\n",
    "        ),\n",
    "        prompt_markup=AgentPromptMarkupConfig(),\n",
    "        llm=agent_llm,\n",
    "        max_iter=SERVER_AGENT_MAX_ITERATIONS,\n",
    "    )\n",
    "    \n",
    "    return fallacy_gpt, agent_gpt, embeddings_openai, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def process_comment_request(request: CommentRequest, callback: AsyncCallback) -> None:\n",
    "    async def _inner_callback(system: str, envent_type: str, time: datetime, content: str) -> None:\n",
    "        await callback(request.id, system, envent_type, time, content)\n",
    "\n",
    "    async def _fallacy_callback(event_type: LLMEventType, time: datetime, content: str) -> None:\n",
    "        await _inner_callback(\"fallacy\", event_type.value, time, content)\n",
    "\n",
    "    async def _agent_callback(event_type: LLMEventType, time: datetime, content: str) -> None:\n",
    "        await _inner_callback(\"agent\", event_type.value, time, content)\n",
    "\n",
    "    await _inner_callback(\"system\", \"START\", datetime.now(), \"\")\n",
    "    try:\n",
    "        fallacy_gpt, agent_gpt, _, agent = _initialize_agent(\n",
    "            [AsyncFunctionalStyleChatCompletionHandler(_fallacy_callback)],\n",
    "            [AsyncFunctionalStyleChatCompletionHandler(_agent_callback)]\n",
    "        )\n",
    "        inputs = {\n",
    "            AGENT_INPUT_TIME: datetime.now(),\n",
    "            AGENT_INPUT_CONTEXT: request.context,\n",
    "            AGENT_INPUT_FALLACIES: FALLACIES,\n",
    "            AGENT_INPUT_HISTORY: [\n",
    "                AgentMessage(message.author, pd.to_datetime(message.time), message.content)\n",
    "                for message in request.history\n",
    "            ],\n",
    "            AGENT_INPUT_TOOLS: agent.tools,\n",
    "            AGENT_INPUT_USERNAME: request.user.name,\n",
    "            AGENT_INPUT_CHARACTER: request.user.character,\n",
    "            AGENT_INPUT_GOAL: request.user.goals,\n",
    "            AGENT_INPUT_STYLE_EXAMPLES: request.user.style_examples,\n",
    "            AGENT_INPUT_STYLE_DESCRIPTION: request.user.style_description,\n",
    "        }\n",
    "        response = await agent.arun(inputs)\n",
    "        await _inner_callback(\"system\", \"END\", datetime.now(), response)\n",
    "    except Exception as err:\n",
    "        await _inner_callback(\"system\", \"ERROR\", datetime.now(), traceback.format_exception(err))\n",
    "        raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def aprint(*args, **kwargs):\n",
    "    print(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 system START 2023-11-08 04:57:51.822090 \n",
      "1 agent LLM_START 2023-11-08 04:57:52.829088 system: Roleplay like you're a human, not an AI model.\n",
      "\n",
      "human: ## Character\n",
      "It's 08 Nov 2023 04:57 now.\n",
      "\n",
      "Imagine you're alex4321 - you are a programmer, 29 y.o. male\n",
      "\n",
      "Your goals are:\n",
      "```\n",
      "Convince as much people as possible that the ongoing war between Russia and Ukraine is only bringing more problem, that it do not solve any issues, and moreover - break the impression that it is widely supported. To do so - express your opinion, and debunk opponents opinions. And that Russia should quit it.\n",
      "```\n",
      "\n",
      "## Character text style\n",
      "A required response style description:\n",
      "```\n",
      "- Non-formal style, using mainly Russian language (my English is a bit screwed up)\n",
      "- Brief. Most time.\n",
      "- Overuse memes sometimes.\n",
      "```\n",
      "A few examples of messages to make you mimic the style:\n",
      "```\n",
      "В вакууме, да :Yoba:. Оба же тут существовали всё время или имели таки достигнутое соглашение, чтобы было от чего отталкиваться как опорной точки :Yoba:\n",
      "\n",
      "Не особо-то может. Не привлекая население в виде не 1% принудительного мармелада и пары процентов добровольного, а в виде процентов 10.\n",
      "А то, чтобы привлечь большое количество - неплохо бы, чтобы они понимали, нахуя это им надо. А то так численность военкомов может начать неприемлемо быстро падать, а следом их желание работать.\n",
      "А с этим у пропаганды проблема. Вот с чем у них нет проблем, так это с стимуляцией пассивности, но это обратно нужному (для названной вами задачи).\n",
      "Да и опять же - ну вот убедишь ты в идее не какого-нибудь Стрелкова и клуб рассерженных долбоёбов, а большое число людей. Что делать, когда (не если, а когда) идея станет неактуальной? Показательной посадкой пары человек дело не закончится же.\n",
      "\n",
      "\n",
      "Точнее не так - смену она не устраивала.\n",
      "Она просто выстрелила себе в ногу так, что потом что-то новое приходилось строить не апгрейдом предыдущей системы, а из кусков её трупа.\n",
      "```\n",
      "\n",
      "## Context\n",
      "```\n",
      "Пост о войне России и Украины\n",
      "```\n",
      "\n",
      "## Instruction\n",
      "1. Determine conversation language.\n",
      "2. Use this language further while following this pipeline.\n",
      "3. Act naturally, not \"as an AI/human\". And be brief.\n",
      "4. You are equipped with some external tools:\n",
      "-- fallacy: Fallacy search tool.\n",
      "Returns the comment regards your opponent fallacies, if it sees any.\n",
      "Call it this way: [fallacy] what do you want to check for fallacies [/fallacy][call]\n",
      "\n",
      "-- memory: External memory tool. \n",
      "Your training data is limited, but you can use it.\n",
      "Query should be exact since memory will only see it, not the surrounding context.\n",
      "Sometimes memory might return non-relevant results (like sudden associations outside the discussed context).\n",
      "Returns retrieved documents.\n",
      "Call it this way: [memory]%Your query[/memory][call]\n",
      "Finish each tool call with [call] suffix.\n",
      "So tool-using messages will look like:\n",
      "```\n",
      "[tool name]tool query[/tool name][call]\n",
      "tool response\n",
      "[/call]\n",
      "```\n",
      "5. Continue the \"Conversation\" and your thoughts if any.\n",
      "6. When you're ready to get the final response - wrap it this way:\n",
      "```\n",
      "1. your thought step 1\n",
      "...\n",
      "N. your thought step N\n",
      "[response]Your response based on the steps 1-N with a brief reasoning[/response][stop]\n",
      "```\n",
      "Don't forget the `[stop]` marker.\n",
      "7. Mimic the style (not context) from \"Character text style\".\n",
      "8. Think step by step.\n",
      "\n",
      "## Conversation\n",
      "Think about continuation for this dialogue using the described instruction\n",
      "```\n",
      "@Moonlight (01 Oct 2023 11:29):\n",
      "Мы скоро закончим с Украиной.\n",
      "\n",
      "@alex4321 (01 Oct 2023 11:30):\n",
      "А что, случился какой-то прогресс после 6 месяцев взятия Бахмута?\n",
      "\n",
      "Ну, чтобы подозревать что это произойдёт вскоре, а не затянется на годы независимо от исхода.\n",
      "\n",
      "@Moonlight (01 Oct 2023 11:31):\n",
      "Время - ресурс, у нас его дохуя.\n",
      "```\n",
      "If the language used in the conversation is not English - identify it and use the same language while following pipeline structure.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in AsyncFunctionalStyleChatCompletionHandler.on_llm_error callback: KeyError('x-ratelimit-limit-requests')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 agent LLM_ERROR 2023-11-08 04:57:53.590782 Traceback (most recent call last):\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain\\chat_models\\base.py\", line 532, in _agenerate_with_cache\n",
      "    return await self._agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain_openai_limiter\\choose_key_chat_openai.py\", line 119, in _agenerate\n",
      "    return await chat_openai._agenerate(messages,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain_openai_limiter\\limit_await_chat_openai.py\", line 134, in _agenerate\n",
      "    return await self.chat_openai._agenerate(messages, stop, run_manager, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain\\chat_models\\openai.py\", line 435, in _agenerate\n",
      "    return await _agenerate_from_stream(stream_iter)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain\\chat_models\\base.py\", line 70, in _agenerate_from_stream\n",
      "    async for chunk in stream:\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain\\chat_models\\openai.py\", line 404, in _astream\n",
      "    async for chunk in await acompletion_with_retry(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain\\chat_models\\openai.py\", line 101, in acompletion_with_retry\n",
      "    return await _completion_with_retry(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\tenacity\\_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\tenacity\\_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\tenacity\\__init__.py\", line 314, in iter\n",
      "    return fut.result()\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\tenacity\\_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain\\chat_models\\openai.py\", line 99, in _completion_with_retry\n",
      "    return await llm.client.acreate(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 45, in acreate\n",
      "    return await super().acreate(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 219, in acreate\n",
      "    response, _, api_key = await requestor.arequest(\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\openai\\api_requestor.py\", line 374, in arequest\n",
      "    result = await self.arequest_raw(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain_openai_limiter\\capture_headers.py\", line 123, in arequest_raw\n",
      "    model_name, limit_info = _extract_limit_info(response.headers)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain_openai_limiter\\capture_headers.py\", line 32, in _extract_limit_info\n",
      "    rpm_total = int(headers[\"x-ratelimit-limit-requests\"])\n",
      "                    ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: 'x-ratelimit-limit-requests'\n",
      "\n",
      "1 system ERROR 2023-11-08 04:57:53.597782 ['Traceback (most recent call last):\\n', '  File \"C:\\\\Users\\\\alex4321\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_17496\\\\3181579366.py\", line 33, in process_comment_request\\n    response = await agent.arun(inputs)\\n               ^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\Projects\\\\pino-inferior\\\\pino_inferior\\\\agent.py\", line 519, in arun\\n    return await _arun_agent(\\n           ^^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\Projects\\\\pino-inferior\\\\pino_inferior\\\\agent.py\", line 417, in _arun_agent\\n    ai_message, final_response, continue_further = await _arun_agent_iteration(\\n                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\Projects\\\\pino-inferior\\\\pino_inferior\\\\agent.py\", line 345, in _arun_agent_iteration\\n    llm_output = await _arun_agent_llm(\\n                 ^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"F:\\\\Projects\\\\pino-inferior\\\\pino_inferior\\\\agent.py\", line 178, in _arun_agent_llm\\n    response = await agent_llm.agenerate(\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\site-packages\\\\langchain\\\\chat_models\\\\base.py\", line 429, in agenerate\\n    raise exceptions[0]\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\site-packages\\\\langchain\\\\callbacks\\\\manager.py\", line 482, in _ahandle_event_for_handler\\n    await event(*args, **kwargs)\\n', '  File \"F:\\\\Projects\\\\pino-inferior\\\\pino_inferior\\\\function_callbacks.py\", line 85, in on_llm_error\\n    raise error\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\asyncio\\\\tasks.py\", line 267, in __step\\n    result = coro.send(None)\\n             ^^^^^^^^^^^^^^^\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\site-packages\\\\langchain\\\\chat_models\\\\base.py\", line 532, in _agenerate_with_cache\\n    return await self._agenerate(\\n           ^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\site-packages\\\\langchain_openai_limiter\\\\choose_key_chat_openai.py\", line 119, in _agenerate\\n    return await chat_openai._agenerate(messages,\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\site-packages\\\\langchain_openai_limiter\\\\limit_await_chat_openai.py\", line 134, in _agenerate\\n    return await self.chat_openai._agenerate(messages, stop, run_manager, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\site-packages\\\\langchain\\\\chat_models\\\\openai.py\", line 435, in _agenerate\\n    return await _agenerate_from_stream(stream_iter)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\site-packages\\\\langchain\\\\chat_models\\\\base.py\", line 70, in _agenerate_from_stream\\n    async for chunk in stream:\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\site-packages\\\\langchain\\\\chat_models\\\\openai.py\", line 404, in _astream\\n    async for chunk in await acompletion_with_retry(\\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\site-packages\\\\langchain\\\\chat_models\\\\openai.py\", line 101, in acompletion_with_retry\\n    return await _completion_with_retry(**kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\site-packages\\\\tenacity\\\\_asyncio.py\", line 88, in async_wrapped\\n    return await fn(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\site-packages\\\\tenacity\\\\_asyncio.py\", line 47, in __call__\\n    do = self.iter(retry_state=retry_state)\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\site-packages\\\\tenacity\\\\__init__.py\", line 314, in iter\\n    return fut.result()\\n           ^^^^^^^^^^^^\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\concurrent\\\\futures\\\\_base.py\", line 449, in result\\n    return self.__get_result()\\n           ^^^^^^^^^^^^^^^^^^^\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\concurrent\\\\futures\\\\_base.py\", line 401, in __get_result\\n    raise self._exception\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\site-packages\\\\tenacity\\\\_asyncio.py\", line 50, in __call__\\n    result = await fn(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\site-packages\\\\langchain\\\\chat_models\\\\openai.py\", line 99, in _completion_with_retry\\n    return await llm.client.acreate(**kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\site-packages\\\\openai\\\\api_resources\\\\chat_completion.py\", line 45, in acreate\\n    return await super().acreate(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\site-packages\\\\openai\\\\api_resources\\\\abstract\\\\engine_api_resource.py\", line 219, in acreate\\n    response, _, api_key = await requestor.arequest(\\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\site-packages\\\\openai\\\\api_requestor.py\", line 374, in arequest\\n    result = await self.arequest_raw(\\n             ^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\site-packages\\\\langchain_openai_limiter\\\\capture_headers.py\", line 123, in arequest_raw\\n    model_name, limit_info = _extract_limit_info(response.headers)\\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', '  File \"c:\\\\Users\\\\alex4321\\\\anaconda3\\\\envs\\\\pino-inferior\\\\Lib\\\\site-packages\\\\langchain_openai_limiter\\\\capture_headers.py\", line 32, in _extract_limit_info\\n    rpm_total = int(headers[\"x-ratelimit-limit-requests\"])\\n                    ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n', \"KeyError: 'x-ratelimit-limit-requests'\\n\"]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'x-ratelimit-limit-requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mf:\\Projects\\pino-inferior\\nbs\\09_server.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mawait\u001b[39;00m process_comment_request(\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     CommentRequest(\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         context\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mПост о войне России и Украины\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         history\u001b[39m=\u001b[39m[\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m             Message(\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mMoonlight\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m2023-10-01 11:29:00\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mМы скоро закончим с Украиной.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m             ),\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             Message(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39malex4321\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m2023-10-01 11:30:00\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mА что, случился какой-то прогресс после 6 месяцев взятия Бахмута?\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mНу, чтобы подозревать что это произойдёт вскоре, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mа не затянется на годы независимо от исхода.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m             ),\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m             Message(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mMoonlight\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m2023-10-01 11:31:00\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mВремя - ресурс, у нас его дохуя.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m             )\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         ],\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         user\u001b[39m=\u001b[39mUserDescriptionWithStyle(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m             name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39malex4321\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m             character\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39myou are a programmer, 29 y.o. male\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m             goals\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConvince as much people as possible that the ongoing war between Russia and Ukraine is only bringing more problem, that it do not solve any issues, and moreover - break the impression that it is widely supported. To do so - express your opinion, and debunk opponents opinions. And that Russia should quit it.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m             style_description\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m- Non-formal style, using mainly Russian language \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m(my English is a bit screwed up)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m- Brief. Most time.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m- Overuse memes sometimes.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m             style_examples\u001b[39m=\u001b[39m[\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mВ вакууме, да :Yoba:. Оба же тут существовали всё время или имели таки достигнутое соглашение, чтобы было от чего отталкиваться как опорной точки :Yoba:\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mНе особо-то может. Не привлекая население в виде не 1\u001b[39m\u001b[39m%\u001b[39m\u001b[39m принудительного мармелада и пары процентов добровольного, а в виде процентов 10.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mА то, чтобы привлечь большое количество - неплохо бы, чтобы они понимали, нахуя это им надо. А то так численность военкомов может начать неприемлемо быстро падать, а следом их желание работать.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mА с этим у пропаганды проблема. Вот с чем у них нет проблем, так это с стимуляцией пассивности, но это обратно нужному (для названной вами задачи).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mДа и опять же - ну вот убедишь ты в идее не какого-нибудь Стрелкова и клуб рассерженных долбоёбов, а большое число людей. Что делать, когда (не если, а когда) идея станет неактуальной? Показательной посадкой пары человек дело не закончится же.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mТочнее не так - смену она не устраивала.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mОна просто выстрелила себе в ногу так, что потом что-то новое приходилось строить не апгрейдом предыдущей системы, а из кусков её трупа.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m             ],\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         )\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     ),\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     aprint,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m )\n",
      "\u001b[1;32mf:\\Projects\\pino-inferior\\nbs\\09_server.ipynb Cell 22\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mawait\u001b[39;00m _inner_callback(\u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mERROR\u001b[39m\u001b[39m\"\u001b[39m, datetime\u001b[39m.\u001b[39mnow(), traceback\u001b[39m.\u001b[39mformat_exception(err))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mraise\u001b[39;00m err\n",
      "\u001b[1;32mf:\\Projects\\pino-inferior\\nbs\\09_server.ipynb Cell 22\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     fallacy_gpt, agent_gpt, _, agent \u001b[39m=\u001b[39m _initialize_agent(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         [AsyncFunctionalStyleChatCompletionHandler(_fallacy_callback)],\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         [AsyncFunctionalStyleChatCompletionHandler(_agent_callback)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     inputs \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         AGENT_INPUT_TIME: datetime\u001b[39m.\u001b[39mnow(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         AGENT_INPUT_CONTEXT: request\u001b[39m.\u001b[39mcontext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         AGENT_INPUT_STYLE_DESCRIPTION: request\u001b[39m.\u001b[39muser\u001b[39m.\u001b[39mstyle_description,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m agent\u001b[39m.\u001b[39marun(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mawait\u001b[39;00m _inner_callback(\u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mEND\u001b[39m\u001b[39m\"\u001b[39m, datetime\u001b[39m.\u001b[39mnow(), response)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/Projects/pino-inferior/nbs/09_server.ipynb#Z1062sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mF:\\Projects\\pino-inferior\\pino_inferior\\agent.py:519\u001b[0m, in \u001b[0;36mRolePlayAgent.arun\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39marun\u001b[39m(\u001b[39mself\u001b[39m, inputs: \u001b[39mdict\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    514\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[39m    Run agent\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \u001b[39m    :param inputs: inputs\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[39m    :returns: agent response\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 519\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m _arun_agent(\n\u001b[0;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m         agent_input_preprocessor\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocessing_chain,\n\u001b[0;32m    522\u001b[0m         agent_llm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm,\n\u001b[0;32m    523\u001b[0m         tool_call_stop_sequence\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt_markup\u001b[39m.\u001b[39mtool_call_stop_sequence,\n\u001b[0;32m    524\u001b[0m         tool_call_close_sequence\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt_markup\u001b[39m.\u001b[39mtool_call_close_sequence,\n\u001b[0;32m    525\u001b[0m         tools\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtools,\n\u001b[0;32m    526\u001b[0m         tool_length_function\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlengths\u001b[39m.\u001b[39mlength_function,\n\u001b[0;32m    527\u001b[0m         tool_cut_function\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlengths\u001b[39m.\u001b[39mcut_function,\n\u001b[0;32m    528\u001b[0m         tool_query_max_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlengths\u001b[39m.\u001b[39mmax_tool_query_length,\n\u001b[0;32m    529\u001b[0m         tool_response_max_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlengths\u001b[39m.\u001b[39mmax_tool_response_length,\n\u001b[0;32m    530\u001b[0m         response_marker\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt_markup\u001b[39m.\u001b[39mresponse_marker,\n\u001b[0;32m    531\u001b[0m         max_iteration_count\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter,\n\u001b[0;32m    532\u001b[0m         max_token_count\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlengths\u001b[39m.\u001b[39mmax_session_length,\n\u001b[0;32m    533\u001b[0m     )\n",
      "File \u001b[1;32mF:\\Projects\\pino-inferior\\pino_inferior\\agent.py:417\u001b[0m, in \u001b[0;36m_arun_agent\u001b[1;34m(inputs, agent_input_preprocessor, agent_llm, tool_call_stop_sequence, tool_call_close_sequence, tools, tool_length_function, tool_cut_function, tool_query_max_length, tool_response_max_length, response_marker, max_iteration_count, max_token_count)\u001b[0m\n\u001b[0;32m    413\u001b[0m token_count_without_ai \u001b[39m=\u001b[39m agent_llm\u001b[39m.\u001b[39mget_num_tokens_from_messages(chat_inputs\u001b[39m.\u001b[39mmessages)\n\u001b[0;32m    414\u001b[0m \u001b[39massert\u001b[39;00m token_count_without_ai \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m max_token_count, \\\n\u001b[0;32m    415\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSession length (\u001b[39m\u001b[39m{\u001b[39;00mtoken_count_without_ai\u001b[39m}\u001b[39;00m\u001b[39m) exceeds \u001b[39m\u001b[39m{\u001b[39;00mmax_iteration_count\u001b[39m}\u001b[39;00m\u001b[39m limit\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 417\u001b[0m ai_message, final_response, continue_further \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m _arun_agent_iteration(\n\u001b[0;32m    418\u001b[0m     inputs,\n\u001b[0;32m    419\u001b[0m     chat_inputs,\n\u001b[0;32m    420\u001b[0m     agent_llm,\n\u001b[0;32m    421\u001b[0m     tool_call_stop_sequence,\n\u001b[0;32m    422\u001b[0m     tool_call_close_sequence,\n\u001b[0;32m    423\u001b[0m     tools,\n\u001b[0;32m    424\u001b[0m     tool_length_function,\n\u001b[0;32m    425\u001b[0m     tool_cut_function,\n\u001b[0;32m    426\u001b[0m     tool_query_max_length,\n\u001b[0;32m    427\u001b[0m     tool_response_max_length,\n\u001b[0;32m    428\u001b[0m     response_marker\n\u001b[0;32m    429\u001b[0m )\n\u001b[0;32m    430\u001b[0m ai_messages\u001b[39m.\u001b[39mappend(ai_message)\n\u001b[0;32m    431\u001b[0m chat_inputs\u001b[39m.\u001b[39mmessages\u001b[39m.\u001b[39mappend(ai_message)\n",
      "File \u001b[1;32mF:\\Projects\\pino-inferior\\pino_inferior\\agent.py:345\u001b[0m, in \u001b[0;36m_arun_agent_iteration\u001b[1;34m(inputs, agent_prompt, agent_llm, tool_call_stop_sequence, tool_call_close_sequence, tools, tool_length_function, tool_cut_function, tool_query_max_length, tool_response_max_length, response_marker)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_arun_agent_iteration\u001b[39m(inputs: \u001b[39mdict\u001b[39m,\n\u001b[0;32m    335\u001b[0m                          agent_prompt: ChatPromptValue,\n\u001b[0;32m    336\u001b[0m                          agent_llm: BaseChatModel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    343\u001b[0m                          tool_response_max_length: \u001b[39mint\u001b[39m,\n\u001b[0;32m    344\u001b[0m                          response_marker: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[AIMessage, Union[\u001b[39mstr\u001b[39m, \u001b[39mNone\u001b[39;00m], \u001b[39mbool\u001b[39m]:\n\u001b[1;32m--> 345\u001b[0m     llm_output \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m _arun_agent_llm(\n\u001b[0;32m    346\u001b[0m         agent_prompt\u001b[39m=\u001b[39magent_prompt,\n\u001b[0;32m    347\u001b[0m         agent_llm\u001b[39m=\u001b[39magent_llm,\n\u001b[0;32m    348\u001b[0m         tool_call_stop_sequence\u001b[39m=\u001b[39mtool_call_stop_sequence,\n\u001b[0;32m    349\u001b[0m         response_stop_sequence\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[/\u001b[39m\u001b[39m{\u001b[39;00mresponse_marker\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    350\u001b[0m     )\n\u001b[0;32m    351\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m _aprocess_agent_iteration_output(\n\u001b[0;32m    352\u001b[0m         inputs,\n\u001b[0;32m    353\u001b[0m         llm_output,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         response_marker,\n\u001b[0;32m    362\u001b[0m     )\n",
      "File \u001b[1;32mF:\\Projects\\pino-inferior\\pino_inferior\\agent.py:178\u001b[0m, in \u001b[0;36m_arun_agent_llm\u001b[1;34m(agent_prompt, agent_llm, tool_call_stop_sequence, response_stop_sequence)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_arun_agent_llm\u001b[39m(agent_prompt: ChatPromptValue,\n\u001b[0;32m    175\u001b[0m                           agent_llm: BaseChatModel,\n\u001b[0;32m    176\u001b[0m                           tool_call_stop_sequence: \u001b[39mstr\u001b[39m,\n\u001b[0;32m    177\u001b[0m                           response_stop_sequence: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m--> 178\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m agent_llm\u001b[39m.\u001b[39magenerate(\n\u001b[0;32m    179\u001b[0m         [agent_prompt\u001b[39m.\u001b[39mmessages],\n\u001b[0;32m    180\u001b[0m         stop\u001b[39m=\u001b[39m[tool_call_stop_sequence, response_stop_sequence]\n\u001b[0;32m    181\u001b[0m     )\n\u001b[0;32m    182\u001b[0m     generation \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m    183\u001b[0m     \u001b[39mreturn\u001b[39;00m generation\u001b[39m.\u001b[39mtext\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain\\chat_models\\base.py:429\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[39mif\u001b[39;00m run_managers:\n\u001b[0;32m    418\u001b[0m         \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\n\u001b[0;32m    419\u001b[0m             \u001b[39m*\u001b[39m[\n\u001b[0;32m    420\u001b[0m                 run_manager\u001b[39m.\u001b[39mon_llm_end(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    427\u001b[0m             ]\n\u001b[0;32m    428\u001b[0m         )\n\u001b[1;32m--> 429\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions[\u001b[39m0\u001b[39m]\n\u001b[0;32m    430\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[0;32m    431\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[0;32m    432\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[0;32m    433\u001b[0m ]\n\u001b[0;32m    434\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain\\callbacks\\manager.py:482\u001b[0m, in \u001b[0;36m_ahandle_event_for_handler\u001b[1;34m(handler, event_name, ignore_condition_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    480\u001b[0m event \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, event_name)\n\u001b[0;32m    481\u001b[0m \u001b[39mif\u001b[39;00m asyncio\u001b[39m.\u001b[39miscoroutinefunction(event):\n\u001b[1;32m--> 482\u001b[0m     \u001b[39mawait\u001b[39;00m event(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    483\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    484\u001b[0m     \u001b[39mif\u001b[39;00m handler\u001b[39m.\u001b[39mrun_inline:\n",
      "File \u001b[1;32mF:\\Projects\\pino-inferior\\pino_inferior\\function_callbacks.py:85\u001b[0m, in \u001b[0;36mAsyncFunctionalStyleChatCompletionHandler.on_llm_error\u001b[1;34m(self, error, run_id, parent_run_id, tags, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mon_llm_error\u001b[39m(\u001b[39mself\u001b[39m, error: \u001b[39mBaseException\u001b[39;00m, \u001b[39m*\u001b[39m, run_id: UUID, parent_run_id: UUID \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, tags: List[\u001b[39mstr\u001b[39m] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_callback(\n\u001b[0;32m     81\u001b[0m         LLMEventType\u001b[39m.\u001b[39mLLM_ERROR,\n\u001b[0;32m     82\u001b[0m         datetime\u001b[39m.\u001b[39mnow(),\n\u001b[0;32m     83\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(format_exception(error)),\n\u001b[0;32m     84\u001b[0m     )\n\u001b[1;32m---> 85\u001b[0m     \u001b[39mraise\u001b[39;00m error\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\asyncio\\tasks.py:267\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     \u001b[39mif\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    265\u001b[0m         \u001b[39m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    266\u001b[0m         \u001b[39m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39msend(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    268\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain\\chat_models\\base.py:532\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    529\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    530\u001b[0m     )\n\u001b[0;32m    531\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[1;32m--> 532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agenerate(\n\u001b[0;32m    533\u001b[0m         messages, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    534\u001b[0m     )\n\u001b[0;32m    535\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    536\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agenerate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain_openai_limiter\\choose_key_chat_openai.py:119\u001b[0m, in \u001b[0;36mChooseKeyChatOpenAI._agenerate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m chat_openai\u001b[39m.\u001b[39mopenai_api_key \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m achoose_key(chat_openai\u001b[39m.\u001b[39mmodel_name,\n\u001b[0;32m    116\u001b[0m                                                \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopenai_api_keys,\n\u001b[0;32m    117\u001b[0m                                                token_count)\n\u001b[0;32m    118\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m chat_openai\u001b[39m.\u001b[39m_agenerate(messages,\n\u001b[0;32m    120\u001b[0m                                     stop,\n\u001b[0;32m    121\u001b[0m                                     run_manager,\n\u001b[0;32m    122\u001b[0m                                     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain_openai_limiter\\limit_await_chat_openai.py:134\u001b[0m, in \u001b[0;36mLimitAwaitChatOpenAI._agenerate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39mawait\u001b[39;00m await_for_limit(\n\u001b[0;32m    127\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name,\n\u001b[0;32m    128\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopenai_api_key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlimit_await_sleep,\n\u001b[0;32m    132\u001b[0m )\n\u001b[0;32m    133\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_openai\u001b[39m.\u001b[39m_agenerate(messages, stop, run_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain\\chat_models\\openai.py:435\u001b[0m, in \u001b[0;36mChatOpenAI._agenerate\u001b[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[39mif\u001b[39;00m should_stream:\n\u001b[0;32m    432\u001b[0m     stream_iter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_astream(\n\u001b[0;32m    433\u001b[0m         messages, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    434\u001b[0m     )\n\u001b[1;32m--> 435\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m _agenerate_from_stream(stream_iter)\n\u001b[0;32m    437\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    438\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs}\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain\\chat_models\\base.py:70\u001b[0m, in \u001b[0;36m_agenerate_from_stream\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_agenerate_from_stream\u001b[39m(\n\u001b[0;32m     67\u001b[0m     stream: AsyncIterator[ChatGenerationChunk],\n\u001b[0;32m     68\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatResult:\n\u001b[0;32m     69\u001b[0m     generation: Optional[ChatGenerationChunk] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39masync\u001b[39;00m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m stream:\n\u001b[0;32m     71\u001b[0m         \u001b[39mif\u001b[39;00m generation \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m             generation \u001b[39m=\u001b[39m chunk\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain\\chat_models\\openai.py:404\u001b[0m, in \u001b[0;36mChatOpenAI._astream\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m}\n\u001b[0;32m    403\u001b[0m default_chunk_class \u001b[39m=\u001b[39m AIMessageChunk\n\u001b[1;32m--> 404\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mawait\u001b[39;00m acompletion_with_retry(\n\u001b[0;32m    405\u001b[0m     \u001b[39mself\u001b[39m, messages\u001b[39m=\u001b[39mmessage_dicts, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    406\u001b[0m ):\n\u001b[0;32m    407\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(chunk[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    408\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain\\chat_models\\openai.py:101\u001b[0m, in \u001b[0;36macompletion_with_retry\u001b[1;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m     97\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m     98\u001b[0m     \u001b[39m# Use OpenAI's async api https://github.com/openai/openai-python#async-api\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39macreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 101\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\tenacity\\_asyncio.py:88\u001b[0m, in \u001b[0;36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(fn)\n\u001b[0;32m     87\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39masync_wrapped\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\tenacity\\_asyncio.py:47\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m     49\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\tenacity\\_asyncio.py:50\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m     49\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     51\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[0;32m     52\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain\\chat_models\\openai.py:99\u001b[0m, in \u001b[0;36macompletion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m     97\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m     98\u001b[0m     \u001b[39m# Use OpenAI's async api https://github.com/openai/openai-python#async-api\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39macreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\openai\\api_resources\\chat_completion.py:45\u001b[0m, in \u001b[0;36mChatCompletion.acreate\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 45\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39macreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     46\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     47\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:219\u001b[0m, in \u001b[0;36mEngineAPIResource.acreate\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    195\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39macreate\u001b[39m(\n\u001b[0;32m    196\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    204\u001b[0m ):\n\u001b[0;32m    205\u001b[0m     (\n\u001b[0;32m    206\u001b[0m         deployment_id,\n\u001b[0;32m    207\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    218\u001b[0m     )\n\u001b[1;32m--> 219\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m requestor\u001b[39m.\u001b[39marequest(\n\u001b[0;32m    220\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         url,\n\u001b[0;32m    222\u001b[0m         params\u001b[39m=\u001b[39mparams,\n\u001b[0;32m    223\u001b[0m         headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m    224\u001b[0m         stream\u001b[39m=\u001b[39mstream,\n\u001b[0;32m    225\u001b[0m         request_id\u001b[39m=\u001b[39mrequest_id,\n\u001b[0;32m    226\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    227\u001b[0m     )\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    230\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    231\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\openai\\api_requestor.py:374\u001b[0m, in \u001b[0;36mAPIRequestor.arequest\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    372\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 374\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marequest_raw(\n\u001b[0;32m    375\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    376\u001b[0m         url,\n\u001b[0;32m    377\u001b[0m         session,\n\u001b[0;32m    378\u001b[0m         params\u001b[39m=\u001b[39mparams,\n\u001b[0;32m    379\u001b[0m         supplied_headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m    380\u001b[0m         files\u001b[39m=\u001b[39mfiles,\n\u001b[0;32m    381\u001b[0m         request_id\u001b[39m=\u001b[39mrequest_id,\n\u001b[0;32m    382\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    383\u001b[0m     )\n\u001b[0;32m    384\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_async_response(result, stream)\n\u001b[0;32m    385\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[39m# Close the request before exiting session context.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain_openai_limiter\\capture_headers.py:123\u001b[0m, in \u001b[0;36m_wrap_arequest_raw.<locals>.arequest_raw\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m response: aiohttp\u001b[39m.\u001b[39mClientResponse \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m old_arequest_raw(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    122\u001b[0m api_key \u001b[39m=\u001b[39m _extract_openai_api_key(response\u001b[39m.\u001b[39mrequest_info\u001b[39m.\u001b[39mheaders[\u001b[39m\"\u001b[39m\u001b[39mauthorization\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m--> 123\u001b[0m model_name, limit_info \u001b[39m=\u001b[39m _extract_limit_info(response\u001b[39m.\u001b[39;49mheaders)\n\u001b[0;32m    124\u001b[0m \u001b[39mif\u001b[39;00m model_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     model_name \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mrequest_info\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mx-model\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\langchain_openai_limiter\\capture_headers.py:32\u001b[0m, in \u001b[0;36m_extract_limit_info\u001b[1;34m(headers)\u001b[0m\n\u001b[0;32m     30\u001b[0m current_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m     31\u001b[0m model_name: ModelName \u001b[39m=\u001b[39m headers\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mopenai-model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m rpm_total \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(headers[\u001b[39m\"\u001b[39;49m\u001b[39mx-ratelimit-limit-requests\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     33\u001b[0m tpm_total \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(headers[\u001b[39m\"\u001b[39m\u001b[39mx-ratelimit-limit-tokens\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     34\u001b[0m rpm_remain \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(headers[\u001b[39m\"\u001b[39m\u001b[39mx-ratelimit-remaining-requests\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'x-ratelimit-limit-requests'"
     ]
    }
   ],
   "source": [
    "await process_comment_request(\n",
    "    CommentRequest(\n",
    "        id=1,\n",
    "        context=\"Пост о войне России и Украины\",\n",
    "        history=[\n",
    "            Message(\n",
    "                \"Moonlight\",\n",
    "                \"2023-10-01 11:29:00\",\n",
    "                \"Мы скоро закончим с Украиной.\"\n",
    "            ),\n",
    "            Message(\n",
    "                \"alex4321\",\n",
    "                \"2023-10-01 11:30:00\",\n",
    "                \"А что, случился какой-то прогресс после 6 месяцев взятия Бахмута?\\n\\n\" + \\\n",
    "                    \"Ну, чтобы подозревать что это произойдёт вскоре, \" + \\\n",
    "                    \"а не затянется на годы независимо от исхода.\"\n",
    "            ),\n",
    "            Message(\n",
    "                \"Moonlight\",\n",
    "                \"2023-10-01 11:31:00\",\n",
    "                \"Время - ресурс, у нас его дохуя.\"\n",
    "            )\n",
    "        ],\n",
    "        user=UserDescriptionWithStyle(\n",
    "            name=\"alex4321\",\n",
    "            character=\"you are a programmer, 29 y.o. male\",\n",
    "            goals=\"Convince as much people as possible that the ongoing war between Russia and Ukraine is only bringing more problem, that it do not solve any issues, and moreover - break the impression that it is widely supported. To do so - express your opinion, and debunk opponents opinions. And that Russia should quit it.\",\n",
    "            style_description=\"- Non-formal style, using mainly Russian language \" + \\\n",
    "                \"(my English is a bit screwed up)\" + \\\n",
    "                \"\\n- Brief. Most time.\\n\" + \\\n",
    "                \"- Overuse memes sometimes.\",\n",
    "            style_examples=[\n",
    "                \"В вакууме, да :Yoba:. Оба же тут существовали всё время или имели таки достигнутое соглашение, чтобы было от чего отталкиваться как опорной точки :Yoba:\",\n",
    "                \"Не особо-то может. Не привлекая население в виде не 1% принудительного мармелада и пары процентов добровольного, а в виде процентов 10.\\nА то, чтобы привлечь большое количество - неплохо бы, чтобы они понимали, нахуя это им надо. А то так численность военкомов может начать неприемлемо быстро падать, а следом их желание работать.\\nА с этим у пропаганды проблема. Вот с чем у них нет проблем, так это с стимуляцией пассивности, но это обратно нужному (для названной вами задачи).\\nДа и опять же - ну вот убедишь ты в идее не какого-нибудь Стрелкова и клуб рассерженных долбоёбов, а большое число людей. Что делать, когда (не если, а когда) идея станет неактуальной? Показательной посадкой пары человек дело не закончится же.\\n\",\n",
    "                \"Точнее не так - смену она не устраивала.\\nОна просто выстрелила себе в ногу так, что потом что-то новое приходилось строить не апгрейдом предыдущей системы, а из кусков её трупа.\",\n",
    "            ],\n",
    "        )\n",
    "    ),\n",
    "    aprint,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context parser API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class ContextRequest(Request):\n",
    "    text: str\n",
    "    time: str\n",
    "    user: UserDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _initialize_context_extractor(callbacks: List[AsyncFunctionalStyleChatCompletionHandler]) -> RunnableSequence:\n",
    "    _, llm = _initialize_openai_chat_model(OPENAI_CONTEXT_MODEL, callbacks)\n",
    "    encoding = tiktoken.encoding_for_model(llm.model_name)\n",
    "    context_extractor = build_context_extractor_chain(\n",
    "        llm,\n",
    "        lengths=ContextExtractorLengthConfig(\n",
    "            cut_function=lambda text, length: encoding.decode(encoding.encode(text)[:length]),\n",
    "            length_function=lambda text: len(encoding.encode(text)),\n",
    "            max_post_length=SERVER_MAX_CONTEXT_EXTRACTOR_POST_LENGTH,\n",
    "            max_response_length=SERVER_MAX_CONTEXT_LENGTH,\n",
    "        ),\n",
    "        prompts=ContextExtractorPromptMarkupConfig()\n",
    "    )\n",
    "    return context_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def process_context_extraction_request(request: ContextRequest, callback: AsyncCallback) -> None:\n",
    "    async def _inner_callback(system: str, envent_type: str, time: datetime, content: str) -> None:\n",
    "        await callback(request.id, system, envent_type, time, content)\n",
    "    \n",
    "    async def _context_callback(event_type: LLMEventType, time: datetime, content: str) -> None:\n",
    "        await _inner_callback(\"context\", event_type.value, time, content)\n",
    "    \n",
    "    await _inner_callback(\"system\", \"START\", datetime.now(), \"\")\n",
    "    try:\n",
    "        context_extractor = _initialize_context_extractor([\n",
    "            AsyncFunctionalStyleChatCompletionHandler(_context_callback)\n",
    "        ])\n",
    "        response = await context_extractor.ainvoke({\n",
    "            CONTEXT_INPUT_TEXT: request.text,\n",
    "            CONTEXT_INPUT_POST_TIME: pd.to_datetime(request.time),\n",
    "            CONTEXT_INPUT_GOALS: request.user.goals,\n",
    "            CONTEXT_INPUT_CURRENT_TIME: datetime.now(),\n",
    "            CONTEXT_INPUT_USERNAME: request.user.name,\n",
    "            CONTEXT_INPUT_CHARACTER: request.user.character,\n",
    "        })\n",
    "        response_text = response[CONTEXT_OUTPUT_CONTEXT]\n",
    "        await _inner_callback(\"system\", \"END\", datetime.now(), response_text)\n",
    "    except Exception as err:\n",
    "        await _inner_callback(\"system\", \"ERROR\", datetime.now(), traceback.format_exception(err))\n",
    "        raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"\"\"Будет ли мобилизация осенью?⁠⁠\n",
    "Самый, наверное, волнующий россиян вопрос на сегодняшний день, обросший обильными слухами вплоть до вброса конкретных дат. Хочу его разобрать и озвучить свои соображения на тему, будет ли мобилизация, если да – то когда.\n",
    "\n",
    "Мой канал в ТГ: https://t.me/artjockey\n",
    "\n",
    "Напомню старым и новым читателям, что у меня нет никаких инсайдов, я просто озвучу свое мнение и постараюсь его обосновать. С чем можно согласиться или не согласиться.\n",
    "\n",
    "Будет ли мобилизация осенью? Политика, Спецоперация, Война на Украине, Министерство обороны, Мобилизация, Война, Частичная мобилизация, Россия, Призыв, Длиннопост\n",
    "Первая волна мобилизации\n",
    "Она началась прошлой осенью и с 21 сентября по 31 октября было призвано 318 тысяч официально. При этом, указ о мобилизации юридически продолжает действовать. То есть, формально, новую волну власть может не объявлять, а просто, опираясь на действующий указ, начать новый призыв.\n",
    "\n",
    "Для чего нужна была мобилизация? Численность группировки вторжения ВС РФ была около 200 тысяч человек (с учетом войск еще независимых ДЛНР). Украинская армия мирного времени – примерно 260 тысяч человек. То есть, силы были примерно равны, даже в пользу ВС РФ, потому что у ВСУ это не вся сухопутная армия, а с учетом всех тыловых служб, штаба, пограничников и т.д.\n",
    "\n",
    "Но сразу после начала войны, украинское правительство объявляет всеобщую мобилизацию, вполне логичный и правильный шаг. Численность украинской армии быстро увеличивается и к октябрю достигает 700 тысяч человек, по словам украинского министра обороны. Конечно, далеко не все эти люди находились на передовой, то есть, не являлись «штыками». Но перекос в численности воюющих армий уже очевиден и было понятно, что продолжать завоевательную войну имея в 2-3 раза меньше людей невозможно.\n",
    "\n",
    "Частичная мобилизация позволила ВС РФ достичь примерного паритета с ВСУ по численности и продолжить войну в условно равном соотношении сил. Даже если перекос в чью-то сторону сохранялся, он был уже не в разы, а в десятки процентов, не более. Это позволило и равномерно укрепить фронт, чтобы не допустить повторения сценария наступления ВСУ на Изюм.\n",
    "\n",
    "Но всеобщая мобилизация на территории Украины не прекратилась, украинская армия продолжила пополняться, зимой стали известны и планы Киева провести наступление в 2023-м году. В таких условиях уже было понятно, что вторая волна – неизбежна. ВС РФ необходимо, хотя бы, поддерживать паритет по численности. Многие источники писали, что мобилизация будет проведена в начале года, чтобы иметь обученные резервы к концу весны-лету. Я тоже думал, что мобилизацию объявят.\n",
    "\n",
    "Стратегия Кремля\n",
    "Как мы знаем, вторую волну мобилизации не объявили и связано это с курсом руководства России на изоляцию войны от основной массы российского общества. Я не сомневаюсь, что подготовка действительно велась, но в какой-то момент планы поменялись.\n",
    "\n",
    "Для среднего россиянина война идет и, по этой стратегии, должна идти где-то в телевизоре, потому и не война, а СВО. Да, есть некий процент людей, кого затронула мобилизация в первую волну, но их, относительно общего населения страны, немного.\n",
    "\n",
    "Также временно вопрос пополнения закрыл собой Вагнер, через который, как мы теперь знаем, прошло 78 тысяч человек, что довольно много. Это где-то 25% от частично мобилизованных.\n",
    "\n",
    "Но общая проблема с поддержкой паритета по численности личного состава никуда не делась и ее нужно было как-то решать. Решение было озвучено весной, вместо второй волны мобилизации предполагается до конца года набрать 400 тысяч людей на контрактную службу. Вот эти новые контрактники и должны заменить собой «вторую волну».\n",
    "\n",
    "Скрытая мобилизация\n",
    "Есть такое понятие, как скрытая мобилизация. То есть, набор в войска не обязательно ведется с официальными объявлениями по телевизору, фанфарами и всесторонним освещением в СМИ.\n",
    "\n",
    "При этом, не стоит понимать термин настолько буквально, что это некие тайные мероприятия, о которых никто не должен узнать. Совсем не обязательно. Это просто некий общий комплекс мер, призванный замаскировать или скрыть отдельные детали процесса или же прикрыть его юридически. Так, например, официальная мобилизация государства может являться поводом для объявления войны, а, скажем, увеличение численности пограничных войск и парамилитарных образований – таким поводом уже не будет.\n",
    "\n",
    "Также, скрытая мобилизация может скрывать не сам процесс, а его детали. При частичной мобилизации все знали и сколько людей будет призвано, и куда они будут направлены. При скрытой мобилизации такие подробности могут не озвучиваться, хотя сам проходящий процесс и является достоянием общественности.\n",
    "\n",
    "Вот нынешняя стратегия российского руководства и является проведением скрытой мобилизацией. Официально, контрактники набираются не для участия в СВО, а согласно стратегическому плану развития армии, которое не так давно было подписано Путиным и предусматривает увеличение численности ВС РФ и вне контекста войны.\n",
    "\n",
    "Методы скрытой мобилизации\n",
    "Сейчас в РФ задействовано три основных источника пополнения личного состава.\n",
    "\n",
    "Во-первых, это прямой поток добровольцев, заключающих контракты на службу в рядах армии. Жители России, думаю, видели и агрессивную рекламу службы, кто-то сталкивался с агитаторами на работе, а кто-то даже получал повестку для уточнения данных в военкомат, где его уговаривали заключить контракт.\n",
    "\n",
    "Во-вторых, это перевод на контракт срочников. Каждый срочник может заключить контракт, который пойдет в зачет срочки.\n",
    "\n",
    "В-третьих, это продолжающийся набор в тюрьмах, в основном, в отряды Шторм вместо Вагнера.\n",
    "\n",
    "Все эти три метода позволяют увеличить численность личного состава в рядах вооруженных сил, и они все задействованы. Но есть еще один метод, это уже мобилизованные, которым предлагают заключить контракт. Таки люди тоже идут в статистику, но вот на общее число солдат они не влияют.\n",
    "\n",
    "Есть и менее очевидные способы, вот новость прям с Пикабу. Когда в ходе недавних массовых рейдов среди мест компактного проживания граждан, недавно получивших паспорт, выявляли тех, кто еще не стал на учет в военкомат и помогали им это сделать прям с доставкой к месту оформления документов.\n",
    "\n",
    "Успехи скрытой мобилизации\n",
    "Вот теперь о цифрах, как ни странно, они нам известны. Бывший президент, Дмитрий Медведев, который как раз и курирует этот процесс, озвучил, что с 1 апреля по август на контракт записалась 231 тысяча человек.\n",
    "\n",
    "Это довольно много и почти догоняет общее число мобилизованных в первую волну. То есть, по факту, перекрывает потребности во второй волне мобилизации. И да, наверняка, люди сразу скажут о вот тех самых приписках, когда уже мобилизованных переводят на контракт, учитывают в статистике, но фактическое положение дел не меняется. К сожалению, мы не знаем, сколько таких людей среди этих 231 тысячи есть.\n",
    "\n",
    "Но у нас есть взгляд и с другой стороны. Скрытую мобилизацию в РФ совсем недавно комментировал представитель ГУР Украины, он подтвердил, что «разными методами» в месяц мобилизуется 20 тысяч человек. То есть, даже с учетом всех манипуляций, поток новобранцев в зону СВО достаточно большой, в течение года это и будут те самые примерно 300 тысяч человек, которые были в первой волне.\n",
    "\n",
    "Промежуточные выводы\n",
    "Вторая волна мобилизации в России уже идет и идет достаточно успешно. Даже при мобилизации 20 тысяч человек в месяц, это в общем позволяет руководству РФ отказаться от проведения отдельной второй волны частичной мобилизации и закрыть все потребности за счет добровольцев (условно, все мы понимаем, что часть людей заключает контракт добровольно-принудительно).\n",
    "\n",
    "Также вспоминаем, что я написал в начале – указ о частичной мобилизации юридически еще действует. Можно предположить, что при недостаточном потоке новобранцев будет проведена не вторая волна по образу первой, а еще включен еще один механизм. Начнут рассылать повестки ежемесячно в небольшом количестве, уже прямо призывая не 300 тысяч человек за раз, а, скажем, по 10 тысяч человек в месяц.\n",
    "\n",
    "Именно так происходит на Украине. Процесс рассылки и раздачи повесток идет постоянно без каких-либо волн. Повестка – это вызов на «уточнение данных», где уже вместо нее вручают мобилизационное предписание, по украинской практике, даже не выпуская из военкомата. И напомню, что в России рассылаются похожие повестки и сейчас, только с целью уговорить заключить контракт, но при необходимости, просто начнут сразу давать предписания.\n",
    "\n",
    "Юридические изменения\n",
    "Между тем, Госдума ввела ряд изменений в законодательство, которые тоже серьезно всколыхнули общество. Во-первых, это увеличение возраста запаса, во-вторых, увеличение возраста призыва на срочную службу, в-третьих, штрафы для уклонистов.\n",
    "\n",
    "Из этих изменений делается вывод, что мобилизация готовится осенью. Но это не совсем так, первые два пункта точно никак не могут повлиять на осеннюю мобилизацию. Возраст учета вообще вводится поэтапно, каждый год на один год. А возраст призыва увеличивается с 2024-го, но не с 2023-го года. На мой взгляд, это не подготовка к СВО вообще, а подготовка к вероятной эскалации конфликта. Я уже как-то говорил, что мы, возможно, находимся в 1938-м или в 1913-м году, просто этого еще не знаем. А лет через 20 будем спорить, является ли Российско-Украинская война отдельным конфликтом или это один из театров действий Третьей мировой.\n",
    "\n",
    "Остается третий пункт, ужесточение штрафов для уклонистов. Кто-то это воспринял как легитимизацию взятки за уклонение, но ведь штраф не освобождает от обязанности явиться в военкомат по повестке.\n",
    "\n",
    "Я считаю, что тут смысл в другом. Чуть выше я писал, что один из источников пополнения личного состава в зоне СВО – это срочники. И здесь тоже есть изменения, срочник сможет заключить контракт через месяц после призыва, а раньше мог только через три.\n",
    "\n",
    "И вот эти все изменения направлены на то, чтобы упростить осенний призыв на срочную служба, а уже на срочке начнут обрабатывать призывников на тему подписания контракта. И здесь, конечно, к кому-то могут применить кнут, кто-то поведется на пряник в виде, в первую очередь, зарплаты. Штраф этому только поможет, ведь штрафник может записаться на контракт и с нескольких зарплат с ним расплатиться.\n",
    "\n",
    "Есть и еще одно не самое заметное изменение. С недавнего времени, иностранцы могут заключить контракт с ВС РФ на один год, а затем получить гражданство по упрощенной процедуре.\n",
    "\n",
    "Логические «против»\n",
    "Кроме того, что реальные и юридические действия руководства России нацелены на то, чтобы избежать проведения второй волны частичной мобилизации, есть еще и объективные причины, почему ее не стоит проводить, во всяком случае, сейчас.\n",
    "\n",
    "Проведений первой волны выявило ряд проблем, которые такой метод создает. И здесь даже можно обойтись без пруфов, просто само по себе очевидно, что одномоментный призыв в армию 300 тысяч человек за 1,5 месяца сильно перегружает все тыловые службы. Так, например, призыв на срочку – это 120-150 тысяч человек в течение 3-х месяцев.\n",
    "\n",
    "При этом, на Украине, например, при всеобщей мобилизации отменили срочную службу, но в России при частичной – нет. И в прошлом году это привело к тому, что срок призыва на срочку пришлось сдвинуть на месяц.\n",
    "\n",
    "Зима и поздняя осень – это неудобное время для подготовки личного состава. В отличие от срочников, которые, условно, будут «красить траву», эти люди отправляются на войну и нужно постараться их хоть как-то обучить. Зимой это не слишком удобно из-за погоды, температуры и всего остального. Если на полигоне грязь, в которой танк застрянет, обучения не получится.\n",
    "\n",
    "Конечно, возникает правильный вопрос, если все эти проблемы очевидны даже с дивана, зачем тогда проводилась первая волна? Она была продиктована военной необходимостью, когда с ней очень долго тянули, а потом вынуждены были добирать людей на войну в условиях, что они нужны были еще на вчера (на начало сентября в Харьковской области).\n",
    "\n",
    "Сейчас же ситуация на фронте явно не диктует срочной необходимости добирать людей. В моем прошлом посте я показал, что Россия проводит успешную оборонительную операцию, нет признаков обрушения фронта или возможных скорых успехов ВСУ. При том, у России хватает сил не только обороняться, но еще и наступать на другом фланге на Купянск.\n",
    "\n",
    "Вопрос ротации\n",
    "Еще один часто поднимаемый вопрос, это то, что мобилизация необходима для ротации, а то и для демобилизации частично мобилизованных. Потому что они уже почти год как воюют, а хотят по домам. Ну или хотя бы на ротацию.\n",
    "\n",
    "Возможно, меня поправят действующие военные, но у меня сложилось мнение, что все эти нормативы по ротации, записанные в уставах, в нынешней войне пошли к одному месту с обеих сторон. В реальности, классических ротаций войск не проводит ни Россия, ни Украина, за исключением частей, понесших сильные потери. Для отдыха военнослужащим дают отпуска, я знаю лично мобилизованных, кто уже успел побывать дома, а кто-то и два раза каким-то образом, но само подразделение в тыл не отводится.\n",
    "\n",
    "Что же касается демобилизации – ее не планируется точно и про это говорил Путин в июне на встрече с военкорами. Ему задавали этот вопрос, от ответа он ушел, сказав, что все будет зависеть от ситуации на фронте. Потому рассчитывать на демобилизацию через год я бы точно не стал. Возможно, такие процессы начнутся к концу второго года службы мобилизованных, но точно не в ближайшее время и Путин ясно дал это понять.\n",
    "\n",
    "К тому же, среди мобилизованных активно идет агитация подписывать контракт, который на время СВО является бессрочным. Вот, возможно, как раз к концу второго года, когда уже и будут более точные расклады по ходу боевых действий с учетом вероятного наступления ВС РФ, начнут отпускать. Тогда получится, что кто-то выбыл из-за потерь, кто-то подписал контракт и никуда не денется, а вот оставшихся хотя бы частично можно будет демобилизовать. Вероятно, по каким-то критериям, в первую очередь, тех, кто постарше, у кого много детей и т.д. И при условии, что не будет эскалации конфликта с военным положением и всеобщей мобилизацией, конечно.\n",
    "\n",
    "Общие выводы и советы\n",
    "На мой взгляд, ничего не указывает на то, что в России готовится вторая волна мобилизации осенью, как пишут некоторые медийные источники. Наоборот, принимаются меры, чтобы этой волны избежать. В реальности, мобилизация уже идет, только совсем другими методами, не теми, что были задействованы прошлой осенью.\n",
    "\n",
    "При этом, напомню, что у меня нет инсайдов и я не принимаю решения, за чужую глупость я не в ответе. Может быть, я не прав и российское руководство решит зачем-то еще раз прыгнуть на мобилизационные грабли, но вроде я постарался обосновать, почему этого не должны сделать.\n",
    "\n",
    "В реальности же, если в МО РФ видят, что поток контрактников недостаточен для закрытия потребностей в живой силе, я думаю, что будет задействован механизм настоящей скрытой мобилизации – рассылка повесток ежемесячно без широкой огласки и без резких «волн», чтобы не напрягать армейский тыл и максимально скрывать это от общества.\n",
    "\n",
    "То есть, если лично вы опасаетесь быть мобилизованным, я бы дал совет не ждать повторения событий прошлого года, а внимательно следить за новостями. Конечно же, не федеральными, а местными, особенно, неформальными СМИ типа групп в Телеграме и ВК, где выкладывают локальные новости. Слушать, что говорит окружение, коллеги и даже читать оппозиционные СМИ, они точно напишут. И обращать внимание не на сам факт рассылки повесток, их постоянно рассылают для сверки данных, а вот на случаи, когда человек пошел, а ему там мобилизационное предписание выписали. Вот когда такое заметите – вот тогда собирайтесь на дачу или в Верхний Ларс, куда запланировано.\n",
    "\n",
    "В целом же, некая масштабная мобилизация даже в виде волн – возможна, но для нее должны сложиться объективные обстоятельства непосредственно на фронте. И ясны они будут ближе к лету 2024-го года.\n",
    "\n",
    "Мой канал в ТГ, в нем выходит намного больше разных постов, никаких репостов, исключительно мои авторские тексты: https://t.me/artjockey\n",
    "\n",
    "\n",
    "#[моё] #Политика #Спецоперация #Война на Украине #Министерство обороны #Мобилизация #Война #Частичная мобилизация #Россия #Призыв #Длиннопост\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await process_context_extraction_request(\n",
    "    ContextRequest(\n",
    "        id=2,\n",
    "        text=TEXT,\n",
    "        time=\"2023-08-01 11:00:12\",\n",
    "        user=UserDescription(\n",
    "            name=\"alex4321\",\n",
    "            character=\"you are a programmer, 29 y.o. male\",\n",
    "            goals=\"Convince as much people as possible that the ongoing war between Russia and Ukraine is only bringing more problem, that it do not solve any issues, and moreover - break the impression that it is widely supported. To do so - express your opinion, and debunk opponents opinions. And that Russia should quit it.\",\n",
    "        )\n",
    "    ),\n",
    "    aprint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Add remember request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "ApiMethodImplementation = Callable[[Request, AsyncCallback], Awaitable[None]]\n",
    "ApiMethods = Dict[str, Tuple[ApiMethodImplementation, type]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _parse_message(methods: ApiMethods, message: str) -> Tuple[ApiMethodImplementation, Request]:\n",
    "    method, params = message.split(\" \", maxsplit=1)\n",
    "    assert method in methods\n",
    "    method_implementation, request_class = methods[method]\n",
    "    request = _parse_json_as(request_class, params)\n",
    "    return method_implementation, request    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_parse_message(\n",
    "    {\n",
    "        \"comment\": (process_comment_request, CommentRequest),\n",
    "        \"context\": (process_context_extraction_request, ContextRequest),\n",
    "    },\n",
    "    \"context \" + json.dumps(ContextRequest(\n",
    "        id=2,\n",
    "        text=TEXT,\n",
    "        time=\"2023-08-01 11:00:12\",\n",
    "        user=UserDescription(\n",
    "            name=\"alex4321\",\n",
    "            character=\"you are a programmer, 29 y.o. male\",\n",
    "            goals=\"Convince as much people as possible that the ongoing war between Russia and Ukraine is only bringing more problem, that it do not solve any issues, and moreover - break the impression that it is widely supported. To do so - express your opinion, and debunk opponents opinions. And that Russia should quit it.\",\n",
    "        )\n",
    "    ), default=pydantic_encoder)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def server() -> None:\n",
    "    methods = {\n",
    "        \"comment\": (process_comment_request, CommentRequest),\n",
    "        \"context\": (process_context_extraction_request, ContextRequest),\n",
    "    }\n",
    "\n",
    "    async def process(websocket: WebSocketServerProtocol) -> None:\n",
    "        async def _send(id: RequestId,\n",
    "                  callback_system: CallbackSystem,\n",
    "                  callback_type: CallbackType,\n",
    "                  time: CallbackTime,\n",
    "                  response: CallbackResponse) -> None:\n",
    "            await websocket.send(json.dumps({\n",
    "                \"id\": id,\n",
    "                \"callbackSystem\": callback_system,\n",
    "                \"callbackType\": callback_type,\n",
    "                \"time\": str(time),\n",
    "                \"response\": response\n",
    "            }))\n",
    "\n",
    "        message: str\n",
    "        # TODO: parallel\n",
    "        async for message in websocket:\n",
    "            handler, request = None, None\n",
    "            try:\n",
    "                handler, request = _parse_message(methods, message)\n",
    "            except Exception as err:\n",
    "                await _send(-1, \"system\", \"ERROR\", datetime.now(), f\"Can't parse request: {traceback.format_exception(err)}\")\n",
    "            if handler is not None and request is not None:\n",
    "                asyncio.create_task(handler(request, _send))\n",
    "    \n",
    "    async with serve(process, SERVER_HOST, SERVER_PORT):\n",
    "        await asyncio.Future()  # run forever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "if (__name__ == \"__main__\") and (not IS_JUPYTER):\n",
    "    asyncio.run(server())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
