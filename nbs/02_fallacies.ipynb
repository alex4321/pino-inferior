{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fallacy detector chain\n",
    "\n",
    "> This module made to make a logical fallacy search tool for LLM-based agent.\n",
    "> So it should find stuff like ad hominem, self-contradicting statements and so on, given the discussion history and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp fallacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pino_inferior.core import DATA_DIR, PROMPTS_DIR, OPENAI_API_KEY\n",
    "from langchain.schema.runnable import RunnableSequence\n",
    "from langchain.llms.openai import BaseLLM\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains.transform import TransformChain\n",
    "from langchain.schema.messages import AIMessage, AIMessageChunk\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Union, List, Callable\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pino_inferior.message import Message\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "FALLACIES_FNAME = os.path.join(DATA_DIR, \"fallacies.json\")\n",
    "FALLACIES_PROMPT_DIR = os.path.join(PROMPTS_DIR, \"fallacies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "INPUT_FALLACIES = \"fallacies\"\n",
    "INPUT_HISTORY = \"history\"\n",
    "INPUT_CONTEXT = \"context\"\n",
    "INPUT_QUERY = \"query\"\n",
    "\n",
    "INTERMEDIATE_FALLACIES_STR = \"fallacies_str\"\n",
    "INTERMEDIATE_HISTORY_STR = \"history_str\"\n",
    "INTERMEDIATE_LAST_AUTHOR = \"last_message_author\"\n",
    "\n",
    "OUTPUT_LLM_OUTPUT = \"llm_output\"\n",
    "OUTPUT_SHORT_ANSWER = \"answer\"\n",
    "\n",
    "LLM_OUTPUT_MARKER = \"Therefore\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fallacy representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following structures represents fallacy description as well as convert it to LLM-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class FallacyExample:\n",
    "    \"\"\"\n",
    "    Example of a logical fallacy\n",
    "    \"\"\"\n",
    "    text: str # Statement\n",
    "    response: str # Fallacy detector response, explaining why it is a fallacy\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Example: {self.text}\\nExample Response: {self.response}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Fallacy:\n",
    "    \"\"\"\n",
    "    Fallacy representation\n",
    "    \"\"\"\n",
    "    name: str # Fallacy name (like \"ad hominem\" and so)\n",
    "    description: str # Fallacy description\n",
    "    example: Union[FallacyExample, None] # Fallacy example\n",
    "\n",
    "    def __str__(self):\n",
    "        result = f\"# {self.name}\\n\\n{self.description}\"\n",
    "        if self.example:\n",
    "            result += \"\\n\\n\" + str(self.example)\n",
    "        return result\n",
    "    \n",
    "\n",
    "def read_fallacies(fname: str) -> List[Fallacy]:\n",
    "    \"\"\"\n",
    "    Read the file with fallacies markup\n",
    "    :param fname: File name. Should contain JSON representing a list of `Fallacy`\n",
    "    :returns: Fallacies list\n",
    "    \"\"\"\n",
    "    with open(fname, \"r\", encoding=\"utf-8\") as src:\n",
    "        data = json.load(src)\n",
    "    result = []\n",
    "    for item in data:\n",
    "        if item.get(\"example\"):\n",
    "            example = FallacyExample(**item[\"example\"])\n",
    "        else:\n",
    "            example = None\n",
    "        fallacy = Fallacy(name=item[\"name\"], description=item[\"description\"], example=example)\n",
    "        result.append(fallacy)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will just read predefined prompt templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "system_prompt = SystemMessagePromptTemplate.from_template_file(\n",
    "    os.path.join(FALLACIES_PROMPT_DIR, \"system.txt\"),\n",
    "    input_variables=[]\n",
    ")\n",
    "instruction_prompt = HumanMessagePromptTemplate.from_template_file(\n",
    "    os.path.join(FALLACIES_PROMPT_DIR, \"instruction.txt\"),\n",
    "    input_variables=[INTERMEDIATE_FALLACIES_STR,\n",
    "                     INTERMEDIATE_HISTORY_STR,\n",
    "                     INTERMEDIATE_LAST_AUTHOR,\n",
    "                     INPUT_CONTEXT,\n",
    "                     INPUT_QUERY]\n",
    ")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt, instruction_prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### History & fallacies to strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially message history / fallacy types presented as objects.\n",
    "\n",
    "We will need to stringify them.\n",
    "\n",
    "Meanwhile we should as well check lengths and maybe cut history to make sure we will fit into LLM context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _stringify(row: dict,\n",
    "               length_function: Callable[[str], int],\n",
    "               max_fallacies_length: int,\n",
    "               max_messages_length: int) -> dict:\n",
    "    fallacies: List[Fallacy] = row[INPUT_FALLACIES]\n",
    "    history: List[Message] = row[INPUT_HISTORY] # TODO: cut\n",
    "\n",
    "    fallacies_str = \"\\n\\n\".join(map(str, fallacies))\n",
    "    while True:\n",
    "        history_str = \"\\n\\n\".join(map(str, history))\n",
    "        if length_function(history_str) <= max_messages_length:\n",
    "            break\n",
    "        else:\n",
    "            history = history[1:]\n",
    "    assert len(history) > 0, f\"History length became less than {max_messages_length} only after removing all messages\"\n",
    "    assert length_function(fallacies_str) <= max_fallacies_length, f\"Too big fallacies representation. Expected up to {max_fallacies_length}, got {length_function(fallacies_str)}\"\n",
    "\n",
    "    return {\n",
    "        INTERMEDIATE_FALLACIES_STR: fallacies_str,\n",
    "        INTERMEDIATE_HISTORY_STR: history_str,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract last user name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explicitly tell model to search for issues in the last user messages - we interested in it, aren't we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _extract_last_user(row: dict) -> dict:\n",
    "    history: List[Message] = row[INPUT_HISTORY]\n",
    "    assert len(history) > 0\n",
    "    return {\n",
    "        INTERMEDIATE_LAST_AUTHOR: history[-1].author\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short answer extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting a full answer from LLM we should extract the short form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _extract_answer_from_cot(row: dict) -> dict:\n",
    "    response: Union[AIMessage, AIMessageChunk] = row[OUTPUT_LLM_OUTPUT]\n",
    "    text: str = response.content\n",
    "    text = text.split(LLM_OUTPUT_MARKER)[-1]\n",
    "    text = text.split(\":\", maxsplit=1)[-1]\n",
    "    text = text.strip()\n",
    "    return {\n",
    "        OUTPUT_SHORT_ANSWER: text\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class LengthConfig:\n",
    "    \"\"\"\n",
    "    Fallacy detector text length configuration\n",
    "    \"\"\"\n",
    "    length_function: Callable[[str], int] # Text length getter\n",
    "    max_messages_length: int = 2048 # Max history size\n",
    "    max_fallacies_length: int = 4096 # Max fallacy list representation size\n",
    "\n",
    "\n",
    "def build_fallacy_detection_chain(llm: BaseLLM, lengths: LengthConfig) -> RunnableSequence:\n",
    "    \"\"\"\n",
    "    Build a sequential chain invoking fallacy detection\n",
    "    :param llm: Language model to use inside fallacy detector (like ChatOpenAI)\n",
    "    :param lengths: Fallacy detector text length configuration\n",
    "    :returns: Sequential chain consuming message history and returning LLM output (and extracted short answer).\n",
    "    \"\"\"\n",
    "    def _stringify_transform(row: dict) -> dict:\n",
    "        return _stringify(\n",
    "            row,\n",
    "            lengths.length_function,\n",
    "            lengths.max_fallacies_length,\n",
    "            lengths.max_messages_length,\n",
    "        )\n",
    "\n",
    "    stringify_transform = TransformChain(\n",
    "        input_variables=[INPUT_FALLACIES, INPUT_HISTORY],\n",
    "        output_variables=[INTERMEDIATE_FALLACIES_STR, INTERMEDIATE_HISTORY_STR],\n",
    "        transform=_stringify_transform,\n",
    "    )\n",
    "    extract_last_user_transform = TransformChain(\n",
    "        input_variables=[INPUT_HISTORY],\n",
    "        output_variables=[INTERMEDIATE_LAST_AUTHOR],\n",
    "        transform=_extract_last_user,\n",
    "    )\n",
    "    extract_answer_transform = TransformChain(\n",
    "        input_variables=[OUTPUT_LLM_OUTPUT],\n",
    "        output_variables=[OUTPUT_SHORT_ANSWER],\n",
    "        transform=_extract_answer_from_cot,\n",
    "    )\n",
    "    return stringify_transform | \\\n",
    "        extract_last_user_transform | \\\n",
    "        chat_prompt | \\\n",
    "        llm | \\\n",
    "        extract_answer_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4-0613\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    streaming=True,\n",
    ")\n",
    "encoding = tiktoken.encoding_for_model(llm.model_name)\n",
    "fallacies_detection_chain = build_fallacy_detection_chain(\n",
    "    llm,\n",
    "    LengthConfig(\n",
    "        lambda text: len(encoding.encode(text)),\n",
    "        max_messages_length=2048,\n",
    "        max_fallacies_length=2048 + 1024,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallacies = read_fallacies(FALLACIES_FNAME)\n",
    "history = [\n",
    "    Message(\"Moonlight\", datetime.now(),\n",
    "            \"Soon we will finish with Ukraine\"),\n",
    "    Message(\"alex4321\", datetime.now(),\n",
    "            \"After six months of taking Bakhmut, did anything new happen?\\n\\n\" + \\\n",
    "                \"Well, so that there is a reason to suspect that it will happen soon, \" + \\\n",
    "                \"and not something regardless of the outcome - this will last for years.\"),\n",
    "    Message(\"Moonlight\", datetime.now(),\n",
    "            \"Time is a resource, we have plenty of it\")\n",
    "]\n",
    "context = \"Post about the war between Russia/Ukraine\"\n",
    "query = \"Moonlight's argument about time being a resource in war\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_output': AIMessageChunk(content='- Possible Fallacies in Moonlight\\'s messages:\\n  - Argumentum ad Ignorantiam (Appeal to Ignorance):\\n    Moonlight could be asserting that Russia will eventually finish with Ukraine because no evidence has been presented to prove otherwise. However, this might not be the case as Moonlight\\'s statement could also be seen as a prediction rather than a claim of inevitable truth.\\n  - Hasty Generalization:\\n    Moonlight might be making a hasty generalization by asserting that time is a resource that Russia has in abundance. This ignores other potential factors that could influence the outcome of the conflict, such as resource limitations, international pressure, or changes in strategy.\\n  - Argumentum ad Baculum (Appeal to Force):\\n    There\\'s a potential for an appeal to force if Moonlight\\'s statement \"Soon we will finish with Ukraine\" is implying that Russia\\'s military might will inevitably lead to victory. However, this is a bit of a stretch as the phrase could simply be expressing confidence in their outcome.\\n\\nTherefore, the answer is:\\n  - Identified Fallacy 1: Hasty Generalization\\n    Moonlight\\'s statement that \"Time is a resource, we have plenty of it\" oversimplifies the complex nature of war and the many factors that can influence its outcome. It\\'s a hasty generalization to imply that having plenty of time guarantees victory.\\n  - Identified Fallacy 2: None\\n    While there\\'s a potential for an appeal to ignorance or force in Moonlight\\'s statements, it\\'s not definitive enough to classify as a fallacy. The phrases could be interpreted in different ways and lack the certainty needed to be considered a logical fallacy.'),\n",
       " 'answer': '- Identified Fallacy 1: Hasty Generalization\\n    Moonlight\\'s statement that \"Time is a resource, we have plenty of it\" oversimplifies the complex nature of war and the many factors that can influence its outcome. It\\'s a hasty generalization to imply that having plenty of time guarantees victory.\\n  - Identified Fallacy 2: None\\n    While there\\'s a potential for an appeal to ignorance or force in Moonlight\\'s statements, it\\'s not definitive enough to classify as a fallacy. The phrases could be interpreted in different ways and lack the certainty needed to be considered a logical fallacy.'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fallacies_detection_chain.invoke({\n",
    "    \"fallacies\": fallacies,\n",
    "    \"history\": history,\n",
    "    \"context\": context,\n",
    "    \"query\": query,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformChain's atransform is not provided, falling back to synchronous transform\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'llm_output': AIMessageChunk(content='- Possible Fallacies in Moonlight\\'s messages:\\n  - Argumentum ad Ignorantiam (Appeal to Ignorance):\\n    This fallacy could potentially be present in Moonlight\\'s messages. In their final statement, they claim that \"Time is a resource, we have plenty of it.\" This could be interpreted as an appeal to ignorance, as they\\'re implying that simply having time on their side means they will achieve their goal, without providing evidence to support this claim.\\n  - Argumentum ad Populum (Appeal to Popular Opinion):\\n    This fallacy is not present in Moonlight\\'s messages. They don\\'t make any references to popular opinion or majority belief in their statements.\\n  - Argumentum ad Hominem (Personal Attack):\\n    This fallacy is not present. Moonlight does not attack anyone\\'s character in their statements.\\n- Therefore, the answer is:\\n  - Identified Fallacy 1: Argumentum ad Ignorantiam (Appeal to Ignorance)\\n    Moonlight\\'s assertion that time is plentiful and therefore they will achieve their goal may be appealing, but it lacks concrete evidence or reasoning. It suggests that because they potentially have more time, they will inevitably succeed, which is not necessarily the case in reality.'),\n",
       " 'answer': \"- Identified Fallacy 1: Argumentum ad Ignorantiam (Appeal to Ignorance)\\n    Moonlight's assertion that time is plentiful and therefore they will achieve their goal may be appealing, but it lacks concrete evidence or reasoning. It suggests that because they potentially have more time, they will inevitably succeed, which is not necessarily the case in reality.\"}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await fallacies_detection_chain.ainvoke({\n",
    "    \"fallacies\": fallacies,\n",
    "    \"history\": history,\n",
    "    \"context\": context,\n",
    "    \"query\": query,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
