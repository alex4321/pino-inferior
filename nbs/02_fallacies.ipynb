{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fallacy detector\n",
    "\n",
    "> This module made to make a logical fallacy search tool for LLM-based agent. So it should find stuff like ad hominem, self-contradicting statements and so on, given the discussion history and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp fallacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pino_inferior.core import DATA_DIR, PROMPTS_DIR, OPENAI_API_KEY\n",
    "from langchain.schema.runnable import RunnableSequence\n",
    "from langchain.llms.openai import BaseLLM\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains.transform import TransformChain\n",
    "from langchain.schema.messages import AIMessage, AIMessageChunk\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Union, List, Callable\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pino_inferior.message import Message\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "FALLACIES_FNAME = os.path.join(DATA_DIR, \"fallacies.json\")\n",
    "FALLACIES_PROMPT_DIR = os.path.join(PROMPTS_DIR, \"fallacies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "INPUT_FALLACIES = \"fallacies\"\n",
    "INPUT_HISTORY = \"history\"\n",
    "INPUT_CONTEXT = \"context\"\n",
    "INPUT_QUERY = \"query\"\n",
    "\n",
    "INTERMEDIATE_FALLACIES_STR = \"fallacies_str\"\n",
    "INTERMEDIATE_HISTORY_STR = \"history_str\"\n",
    "INTERMEDIATE_LAST_AUTHOR = \"last_message_author\"\n",
    "\n",
    "OUTPUT_LLM_OUTPUT = \"llm_output\"\n",
    "OUTPUT_SHORT_ANSWER = \"answer\"\n",
    "\n",
    "LLM_OUTPUT_MARKER = \"Therefore\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fallacy representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following structures represents fallacy description as well as convert it to LLM-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class FallacyExample:\n",
    "    \"\"\"\n",
    "    Example of a logical fallacy\n",
    "    \"\"\"\n",
    "    text: str # Statement\n",
    "    response: str # Fallacy detector response, explaining why it is a fallacy\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Example: {self.text}\\nExample Response: {self.response}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Fallacy:\n",
    "    \"\"\"\n",
    "    Fallacy representation\n",
    "    \"\"\"\n",
    "    name: str # Fallacy name (like \"ad hominem\" and so)\n",
    "    description: str # Fallacy description\n",
    "    example: Union[FallacyExample, None] # Fallacy example\n",
    "\n",
    "    def __str__(self):\n",
    "        result = f\"# {self.name}\\n\\n{self.description}\"\n",
    "        if self.example:\n",
    "            result += \"\\n\\n\" + str(self.example)\n",
    "        return result\n",
    "    \n",
    "\n",
    "def read_fallacies(fname: str) -> List[Fallacy]:\n",
    "    \"\"\"\n",
    "    Read the file with fallacies markup\n",
    "    :param fname: File name. Should contain JSON representing a list of `Fallacy`\n",
    "    :returns: Fallacies list\n",
    "    \"\"\"\n",
    "    with open(fname, \"r\", encoding=\"utf-8\") as src:\n",
    "        data = json.load(src)\n",
    "    result = []\n",
    "    for item in data:\n",
    "        if item.get(\"example\"):\n",
    "            example = FallacyExample(**item[\"example\"])\n",
    "        else:\n",
    "            example = None\n",
    "        fallacy = Fallacy(name=item[\"name\"], description=item[\"description\"], example=example)\n",
    "        result.append(fallacy)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will just read predefined prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'fallacies_str', 'history_str', 'last_message_author', 'query'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"You are a logical fallacy search subsystem.\\nYou're going to help us debating different topics, so you need to find our opponents potential weak points.\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'fallacies_str', 'history_str', 'last_message_author', 'query'], template=\"Fallacies to find (do not use thes examples):\\n<Fallacies>\\n{fallacies_str}\\n</Fallacies>\\n\\nAdditional context:\\n<Context>\\n{context}\\n</Context>\\n\\nChat history:\\n<Chat>\\n{history_str}\\n</Chat>\\n\\nQuery:\\n<Query>\\n{query}\\n</Query>\\n\\nNow, review {last_message_author}'s messages, paying close attention to their last one. If query provided - pay attention to it.\\n\\nYour task is to identify any potential faults or fallacies.\\n\\nIf you identify any fallacies from the given list, provide a brief explanation. If none are found, specify that as well.\\n\\nFramework:\\n- Possible Fallacies in {last_message_author}'s messages:\\n  - Fallacy Name 1:\\n    Describe whether this fallacy could be present in the messages, and provide an explanation. Be concise.  \\n  - Fallacy Name M:\\n    As above, explore the potential presence of this fallacy and explain your reasoning.\\n- (Use this market as is) Therefore, the answer is:\\n  - Identified Fallacy 1:\\n    Give a short explanation of why you identified this as a fallacy.\\n  - Identified Fallacy K:\\n    Provide a brief summary of your findings.\"))])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "system_prompt = SystemMessagePromptTemplate.from_template_file(\n",
    "    os.path.join(FALLACIES_PROMPT_DIR, \"system.txt\"),\n",
    "    input_variables=[]\n",
    ")\n",
    "instruction_prompt = HumanMessagePromptTemplate.from_template_file(\n",
    "    os.path.join(FALLACIES_PROMPT_DIR, \"instruction.txt\"),\n",
    "    input_variables=[INTERMEDIATE_FALLACIES_STR,\n",
    "                     INTERMEDIATE_HISTORY_STR,\n",
    "                     INTERMEDIATE_LAST_AUTHOR,\n",
    "                     INPUT_CONTEXT,\n",
    "                     INPUT_QUERY]\n",
    ")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt, instruction_prompt])\n",
    "chat_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### History & fallacies to strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially message history / fallacy types presented as objects.\n",
    "\n",
    "We will need to stringify them.\n",
    "\n",
    "Meanwhile we should as well check lengths and maybe cut history to make sure we will fit into LLM context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _stringify(row: dict,\n",
    "               length_function: Callable[[str], int],\n",
    "               max_fallacies_length: int,\n",
    "               max_messages_length: int) -> dict:\n",
    "    fallacies: List[Fallacy] = row[INPUT_FALLACIES]\n",
    "    history: List[Message] = row[INPUT_HISTORY] # TODO: cut\n",
    "\n",
    "    fallacies_str = \"\\n\\n\".join(map(str, fallacies))\n",
    "    while True:\n",
    "        history_str = \"\\n\\n\".join(map(str, history))\n",
    "        if length_function(history_str) <= max_messages_length:\n",
    "            break\n",
    "        else:\n",
    "            history = history[1:]\n",
    "    assert len(history) > 0, f\"History length became less than {max_messages_length} only after removing all messages\"\n",
    "    assert length_function(fallacies_str) <= max_fallacies_length, f\"Too big fallacies representation. Expected up to {max_fallacies_length}, got {length_function(fallacies_str)}\"\n",
    "\n",
    "    return {\n",
    "        INTERMEDIATE_FALLACIES_STR: fallacies_str,\n",
    "        INTERMEDIATE_HISTORY_STR: history_str,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract last user name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explicitly tell model to search for issues in the last user messages - we interested in it, aren't we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _extract_last_user(row: dict) -> dict:\n",
    "    history: List[Message] = row[INPUT_HISTORY]\n",
    "    assert len(history) > 0\n",
    "    return {\n",
    "        INTERMEDIATE_LAST_AUTHOR: history[-1].author\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short answer extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting a full answer from LLM we should extract the short form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _extract_answer_from_cot(row: dict) -> dict:\n",
    "    response: Union[AIMessage, AIMessageChunk] = row[OUTPUT_LLM_OUTPUT]\n",
    "    text: str = response.content\n",
    "    text = text.split(LLM_OUTPUT_MARKER)[-1]\n",
    "    text = text.split(\":\", maxsplit=1)[-1]\n",
    "    text = text.strip()\n",
    "    return {\n",
    "        OUTPUT_SHORT_ANSWER: text\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class LengthConfig:\n",
    "    \"\"\"\n",
    "    Fallacy detector text length configuration\n",
    "    \"\"\"\n",
    "    length_function: Callable[[str], int] # Text length getter\n",
    "    max_messages_length: int = 2048 # Max history size\n",
    "    max_fallacies_length: int = 4096 # Max fallacy list representation size\n",
    "\n",
    "\n",
    "def build_fallacy_detection_chain(llm: BaseLLM, lengths: LengthConfig) -> RunnableSequence:\n",
    "    \"\"\"\n",
    "    Build a sequential chain invoking fallacy detection\n",
    "    :param llm: Language model to use inside fallacy detector (like ChatOpenAI)\n",
    "    :param lengths: Fallacy detector text length configuration\n",
    "    :returns: Sequential chain consuming message history and returning LLM output (and extracted short answer).\n",
    "    \"\"\"\n",
    "    def _stringify_transform(row: dict) -> dict:\n",
    "        return _stringify(\n",
    "            row,\n",
    "            lengths.length_function,\n",
    "            lengths.max_fallacies_length,\n",
    "            lengths.max_messages_length,\n",
    "        )\n",
    "\n",
    "    stringify_transform = TransformChain(\n",
    "        input_variables=[INPUT_FALLACIES, INPUT_HISTORY],\n",
    "        output_variables=[INTERMEDIATE_FALLACIES_STR, INTERMEDIATE_HISTORY_STR],\n",
    "        transform=_stringify_transform,\n",
    "    )\n",
    "    extract_last_user_transform = TransformChain(\n",
    "        input_variables=[INPUT_HISTORY],\n",
    "        output_variables=[INTERMEDIATE_LAST_AUTHOR],\n",
    "        transform=_extract_last_user,\n",
    "    )\n",
    "    extract_answer_transform = TransformChain(\n",
    "        input_variables=[OUTPUT_LLM_OUTPUT],\n",
    "        output_variables=[OUTPUT_SHORT_ANSWER],\n",
    "        transform=_extract_answer_from_cot,\n",
    "    )\n",
    "    return stringify_transform | \\\n",
    "        extract_last_user_transform | \\\n",
    "        chat_prompt | \\\n",
    "        llm | \\\n",
    "        extract_answer_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4-0613\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    streaming=True,\n",
    ")\n",
    "encoding = tiktoken.encoding_for_model(llm.model_name)\n",
    "fallacies_detection_chain = build_fallacy_detection_chain(\n",
    "    llm,\n",
    "    LengthConfig(\n",
    "        lambda text: len(encoding.encode(text)),\n",
    "        max_messages_length=2048,\n",
    "        max_fallacies_length=2048 + 1024,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallacies = read_fallacies(FALLACIES_FNAME)\n",
    "history = [\n",
    "    Message(\"Moonlight\", datetime.now(),\n",
    "            \"Soon we will finish with Ukraine\"),\n",
    "    Message(\"alex4321\", datetime.now(),\n",
    "            \"After six months of taking Bakhmut, did anything new happen?\\n\\n\" + \\\n",
    "                \"Well, so that there is a reason to suspect that it will happen soon, \" + \\\n",
    "                \"and not something regardless of the outcome - this will last for years.\"),\n",
    "    Message(\"Moonlight\", datetime.now(),\n",
    "            \"Time is a resource, we have plenty of it\")\n",
    "]\n",
    "context = \"Post about the war between Russia/Ukraine\"\n",
    "query = \"Moonlight's argument about time being a resource in war\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_output': AIMessageChunk(content='- Possible Fallacies in Moonlight\\'s messages:\\n  - Argumentum ad Ignorantiam (Appeal to Ignorance):\\n    Moonlight\\'s assertion that \"Time is a resource, we have plenty of it\" could be seen as an appeal to ignorance. This statement assumes that because it has not been proven that time is not a plentiful resource in this context, it must be plentiful.\\n  - Argumentum ad Baculum (Appeal to Force):\\n    The statement \"Soon we will finish with Ukraine\" can be perceived as an appeal to force. Moonlight attempts to establish the inevitability of their position, not supported by logic or evidence but by the implied threat of force.\\n  - Hasty Generalization:\\n    Moonlight seems to make a hasty generalization when they say \"Soon we will finish with Ukraine\". This statement assumes that previous victories guarantee future success, without considering other variables that could affect the outcome of the war.\\n\\nTherefore, the answer is:\\n  - Identified Fallacy 1: Argumentum ad Ignorantiam (Appeal to Ignorance)\\n    Moonlight\\'s statement about time being a plentiful resource is a fallacy because it rests on the absence of evidence to the contrary, rather than providing evidence to support the claim.\\n  - Identified Fallacy 2: Argumentum ad Baculum (Appeal to Force)\\n    The assertion \"Soon we will finish with Ukraine\" is a fallacy because it relies on the threat of force rather than evidence or logical reasoning.\\n  - Identified Fallacy 3: Hasty Generalization\\n    The claim about finishing with Ukraine soon is a fallacy because it overgeneralizes the situation, assuming that past victories necessarily predict future success.'),\n",
       " 'answer': '- Identified Fallacy 1: Argumentum ad Ignorantiam (Appeal to Ignorance)\\n    Moonlight\\'s statement about time being a plentiful resource is a fallacy because it rests on the absence of evidence to the contrary, rather than providing evidence to support the claim.\\n  - Identified Fallacy 2: Argumentum ad Baculum (Appeal to Force)\\n    The assertion \"Soon we will finish with Ukraine\" is a fallacy because it relies on the threat of force rather than evidence or logical reasoning.\\n  - Identified Fallacy 3: Hasty Generalization\\n    The claim about finishing with Ukraine soon is a fallacy because it overgeneralizes the situation, assuming that past victories necessarily predict future success.'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fallacies_detection_chain.invoke({\n",
    "    \"fallacies\": fallacies,\n",
    "    \"history\": history,\n",
    "    \"context\": context,\n",
    "    \"query\": query,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TransformChain's atransform is not provided, falling back to synchronous transform\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'llm_output': AIMessageChunk(content='- Possible Fallacies in Moonlight\\'s messages:\\n  - Argumentum ad Ignorantiam (Appeal to Ignorance):\\n    Moonlight\\'s claim that \"time is a resource, we have plenty of it\" may be perceived as an appeal to ignorance. He suggests that because we don\\'t know when the war will end, it\\'s okay to assume it could last indefinitely.\\n  - Argumentum ad Baculum (Appeal to Force):\\n    Depending on the context, the statement \"soon we will finish with Ukraine\" could be interpreted as an appeal to force - implying that they will win the war through sheer force, without providing a logical or strategic argument.\\n  - Hasty Generalization:\\n    The assertion \"soon we will finish with Ukraine\" could be a hasty generalization, as it assumes a conclusion about the war\\'s outcome without providing substantial evidence or considering potential variables.\\n- Therefore, the answer is:\\n  - Identified Fallacy 1: Argumentum ad Ignorantiam (Appeal to Ignorance)\\n    Moonlight\\'s assertion that time is a plentiful resource in this context is an appeal to ignorance. This statement lacks a logical basis and seems to ignore the complexities and uncertainties of warfare.\\n  - Identified Fallacy 2: Argumentum ad Baculum (Appeal to Force)\\n    The claim \"soon we will finish with Ukraine\" can be seen as an appeal to force. It\\'s a declaration of victory without providing a logical or strategic argument. It could be seen as an attempt to intimidate or impose a certain perspective through the projection of power.\\n  - Identified Fallacy 3: Hasty Generalization\\n    The statement \"soon we will finish with Ukraine\" can be considered a hasty generalization. It assumes a specific outcome without providing substantial evidence or considering the complexities and variables present in the situation.'),\n",
       " 'answer': '- Identified Fallacy 1: Argumentum ad Ignorantiam (Appeal to Ignorance)\\n    Moonlight\\'s assertion that time is a plentiful resource in this context is an appeal to ignorance. This statement lacks a logical basis and seems to ignore the complexities and uncertainties of warfare.\\n  - Identified Fallacy 2: Argumentum ad Baculum (Appeal to Force)\\n    The claim \"soon we will finish with Ukraine\" can be seen as an appeal to force. It\\'s a declaration of victory without providing a logical or strategic argument. It could be seen as an attempt to intimidate or impose a certain perspective through the projection of power.\\n  - Identified Fallacy 3: Hasty Generalization\\n    The statement \"soon we will finish with Ukraine\" can be considered a hasty generalization. It assumes a specific outcome without providing substantial evidence or considering the complexities and variables present in the situation.'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await fallacies_detection_chain.ainvoke({\n",
    "    \"fallacies\": fallacies,\n",
    "    \"history\": history,\n",
    "    \"context\": context,\n",
    "    \"query\": query,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
