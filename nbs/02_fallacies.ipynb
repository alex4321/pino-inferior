{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fallacy detector chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp fallacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pino_inferior.core import DATA_DIR, PROMPTS_DIR, OPENAI_API_KEY\n",
    "from langchain.schema.runnable import RunnableSequence\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.llms.openai import BaseLLM\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains.transform import TransformChain\n",
    "from langchain.schema.messages import AIMessage, AIMessageChunk\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Union, List\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pino_inferior.message import Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "FALLACIES_FNAME = os.path.join(DATA_DIR, \"fallacies.json\")\n",
    "FALLACIES_PROMPT_DIR = os.path.join(PROMPTS_DIR, \"fallacies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "INPUT_FALLACIES = \"fallacies\"\n",
    "INPUT_HISTORY = \"history\"\n",
    "INPUT_CONTEXT = \"context\"\n",
    "INPUT_QUERY = \"query\"\n",
    "\n",
    "INTERMEDIATE_FALLACIES_STR = \"fallacies_str\"\n",
    "INTERMEDIATE_HISTORY_STR = \"history_str\"\n",
    "INTERMEDIATE_LAST_AUTHOR = \"last_message_author\"\n",
    "\n",
    "OUTPUT_LLM_OUTPUT = \"llm_output\"\n",
    "OUTPUT_SHORT_ANSWER = \"answer\"\n",
    "\n",
    "LLM_OUTPUT_MARKER = \"Therefore\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fallacy representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class FallacyExample:\n",
    "    text: str\n",
    "    response: str\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Example: {self.text}\\nExample Response: {self.response}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Fallacy:\n",
    "    name: str\n",
    "    description: str\n",
    "    example: Union[FallacyExample, None]\n",
    "\n",
    "    def __str__(self):\n",
    "        result = f\"# {self.name}\\n\\n{self.description}\"\n",
    "        if self.example:\n",
    "            result += \"\\n\\n\" + str(self.example)\n",
    "        return result\n",
    "    \n",
    "\n",
    "def read_fallacies(fname: str) -> List[Fallacy]:\n",
    "    with open(fname, \"r\", encoding=\"utf-8\") as src:\n",
    "        data = json.load(src)\n",
    "    result = []\n",
    "    for item in data:\n",
    "        if item.get(\"example\"):\n",
    "            example = FallacyExample(**item[\"example\"])\n",
    "        else:\n",
    "            example = None\n",
    "        fallacy = Fallacy(name=item[\"name\"], description=item[\"description\"], example=example)\n",
    "        result.append(fallacy)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "system_prompt = SystemMessagePromptTemplate.from_template_file(\n",
    "    os.path.join(FALLACIES_PROMPT_DIR, \"system.txt\"),\n",
    "    input_variables=[]\n",
    ")\n",
    "instruction_prompt = HumanMessagePromptTemplate.from_template_file(\n",
    "    os.path.join(FALLACIES_PROMPT_DIR, \"instruction.txt\"),\n",
    "    input_variables=[INTERMEDIATE_FALLACIES_STR,\n",
    "                     INTERMEDIATE_HISTORY_STR,\n",
    "                     INTERMEDIATE_LAST_AUTHOR,\n",
    "                     INPUT_CONTEXT,\n",
    "                     INPUT_QUERY]\n",
    ")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt, instruction_prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### History & fallacies to strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially message history / fallacy types presented as objects.\n",
    "\n",
    "We will need to stringify them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Cut too long message lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def stringify(row):\n",
    "    fallacies: List[Fallacy] = row[INPUT_FALLACIES]\n",
    "    history: List[Message] = row[INPUT_HISTORY] # TODO: cut\n",
    "    return {\n",
    "        INTERMEDIATE_FALLACIES_STR: \"\\n\\n\".join(map(str, fallacies)),\n",
    "        INTERMEDIATE_HISTORY_STR: \"\\n\\n\".join(map(str, history))\n",
    "    }\n",
    "\n",
    "async def astringify(row):\n",
    "    return stringify(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract last user name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explicitly tell model to search for issues in the last user messages - we interested in it, aren't we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_last_user(row):\n",
    "    history: List[Message] = row[INPUT_HISTORY]\n",
    "    assert len(history) > 0\n",
    "    return {\n",
    "        INTERMEDIATE_LAST_AUTHOR: history[-1].author\n",
    "    }\n",
    "\n",
    "async def aextract_last_user(row):\n",
    "    return extract_last_user(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short answer extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting a full answer from LLM we should extract the short form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_answer_from_cot(row):\n",
    "    response: Union[AIMessage, AIMessageChunk] = row[OUTPUT_LLM_OUTPUT]\n",
    "    text: str = response.content\n",
    "    text = text.split(LLM_OUTPUT_MARKER)[-1]\n",
    "    text = text.split(\":\", maxsplit=1)[-1]\n",
    "    text = text.strip()\n",
    "    return {\n",
    "        OUTPUT_SHORT_ANSWER: text\n",
    "    }\n",
    "\n",
    "async def aextract_answer_from_cot(row):\n",
    "    return extract_answer_from_cot(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_fallacy_detection_chain(llm: BaseLLM) -> RunnableSequence:\n",
    "    stringify_transform = TransformChain(\n",
    "        input_variables=[INPUT_FALLACIES, INPUT_HISTORY],\n",
    "        output_variables=[INTERMEDIATE_FALLACIES_STR, INTERMEDIATE_HISTORY_STR],\n",
    "        transform=stringify,\n",
    "        atransform=astringify,\n",
    "    )\n",
    "    extract_last_user_transform = TransformChain(\n",
    "        input_variables=[INPUT_HISTORY],\n",
    "        output_variables=[INTERMEDIATE_LAST_AUTHOR],\n",
    "        transform=extract_last_user,\n",
    "        atransform=aextract_last_user,\n",
    "    )\n",
    "    extract_answer_transform = TransformChain(\n",
    "        input_variables=[OUTPUT_LLM_OUTPUT],\n",
    "        output_variables=[OUTPUT_SHORT_ANSWER],\n",
    "        transform=extract_answer_from_cot,\n",
    "        atransform=aextract_answer_from_cot,\n",
    "    )\n",
    "    return stringify_transform | \\\n",
    "        extract_last_user_transform | \\\n",
    "        chat_prompt | \\\n",
    "        llm | \\\n",
    "        extract_answer_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallacies_detection_chain = build_fallacy_detection_chain(\n",
    "    ChatOpenAI(\n",
    "        model_name=\"gpt-4-0613\",\n",
    "        openai_api_key=OPENAI_API_KEY,\n",
    "        streaming=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallacies = read_fallacies(FALLACIES_FNAME)\n",
    "history = [\n",
    "    Message(\"Moonlight\", datetime.now(),\n",
    "            \"Soon we will finish with Ukraine\"),\n",
    "    Message(\"alex4321\", datetime.now(),\n",
    "            \"After six months of taking Bakhmut, did anything new happen?\\n\\n\" + \\\n",
    "                \"Well, so that there is a reason to suspect that it will happen soon, \" + \\\n",
    "                \"and not something regardless of the outcome - this will last for years.\"),\n",
    "    Message(\"Moonlight\", datetime.now(),\n",
    "            \"Time is a resource, we have plenty of it\")\n",
    "]\n",
    "context = \"Post about the war between Russia/Ukraine\"\n",
    "query = \"Moonlight's argument about time being a resource in war\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_output': AIMessageChunk(content='- Possible Fallacies in Moonlight\\'s messages:\\n  - Argumentum ad Ignorantiam (Appeal to Ignorance):\\n    This fallacy could be present if Moonlight is implying that because no one has proven that Russia will not finish with Ukraine soon, it must be true. Moonlight\\'s vague statement about time being a resource could be seen as avoiding the need to provide proof for his claim.\\n  - Argumentum ad Baculum (Appeal to Force):\\n    If Moonlight\\'s statement \"Soon we will finish with Ukraine\" is interpreted as a threat or an attempt to intimidate, it could represent this fallacy. However, without further context and without knowing Moonlight\\'s position or influence, it\\'s hard to definitively identify this fallacy.\\n  - False Dichotomy: \\n    Moonlight\\'s message doesn\\'t present only two choices as the only options, so this fallacy is likely not present.\\n  - Begging the Question (Circular Reasoning):\\n    Moonlight\\'s statement does not assume the conclusion in the premises, so this fallacy is probably not present.\\n  - Straw Man Fallacy:\\n    Moonlight\\'s message doesn\\'t appear to misrepresent an argument to make it easier to attack, so this fallacy is not likely present.\\n  - Red Herring:\\n    Moonlight\\'s message does not seem to provide irrelevant information to distract from the main issue, so this fallacy is not likely present.\\n  - Hasty Generalization:\\n    Moonlight\\'s message does not draw a conclusion based on insufficient or unrepresentative evidence, so this fallacy is not likely present.\\n  - Equivocation:\\n    Moonlight\\'s use of the phrase \"Time is a resource, we have plenty of it\" could be interpreted in more than one way and might be used to mislead or confuse. However, without additional context, it\\'s hard to definitively identify this as a fallacy.\\n  - Contradicting (Inconsistent) statements the same author:\\n    Moonlight\\'s messages do not seem to contradict each other, so this fallacy is likely not present.\\n  - Misleading Vividness:\\n    Moonlight\\'s messages do not present a small number of dramatic events as outweighing a significant amount of statistical evidence, so this fallacy is likely not present.\\n  - Appeal to Fear:\\n    Moonlight\\'s messages do not appear to increase fear towards an alternative to create support for an idea, so this fallacy is not likely present.\\n\\nTherefore, the answer is:\\n  - Identified Fallacy 1: Argumentum ad Ignorantiam (Appeal to Ignorance):\\n    Moonlight\\'s statement about time being a resource could be seen as avoiding the need to provide proof for his claim that Russia will soon finish with Ukraine. This avoidance could be seen as an appeal to ignorance.\\n  - Identified Fallacy 2: Equivocation:\\n    Moonlight\\'s use of the phrase \"Time is a resource, we have plenty of it\" could be interpreted in more than one way and might be used to mislead or confuse. However, without additional context, it\\'s hard to definitively identify this as a fallacy. This fallacy is identified with low confidence.'),\n",
       " 'answer': '- Identified Fallacy 1: Argumentum ad Ignorantiam (Appeal to Ignorance):\\n    Moonlight\\'s statement about time being a resource could be seen as avoiding the need to provide proof for his claim that Russia will soon finish with Ukraine. This avoidance could be seen as an appeal to ignorance.\\n  - Identified Fallacy 2: Equivocation:\\n    Moonlight\\'s use of the phrase \"Time is a resource, we have plenty of it\" could be interpreted in more than one way and might be used to mislead or confuse. However, without additional context, it\\'s hard to definitively identify this as a fallacy. This fallacy is identified with low confidence.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fallacies_detection_chain.invoke({\n",
    "    \"fallacies\": fallacies,\n",
    "    \"history\": history,\n",
    "    \"context\": context,\n",
    "    \"query\": query,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_output': AIMessageChunk(content='- Possible Fallacies in Moonlight\\'s messages:\\n  - Argumentum ad Ignorantiam (Appeal to Ignorance):\\n    Moonlight asserts that \"Time is a resource, we have plenty of it\" in the context of war. However, they provide no evidence or justification for this assertion. It implies that because no one has proven the contrary, their statement must be true.\\n  - Begging the Question (Circular Reasoning):\\n    Moonlight\\'s statement that \"we have plenty of time\" assumes the conclusion within the premise, that they have an unlimited amount of time, without providing any evidence or explanation.\\n  - Argumentum ad Baculum (Appeal to Force):\\n    Moonlight\\'s statement may imply a veiled threat that they can keep fighting indefinitely due to their perceived abundance of time. However, this doesn\\'t contribute to an informed discussion about the war situation and its potential resolution.\\n  - Misleading Vividness:\\n    Moonlight\\'s statement could be seen as an instance of Misleading Vividness, as it presents a vivid, but unsubstantiated claim without providing supporting evidence or context.\\n\\n- Therefore, the answer is:\\n  - Identified Fallacy 1: Argumentum ad Ignorantiam (Appeal to Ignorance)\\n    Moonlight\\'s claim that \"time is a resource, we have plenty of it\" is unsupported, implying that since no one has proven otherwise, this assertion must be true.\\n  - Identified Fallacy 2: Begging the Question (Circular Reasoning)\\n    The assertion of having plenty of time is presented without any evidence or explanation, assuming the conclusion within the premise.\\n  - Identified Fallacy 3: Argumentum ad Baculum (Appeal to Force)\\n    The suggestion that the war can continue indefinitely due to an abundance of time could be seen as a veiled threat, which doesn\\'t contribute to a logical argument or discussion.\\n  - Identified Fallacy 4: Misleading Vividness\\n    The claim about having plenty of time is presented without context or evidence, making it potentially misleading.'),\n",
       " 'answer': '- Identified Fallacy 1: Argumentum ad Ignorantiam (Appeal to Ignorance)\\n    Moonlight\\'s claim that \"time is a resource, we have plenty of it\" is unsupported, implying that since no one has proven otherwise, this assertion must be true.\\n  - Identified Fallacy 2: Begging the Question (Circular Reasoning)\\n    The assertion of having plenty of time is presented without any evidence or explanation, assuming the conclusion within the premise.\\n  - Identified Fallacy 3: Argumentum ad Baculum (Appeal to Force)\\n    The suggestion that the war can continue indefinitely due to an abundance of time could be seen as a veiled threat, which doesn\\'t contribute to a logical argument or discussion.\\n  - Identified Fallacy 4: Misleading Vividness\\n    The claim about having plenty of time is presented without context or evidence, making it potentially misleading.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await fallacies_detection_chain.ainvoke({\n",
    "    \"fallacies\": fallacies,\n",
    "    \"history\": history,\n",
    "    \"context\": context,\n",
    "    \"query\": query,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alex4321\\anaconda3\\envs\\pino-inferior\\Lib\\site-packages\\nbdev\\export.py:73: UserWarning: Notebook 'f:\\Projects\\pino-inferior\\nbs\\07_agent.ipynb' uses `#|export` without `#|default_exp` cell.\n",
      "Note nbdev2 no longer supports nbdev1 syntax. Run `nbdev_migrate` to upgrade.\n",
      "See https://nbdev.fast.ai/getting_started.html for more information.\n",
      "  warn(f\"Notebook '{nbname}' uses `#|export` without `#|default_exp` cell.\\n\"\n"
     ]
    }
   ],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
