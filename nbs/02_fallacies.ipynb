{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fallacy detector chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp fallacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pino_inferior.core import DATA_DIR, PROMPTS_DIR, OPENAI_API_KEY\n",
    "from langchain.schema.runnable import RunnableSequence\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.llms.openai import BaseLLM\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains.transform import TransformChain\n",
    "from langchain.schema.messages import AIMessage, AIMessageChunk\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Union, List\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pino_inferior.message import Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "FALLACIES_FNAME = os.path.join(DATA_DIR, \"fallacies.json\")\n",
    "FALLACIES_PROMPT_DIR = os.path.join(PROMPTS_DIR, \"fallacies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "INPUT_FALLACIES = \"fallacies\"\n",
    "INPUT_HISTORY = \"history\"\n",
    "INPUT_CONTEXT = \"context\"\n",
    "INPUT_QUERY = \"query\"\n",
    "\n",
    "INTERMEDIATE_FALLACIES_STR = \"fallacies_str\"\n",
    "INTERMEDIATE_HISTORY_STR = \"history_str\"\n",
    "INTERMEDIATE_LAST_AUTHOR = \"last_message_author\"\n",
    "\n",
    "OUTPUT_LLM_OUTPUT = \"llm_output\"\n",
    "OUTPUT_SHORT_ANSWER = \"answer\"\n",
    "\n",
    "LLM_OUTPUT_MARKER = \"Therefore\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fallacy representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class FallacyExample:\n",
    "    text: str\n",
    "    response: str\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Example: {self.text}\\nExample Response: {self.response}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Fallacy:\n",
    "    name: str\n",
    "    description: str\n",
    "    example: Union[FallacyExample, None]\n",
    "\n",
    "    def __str__(self):\n",
    "        result = f\"# {self.name}\\n\\n{self.description}\"\n",
    "        if self.example:\n",
    "            result += \"\\n\\n\" + str(self.example)\n",
    "        return result\n",
    "    \n",
    "\n",
    "def read_fallacies(fname: str) -> List[Fallacy]:\n",
    "    with open(fname, \"r\", encoding=\"utf-8\") as src:\n",
    "        data = json.load(src)\n",
    "    result = []\n",
    "    for item in data:\n",
    "        if item.get(\"example\"):\n",
    "            example = FallacyExample(**item[\"example\"])\n",
    "        else:\n",
    "            example = None\n",
    "        fallacy = Fallacy(name=item[\"name\"], description=item[\"description\"], example=example)\n",
    "        result.append(fallacy)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "system_prompt = SystemMessagePromptTemplate.from_template_file(\n",
    "    os.path.join(FALLACIES_PROMPT_DIR, \"system.txt\"),\n",
    "    input_variables=[]\n",
    ")\n",
    "instruction_prompt = HumanMessagePromptTemplate.from_template_file(\n",
    "    os.path.join(FALLACIES_PROMPT_DIR, \"instruction.txt\"),\n",
    "    input_variables=[INTERMEDIATE_FALLACIES_STR,\n",
    "                     INTERMEDIATE_HISTORY_STR,\n",
    "                     INTERMEDIATE_LAST_AUTHOR,\n",
    "                     INPUT_CONTEXT,\n",
    "                     INPUT_QUERY]\n",
    ")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt, instruction_prompt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### History & fallacies to strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially message history / fallacy types presented as objects.\n",
    "\n",
    "We will need to stringify them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Cut too long message lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def stringify(row):\n",
    "    fallacies: List[Fallacy] = row[INPUT_FALLACIES]\n",
    "    history: List[Message] = row[INPUT_HISTORY] # TODO: cut\n",
    "    return {\n",
    "        INTERMEDIATE_FALLACIES_STR: \"\\n\\n\".join(map(str, fallacies)),\n",
    "        INTERMEDIATE_HISTORY_STR: \"\\n\\n\".join(map(str, history))\n",
    "    }\n",
    "\n",
    "async def astringify(row):\n",
    "    return stringify(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract last user name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explicitly tell model to search for issues in the last user messages - we interested in it, aren't we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_last_user(row):\n",
    "    history: List[Message] = row[INPUT_HISTORY]\n",
    "    assert len(history) > 0\n",
    "    return {\n",
    "        INTERMEDIATE_LAST_AUTHOR: history[-1].author\n",
    "    }\n",
    "\n",
    "async def aextract_last_user(row):\n",
    "    return extract_last_user(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short answer extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting a full answer from LLM we should extract the short form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def extract_answer_from_cot(row):\n",
    "    response: Union[AIMessage, AIMessageChunk] = row[OUTPUT_LLM_OUTPUT]\n",
    "    text: str = response.content\n",
    "    text = text.split(LLM_OUTPUT_MARKER)[-1]\n",
    "    text = text.split(\":\", maxsplit=1)[-1]\n",
    "    text = text.strip()\n",
    "    return {\n",
    "        OUTPUT_SHORT_ANSWER: text\n",
    "    }\n",
    "\n",
    "async def aextract_answer_from_cot(row):\n",
    "    return extract_answer_from_cot(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_fallacy_detection_chain(llm: BaseLLM) -> RunnableSequence:\n",
    "    stringify_transform = TransformChain(\n",
    "        input_variables=[INPUT_FALLACIES, INPUT_HISTORY],\n",
    "        output_variables=[INTERMEDIATE_FALLACIES_STR, INTERMEDIATE_HISTORY_STR],\n",
    "        transform=stringify,\n",
    "        atransform=astringify,\n",
    "    )\n",
    "    extract_last_user_transform = TransformChain(\n",
    "        input_variables=[INPUT_HISTORY],\n",
    "        output_variables=[INTERMEDIATE_LAST_AUTHOR],\n",
    "        transform=extract_last_user,\n",
    "        atransform=aextract_last_user,\n",
    "    )\n",
    "    extract_answer_transform = TransformChain(\n",
    "        input_variables=[OUTPUT_LLM_OUTPUT],\n",
    "        output_variables=[OUTPUT_SHORT_ANSWER],\n",
    "        transform=extract_answer_from_cot,\n",
    "        atransform=aextract_answer_from_cot,\n",
    "    )\n",
    "    return stringify_transform | \\\n",
    "        extract_last_user_transform | \\\n",
    "        chat_prompt | \\\n",
    "        llm | \\\n",
    "        extract_answer_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallacies_detection_chain = build_fallacy_detection_chain(\n",
    "    ChatOpenAI(\n",
    "        model_name=\"gpt-4-0613\",\n",
    "        openai_api_key=OPENAI_API_KEY,\n",
    "        streaming=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fallacies = read_fallacies(FALLACIES_FNAME)\n",
    "history = [\n",
    "    Message(\"Moonlight\", datetime.now(),\n",
    "            \"Soon we will finish with Ukraine\"),\n",
    "    Message(\"alex4321\", datetime.now(),\n",
    "            \"After six months of taking Bakhmut, did anything new happen?\\n\\n\" + \\\n",
    "                \"Well, so that there is a reason to suspect that it will happen soon, \" + \\\n",
    "                \"and not something regardless of the outcome - this will last for years.\"),\n",
    "    Message(\"Moonlight\", datetime.now(),\n",
    "            \"Time is a resource, we have plenty of it\")\n",
    "]\n",
    "context = \"Post about the war between Russia/Ukraine\"\n",
    "query = \"Moonlight's argument about time being a resource in war\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_output': AIMessageChunk(content='- Possible Fallacies in Moonlight\\'s messages:\\n  - Argumentum ad Ignorantiam (Appeal to Ignorance):\\n    Moonlight\\'s claim \"Time is a resource, we have plenty of it\" could potentially contain this fallacy as the speaker assumes that because no one has proven Russia doesn\\'t have enough time, it must have an abundance of time. However, this is not enough to confirm the presence of this fallacy definitely.\\n  - Argumentum ad Populum (Appeal to Popular Opinion):\\n    There is no indication of this fallacy since Moonlight doesn\\'t justify their argument by saying \\'everyone else believes it\\' or \\'it is popular.\\'\\n  - Argumentum ad Hominem (Personal Attack):\\n    This fallacy is not present as Moonlight doesn\\'t attack the character, motive, or attributes of the person making the argument.\\n  - Argumentum ad Misericordiam (Appeal to Pity):\\n    Moonlight\\'s argument does not use emotional appeals like sympathy, pity, or fear to persuade the audience. Hence, this fallacy is not identified.\\n  - Argumentum ad Baculum (Appeal to Force):\\n    Moonlight does not use threats or force to win the argument but rather presents an opinion. Therefore, this fallacy is not present.\\n  - Slippery Slope:\\n    Moonlight\\'s argument does not make a claim that one event will inevitably follow from another without adequate evidence. Hence, this fallacy is not present.\\n- Therefore, the answer is:\\n  - Identified Fallacy 1: Argumentum ad Ignorantiam (Appeal to Ignorance)\\n    Moonlight\\'s argument that \"Time is a resource, we have plenty of it\" could potentially represent an Appeal to Ignorance. The speaker assumes that since it hasn\\'t been proven that Russia is running out of time, they must have an abundance of it. However, this is not definitive proof of a fallacy, only a potential indication. More context or evidence would be needed to confirm this fallacy definitively.'),\n",
       " 'answer': '- Identified Fallacy 1: Argumentum ad Ignorantiam (Appeal to Ignorance)\\n    Moonlight\\'s argument that \"Time is a resource, we have plenty of it\" could potentially represent an Appeal to Ignorance. The speaker assumes that since it hasn\\'t been proven that Russia is running out of time, they must have an abundance of it. However, this is not definitive proof of a fallacy, only a potential indication. More context or evidence would be needed to confirm this fallacy definitively.'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fallacies_detection_chain.invoke({\n",
    "    \"fallacies\": fallacies,\n",
    "    \"history\": history,\n",
    "    \"context\": context,\n",
    "    \"query\": query,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_output': AIMessageChunk(content='- Possible Fallacies in Moonlight\\'s messages:\\n  - Argumentum ad Ignorantiam (Appeal to Ignorance):\\n    Moonlight\\'s assertion that \"Time is a resource, we have plenty of it\" could potentially be seen as an appeal to ignorance. The speaker does not provide specific evidence or logical reasons to support the claim that they have plenty of time, which could suggest an assumption that the argument is valid because it hasn\\'t been proven false.\\n  - Contradicting (Inconsistent) statements the same author:\\n    In the first message, Moonlight asserts confidently that \"Soon we will finish with Ukraine\", which suggests a prompt resolution to the conflict. However, in the last message, the speaker seems to contradict this initial statement by saying \"Time is a resource, we have plenty of it\", implying that the resolution of the conflict could take a significant amount of time.\\n\\nTherefore, the answer is:\\n  - Identified Fallacy 1: Argumentum ad Ignorantiam (Appeal to Ignorance)\\n    Moonlight\\'s claim that they have plenty of time because time is a resource could be seen as an appeal to ignorance. The speaker does not provide supporting evidence or logical reasons, which leaves the claim open to question.\\n  - Identified Fallacy 2: Contradicting (Inconsistent) statements the same author\\n    Moonlight\\'s messages contain possible contradictions, with the speaker initially suggesting a prompt resolution to the conflict and later implying that the resolution could take a significant amount of time.'),\n",
       " 'answer': \"- Identified Fallacy 1: Argumentum ad Ignorantiam (Appeal to Ignorance)\\n    Moonlight's claim that they have plenty of time because time is a resource could be seen as an appeal to ignorance. The speaker does not provide supporting evidence or logical reasons, which leaves the claim open to question.\\n  - Identified Fallacy 2: Contradicting (Inconsistent) statements the same author\\n    Moonlight's messages contain possible contradictions, with the speaker initially suggesting a prompt resolution to the conflict and later implying that the resolution could take a significant amount of time.\"}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await fallacies_detection_chain.ainvoke({\n",
    "    \"fallacies\": fallacies,\n",
    "    \"history\": history,\n",
    "    \"context\": context,\n",
    "    \"query\": query,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
