{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from typing import List, Tuple, Union, Dict\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, \\\n",
    "    ChatPromptTemplate\n",
    "from pino_inferior.models import aengine\n",
    "from pino_inferior.core import PROMPTS_DIR, OPENAI_API_KEY, VECTOR_DB, VECTOR_DB_PARAMS, MEMORY_PARAMS\n",
    "from pino_inferior.message import Message\n",
    "from pino_inferior.memory import Memory, INPUT_RETRIEVER_QUERY, OUTPUT_RETRIEVER_DOCUMENTS\n",
    "from langchain.schema.runnable import RunnableSequence\n",
    "from langchain.chains import TransformChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Milvus\n",
    "from dataclasses import dataclass\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import AIMessage\n",
    "from langchain.prompts.chat import ChatPromptValue\n",
    "from pino_inferior.fallacy import build_fallacy_detection_chain, read_fallacies, FALLACIES_FNAME, \\\n",
    "    INPUT_QUERY as INPUT_FALLACY_QUERY, OUTPUT_SHORT_ANSWER as OUTPUT_FALLACY_QUERY\n",
    "from datetime import datetime\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from enum import Enum\n",
    "from typing import Callable\n",
    "import asyncio\n",
    "import tiktoken\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "AGENT_PROMPTS_DIR = os.path.join(PROMPTS_DIR, \"roleplay_agent\")\n",
    "TOOLS_PROMPTS_DIR = os.path.join(AGENT_PROMPTS_DIR, \"tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _read_file(fname: str) -> str:\n",
    "    with open(fname, \"r\", encoding=\"utf-8\") as src:\n",
    "        return src.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "AGENT_INPUT_HISTORY = \"history\"\n",
    "AGENT_INPUT_TOOLS = \"tools\"\n",
    "AGENT_INPUT_CONTEXT = \"context\"\n",
    "AGENT_INPUT_FALLACIES = \"fallacies\"\n",
    "AGENT_INPUT_USERNAME = \"name\"\n",
    "AGENT_INPUT_CHARACTER = \"character\"\n",
    "AGENT_INPUT_GOAL = \"goals\"\n",
    "AGENT_INPUT_TIME = \"time\"\n",
    "AGENT_INPUT_STYLE_EXAMPLES = \"style_examples\"\n",
    "AGENT_INPUT_STYLE_DESCRIPTION = \"style_description\"\n",
    "\n",
    "AGENT_INTERMEDIATE_HISTORY_STR = \"input_str\"\n",
    "AGENT_INTERMEDIATE_TOOLS_STR = \"tools_str\"\n",
    "AGENT_INTERMEDIATE_TIME_STR = \"time_str\"\n",
    "AGENT_INTERMEDIATE_STYLE_EXAMPLES = \"style_examples_str\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class ToolDescription:\n",
    "    name: str\n",
    "    description: str\n",
    "    input_key: Union[str, None]\n",
    "    output_key: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "agent_system_prompt = SystemMessagePromptTemplate.from_template(_read_file(\n",
    "    os.path.join(AGENT_PROMPTS_DIR, \"system.txt\")\n",
    "))\n",
    "agent_instruction_prompt = HumanMessagePromptTemplate.from_template(_read_file(\n",
    "    os.path.join(AGENT_PROMPTS_DIR, \"instruction.txt\")\n",
    "))\n",
    "agent_llm_prompt = ChatPromptTemplate.from_messages([agent_system_prompt, agent_instruction_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_stringification_chain(\n",
    "        length_function: Callable[[str], int],\n",
    "        max_messages_length: int,\n",
    "        max_tools_length: int,\n",
    "        max_context_length: int,\n",
    "        max_username_length: int,\n",
    "        max_character_length: int,\n",
    "        max_goal_length: int,\n",
    "        max_style_examples_length: int,\n",
    "        max_style_description_length: int,\n",
    ") -> TransformChain:\n",
    "    def _stringify_messages(row):\n",
    "        messages: List[Message] = row[AGENT_INPUT_HISTORY]\n",
    "        while True:\n",
    "            messages_str = \"\\n\\n\".join(map(str, messages))\n",
    "            if length_function(messages_str) <= max_messages_length:\n",
    "                break\n",
    "            else:\n",
    "                messages = messages[1:]\n",
    "        assert len(messages) > 0, \\\n",
    "            f\"Only after cutting all the messages total length become less than {max_messages_length}\"\n",
    "        return messages_str\n",
    "    \n",
    "    def _stringify_tools(row):\n",
    "        tools: List[Tuple[ToolDescription, RunnableSequence]] = row[AGENT_INPUT_TOOLS]\n",
    "        tools_str = \"\\n\\n\".join([\n",
    "            f\"-- {tool.name}: {tool.description}\"\n",
    "            for tool, _ in tools\n",
    "        ])\n",
    "        assert length_function(tools_str) <= max_tools_length, \\\n",
    "            f\"Total size of tools description should be lower than {max_tools_length}\"\n",
    "        return tools_str\n",
    "        \n",
    "    def _stringify_time(row):\n",
    "        time: datetime = row[AGENT_INPUT_TIME]\n",
    "        return time.strftime(\"%d %b %Y %H:%M\")\n",
    "    \n",
    "    def _stringify_style_examples(row):\n",
    "        examples: List[str] = row[AGENT_INPUT_STYLE_EXAMPLES]\n",
    "        examples_str = \"\\n\\n\".join(examples)\n",
    "        assert length_function(examples_str) <= max_style_examples_length, \\\n",
    "            f\"Total size of style exampes should be lower than {max_style_examples_length}\"\n",
    "        return examples_str    \n",
    "\n",
    "    def agent_stringify(row):\n",
    "        messages_str = _stringify_messages(row)\n",
    "        tools_str = _stringify_tools(row)\n",
    "        time_str = _stringify_time(row)\n",
    "        style_examples_str = _stringify_style_examples(row)\n",
    "        \n",
    "        assert length_function(row[AGENT_INPUT_CONTEXT]) <= max_context_length\n",
    "        assert length_function(row[AGENT_INPUT_USERNAME]) <= max_username_length\n",
    "        assert length_function(row[AGENT_INPUT_CHARACTER]) <= max_character_length\n",
    "        assert length_function(row[AGENT_INPUT_GOAL]) <= max_goal_length\n",
    "        assert length_function(row[AGENT_INPUT_STYLE_DESCRIPTION]) <= max_style_description_length\n",
    "\n",
    "        return {\n",
    "            AGENT_INTERMEDIATE_HISTORY_STR: messages_str,\n",
    "            AGENT_INTERMEDIATE_TOOLS_STR: tools_str,\n",
    "            AGENT_INTERMEDIATE_TIME_STR: time_str,\n",
    "            AGENT_INTERMEDIATE_STYLE_EXAMPLES: style_examples_str\n",
    "        }\n",
    "    \n",
    "    async def aagent_stringify(row):\n",
    "        return agent_stringify(row)\n",
    "    \n",
    "    return TransformChain(\n",
    "        transform=agent_stringify,\n",
    "        atransform=aagent_stringify,\n",
    "        input_variables=[\n",
    "            AGENT_INPUT_HISTORY,\n",
    "            AGENT_INPUT_TOOLS,\n",
    "            AGENT_INPUT_CONTEXT,\n",
    "            AGENT_INPUT_USERNAME,\n",
    "            AGENT_INPUT_CHARACTER,\n",
    "            AGENT_INPUT_GOAL,\n",
    "            AGENT_INPUT_TIME,\n",
    "            AGENT_INPUT_STYLE_EXAMPLES,\n",
    "            AGENT_INPUT_STYLE_DESCRIPTION,\n",
    "        ],\n",
    "        output_variables=[\n",
    "            AGENT_INTERMEDIATE_HISTORY_STR,\n",
    "            AGENT_INTERMEDIATE_TOOLS_STR,\n",
    "            AGENT_INTERMEDIATE_TIME_STR,\n",
    "            AGENT_INTERMEDIATE_STYLE_EXAMPLES,\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def _arun_agent_llm(agent_prompt: ChatPromptTemplate,\n",
    "                          agent_llm: BaseChatModel,\n",
    "                          tool_call_stop_sequence: str,\n",
    "                          response_stop_sequence: str) -> str:\n",
    "    response = await agent_llm.ainvoke(\n",
    "        agent_prompt,\n",
    "        stop=[tool_call_stop_sequence, response_stop_sequence]\n",
    "    )\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _split_by_marker(text: str, open_marker: str, close_marker: str) -> List[str]:\n",
    "    blocks = text.split(open_marker)\n",
    "    before_last_open_marker = open_marker.join(blocks[:-1])\n",
    "    before_last_close_marker = blocks[-1].split(close_marker)[0]\n",
    "    return before_last_open_marker, before_last_close_marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class _NextAction:\n",
    "    TOOL = 1\n",
    "    RESPONSE = 2\n",
    "    UNKNOWN = 3\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class _ParsedResponse:\n",
    "    chain_of_thoughts: str\n",
    "    next_action: _NextAction\n",
    "    next_action_type: str\n",
    "    next_action_query: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LLMOutputParseError(ValueError):\n",
    "    def __init__(self, output: str):\n",
    "        super(LLMOutputParseError, self).__init__(\n",
    "            f\"LLM output parsing error: {output}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _parse_agent_output(response: str, tools: List[ToolDescription], response_marker: str) -> _ParsedResponse:\n",
    "    for tool in tools:\n",
    "        tool_open_marker = f\"[{tool.name}]\"\n",
    "        tool_close_marker = f\"[/{tool.name}]\"\n",
    "        if response.endswith(tool_close_marker):\n",
    "            chain_of_thoughts, query = _split_by_marker(response,\n",
    "                                                        tool_open_marker,\n",
    "                                                        tool_close_marker)\n",
    "            return _ParsedResponse(\n",
    "                chain_of_thoughts=chain_of_thoughts,\n",
    "                next_action=_NextAction.TOOL,\n",
    "                next_action_type=tool.name,\n",
    "                next_action_query=query\n",
    "            )\n",
    "    response_open_marker = f\"[{response_marker}]\"\n",
    "    response_close_marker = f\"[/{response_marker}]\"\n",
    "    if response_open_marker in response:\n",
    "        chain_of_thoughts, response = _split_by_marker(response,\n",
    "                                                       response_open_marker,\n",
    "                                                       response_close_marker)\n",
    "        return _ParsedResponse(\n",
    "            chain_of_thoughts=chain_of_thoughts,\n",
    "            next_action=_NextAction.RESPONSE,\n",
    "            next_action_type=\"\",\n",
    "            next_action_query=response,\n",
    "        )\n",
    "    raise LLMOutputParseError(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _extract_tool_representations(tools: List[Tuple[ToolDescription, RunnableSequence]]) -> Tuple[List[ToolDescription], Dict[str, Tuple[ToolDescription, RunnableSequence]]]:\n",
    "    tool_descriptions = [\n",
    "        description\n",
    "        for description, _ in tools\n",
    "    ]\n",
    "    tools_by_name = {\n",
    "        description.name: (description, tool)\n",
    "        for description, tool in tools\n",
    "    }\n",
    "    return tool_descriptions, tools_by_name\n",
    "\n",
    "\n",
    "def _process_tool(inputs: dict, response: _NextAction, tools_by_name: Dict[str, Tuple[ToolDescription, RunnableSequence]]) \\\n",
    "    -> Tuple[dict, RunnableSequence, str, str]:\n",
    "    assert response.next_action_type in tools_by_name\n",
    "    tool_description, tool_chain = tools_by_name[response.next_action_type]\n",
    "    tool_inputs = dict(inputs, **{tool_description.input_key: response.next_action_query})\n",
    "    return tool_inputs, tool_chain, tool_description.output_key, tool_description.name\n",
    "\n",
    "\n",
    "async def _process_tool_response(tool_name: str,\n",
    "                                 query: str,\n",
    "                                 output: str,\n",
    "                                 tool_call_stop_sequence: str,\n",
    "                                 tool_call_close_sequence: str,\n",
    "                                 tool_length_function: Callable[[str], int],\n",
    "                                 tool_cut_function: Callable[[str, int], str],\n",
    "                                 tool_query_max_length: int,\n",
    "                                 tool_response_max_length: int) \\\n",
    "                                    -> Tuple[str, None, bool]:\n",
    "    if tool_length_function(query) > tool_query_max_length:\n",
    "        query = tool_cut_function(query, tool_query_max_length)\n",
    "    if tool_length_function(output) > tool_response_max_length:\n",
    "        output = tool_cut_function(output, tool_response_max_length)\n",
    "    suffix = f\"[{tool_name}]{query}[/{tool_name}]\" + \\\n",
    "        f\"{tool_call_stop_sequence}\\n\" + \\\n",
    "        f\"```\\n{output}\\n```\\n\" + \\\n",
    "        f\"{tool_call_close_sequence}\"\n",
    "    final_response = None\n",
    "    continue_further = True\n",
    "    return suffix, final_response, continue_further\n",
    "\n",
    "\n",
    "async def _aprocess_agent_iteration_output(\n",
    "        inputs: dict,\n",
    "        llm_output: str,\n",
    "        tool_call_stop_sequence: str,\n",
    "        tool_call_close_sequence: str,\n",
    "        tools: List[Tuple[ToolDescription, RunnableSequence]],\n",
    "        tool_length_function: Callable[[str], int],\n",
    "        tool_cut_function: Callable[[str, int], str],\n",
    "        tool_query_max_length: int,\n",
    "        tool_response_max_length: int,\n",
    "        response_marker: str,\n",
    ") -> Tuple[AIMessage, Union[str, None], bool]:\n",
    "    tool_descriptions, tools_by_name = _extract_tool_representations(tools)\n",
    "    response = _parse_agent_output(llm_output, tool_descriptions, response_marker)\n",
    "    assert response.next_action in {_NextAction.TOOL, _NextAction.RESPONSE}\n",
    "    if response.next_action == _NextAction.TOOL:\n",
    "        tool_inputs, tool_chain, tool_output_key, tool_name = _process_tool(inputs, response, tools_by_name)\n",
    "        tool_output = (await tool_chain.ainvoke(tool_inputs))[tool_output_key]\n",
    "        suffix, final_response, continue_further = await _process_tool_response(\n",
    "            tool_name,\n",
    "            response.next_action_query,\n",
    "            tool_output,\n",
    "            tool_call_stop_sequence,\n",
    "            tool_call_close_sequence,\n",
    "            tool_length_function,\n",
    "            tool_cut_function,\n",
    "            tool_query_max_length,\n",
    "            tool_response_max_length,\n",
    "        )\n",
    "    elif response.next_action == _NextAction.RESPONSE:\n",
    "        suffix = f\"[{response_marker}]{response.next_action_query}[/{response_marker}]\"\n",
    "        final_response = response.next_action_query\n",
    "        continue_further = False\n",
    "    message = AIMessage(content=f\"{response.chain_of_thoughts}{suffix}\")\n",
    "    return message, final_response, continue_further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running one iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def _arun_agent_iteration(inputs: dict,\n",
    "                         agent_prompt: ChatPromptValue,\n",
    "                         agent_llm: BaseChatModel,\n",
    "                         tool_call_stop_sequence: str,\n",
    "                         tool_call_close_sequence: str,\n",
    "                         tools: List[Tuple[ToolDescription, RunnableSequence]],\n",
    "                         tool_length_function: Callable[[str], int],\n",
    "                         tool_cut_function: Callable[[str, int], str],\n",
    "                         tool_query_max_length: int,\n",
    "                         tool_response_max_length: int,\n",
    "                         response_marker: str) -> Tuple[AIMessage, Union[str, None], bool]:\n",
    "    llm_output = await _arun_agent_llm(\n",
    "        agent_prompt=agent_prompt,\n",
    "        agent_llm=agent_llm,\n",
    "        tool_call_stop_sequence=tool_call_stop_sequence,\n",
    "        response_stop_sequence=f\"[/{response_marker}]\",\n",
    "    )\n",
    "    return await _aprocess_agent_iteration_output(\n",
    "        inputs,\n",
    "        llm_output,\n",
    "        tool_call_stop_sequence,\n",
    "        tool_call_close_sequence,\n",
    "        tools,\n",
    "        tool_length_function,\n",
    "        tool_cut_function,\n",
    "        tool_query_max_length,\n",
    "        tool_response_max_length,\n",
    "        response_marker,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_agent(inputs: dict,\n",
    "              agent_input_preprocessor: RunnableSequence,\n",
    "              agent_llm: BaseChatModel,\n",
    "              tool_call_stop_sequence: str,\n",
    "              tool_call_close_sequence: str,\n",
    "              tools: List[Tuple[ToolDescription, RunnableSequence]],\n",
    "              tool_length_function: Callable[[str], int],\n",
    "              tool_cut_function: Callable[[str, int], str],\n",
    "              tool_query_max_length: int,\n",
    "              tool_response_max_length: int,\n",
    "              response_marker: str,\n",
    "              max_iteration_count: int,\n",
    "              max_token_count: int) -> str:\n",
    "    return asyncio.get_event_loop().run_until_complete(\n",
    "        arun_agent(\n",
    "            inputs,\n",
    "            agent_input_preprocessor,\n",
    "            agent_llm,\n",
    "            tool_call_stop_sequence,\n",
    "            tool_call_close_sequence,\n",
    "            tools,\n",
    "            tool_length_function,\n",
    "            tool_cut_function,\n",
    "            tool_query_max_length,\n",
    "            tool_response_max_length,\n",
    "            response_marker,\n",
    "            max_iteration_count,\n",
    "            max_token_count,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \n",
    "async def arun_agent(inputs: dict,\n",
    "                     agent_input_preprocessor: RunnableSequence,\n",
    "                     agent_llm: BaseChatModel,\n",
    "                     tool_call_stop_sequence: str,\n",
    "                     tool_call_close_sequence: str,\n",
    "                     tools: List[Tuple[ToolDescription, RunnableSequence]],\n",
    "                     tool_length_function: Callable[[str], int],\n",
    "                     tool_cut_function: Callable[[str, int], str],\n",
    "                     tool_query_max_length: int,\n",
    "                     tool_response_max_length: int,\n",
    "                     response_marker: str,\n",
    "                     max_iteration_count: int,\n",
    "                     max_token_count: int) -> str:\n",
    "    chat_inputs: ChatPromptValue = await agent_input_preprocessor.ainvoke(inputs)\n",
    "    ai_messages = []\n",
    "    for _ in range(max_iteration_count):\n",
    "        token_count_without_ai = agent_llm.get_num_tokens_from_messages(chat_inputs.messages)\n",
    "        assert token_count_without_ai <= max_token_count, \\\n",
    "            f\"Session length ({token_count_without_ai}) exceeds {max_iteration_count} limit\"\n",
    "\n",
    "        ai_message, final_response, continue_further = await _arun_agent_iteration(\n",
    "            inputs,\n",
    "            chat_inputs,\n",
    "            agent_llm,\n",
    "            tool_call_stop_sequence,\n",
    "            tool_call_close_sequence,\n",
    "            tools,\n",
    "            tool_length_function,\n",
    "            tool_cut_function,\n",
    "            tool_query_max_length,\n",
    "            tool_response_max_length,\n",
    "            response_marker\n",
    "        )\n",
    "        ai_messages.append(ai_message)\n",
    "        chat_inputs.messages.append(ai_message)\n",
    "\n",
    "        if continue_further:\n",
    "            token_count_with_ai = agent_llm.get_num_tokens_from_messages(chat_inputs.messages)\n",
    "            assert token_count_with_ai <= max_token_count, \\\n",
    "                f\"With latest AI message session length ({token_count_with_ai}) exceeds {max_token_count} while no response achieved\"\n",
    "            continue\n",
    "        assert final_response is not None, \"Did not got final response\"\n",
    "        return final_response\n",
    "    raise ValueError(f\"Did not got final LLM response after {max_iteration_count} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ToolDescription(name='memory', description='External memory tool. \\nYour training data is limited, but you can use it.\\nQuery should be exact since memory will only see it, not the surrounding context\\nReturns retrieved documents.\\nCall it this way: [memory]%Your query[/memory][call]', input_key='query', output_key='documents_text'),\n",
       " TransformChain(input_variables=['query'], output_variables=['documents'], transform_cb=<function Memory.build_retriever_chain.<locals>._retrieve_documents at 0x7f784bf571a0>, atransform_cb=<function Memory.build_retriever_chain.<locals>._aretrieve_documents at 0x7f7850529260>)\n",
       " | TransformChain(input_variables=['documents'], output_variables=['documents_text'], transform_cb=<function Memory.build_retriever_chain.<locals>._stringify_documents at 0x7f784bfb89a0>, atransform_cb=<function Memory.build_retriever_chain.<locals>._astringify_documents at 0x7f784bfb8a40>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_encoder = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model=\"text-embedding-ada-002\",\n",
    ")\n",
    "memory_container = Memory(\n",
    "    engine=aengine,\n",
    "    vector_db=VECTOR_DB(\n",
    "        embedding_function=sentence_encoder,\n",
    "        **VECTOR_DB_PARAMS\n",
    "    ),\n",
    "    **MEMORY_PARAMS\n",
    ")\n",
    "memory_tool = (\n",
    "    ToolDescription(\n",
    "        name=\"memory\",\n",
    "        description=_read_file(os.path.join(TOOLS_PROMPTS_DIR, \"memory.txt\")),\n",
    "        input_key=INPUT_RETRIEVER_QUERY,\n",
    "        output_key=OUTPUT_RETRIEVER_DOCUMENTS,\n",
    "    ),\n",
    "    memory_container.build_retriever_chain()\n",
    ")\n",
    "memory_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ToolDescription(name='fallacy', description='Fallacy search tool.\\nReturns the comment regards your opponent fallacies, if it sees any.\\nCall it this way: [fallacy] what do you want to check for fallacies [/fallacy][call]', input_key='query', output_key='answer'),\n",
       " TransformChain(input_variables=['fallacies', 'history'], output_variables=['fallacies_str', 'history_str'], transform_cb=<function stringify at 0x7f7850c29580>, atransform_cb=<function astringify at 0x7f7850c294e0>)\n",
       " | TransformChain(input_variables=['history'], output_variables=['last_message_author'], transform_cb=<function extract_last_user at 0x7f7850c29620>, atransform_cb=<function aextract_last_user at 0x7f7850c296c0>)\n",
       " | ChatPromptTemplate(input_variables=['context', 'fallacies_str', 'history_str', 'last_message_author', 'query'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"You are a logical fallacy search subsystem.\\nYou're going to help us debating different topics, so you need to find our opponents potential weak points.\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['fallacies_str', 'history_str', 'last_message_author', 'context', 'query'], template=\"Fallacies to find (do not use thes examples):\\n<Fallacies>\\n{fallacies_str}\\n</Fallacies>\\n\\nAdditional context:\\n<Context>\\n{context}\\n</Context>\\n\\nChat history:\\n<Chat>\\n{history_str}\\n</Chat>\\n\\nQuery:\\n<Query>\\n{query}\\n</Query>\\n\\nNow, review {last_message_author}'s messages, paying close attention to their last one. If query provided - pay attention to it.\\n\\nYour task is to identify any potential faults or fallacies.\\n\\nIf you identify any fallacies from the given list, provide a brief explanation. If none are found, specify that as well.\\n\\nFramework:\\n- Possible Fallacies in {last_message_author}'s messages:\\n  - Fallacy Name 1:\\n    Describe whether this fallacy could be present in the messages, and provide an explanation. Be concise.  \\n  - Fallacy Name M:\\n    As above, explore the potential presence of this fallacy and explain your reasoning.\\n- (Use this market as is) Therefore, the answer is:\\n  - Identified Fallacy 1:\\n    Give a short explanation of why you identified this as a fallacy.\\n  - Identified Fallacy K:\\n    Provide a brief summary of your findings.\"))])\n",
       " | ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-4-0613', openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', streaming=True)\n",
       " | TransformChain(input_variables=['llm_output'], output_variables=['answer'], transform_cb=<function extract_answer_from_cot at 0x7f7850c29760>, atransform_cb=<function aextract_answer_from_cot at 0x7f7850c29800>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fallacy_llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4-0613\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    streaming=True,\n",
    ")\n",
    "fallacy_tool = (\n",
    "    ToolDescription(\n",
    "        name=\"fallacy\",\n",
    "        description=_read_file(os.path.join(TOOLS_PROMPTS_DIR, \"fallacy.txt\")),\n",
    "        input_key=INPUT_FALLACY_QUERY,\n",
    "        output_key=OUTPUT_FALLACY_QUERY,\n",
    "    ),\n",
    "    build_fallacy_detection_chain(fallacy_llm)\n",
    ")\n",
    "fallacy_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformChain(input_variables=['history', 'tools', 'context', 'name', 'character', 'goals', 'time', 'style_examples', 'style_description'], output_variables=['input_str', 'tools_str', 'time_str', 'style_examples_str'], transform_cb=<function build_stringification_chain.<locals>.agent_stringify at 0x7f78500944a0>, atransform_cb=<function build_stringification_chain.<locals>.aagent_stringify at 0x7f78500947c0>)\n",
       "| ChatPromptTemplate(input_variables=['character', 'context', 'goals', 'input_str', 'name', 'style_description', 'style_examples_str', 'time_str', 'tools_str'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=\"Roleplay like you're a human, not an AI model.\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['character', 'context', 'goals', 'input_str', 'name', 'style_description', 'style_examples_str', 'time_str', 'tools_str'], template='## Character\\nIt\\'s {time_str} now.\\n\\nImagine you\\'re {name} - {character}\\n\\nYour goals are:\\n```\\n{goals}\\n```\\n\\n## Character text style\\nA required response style description:\\n```\\n{style_description}\\n```\\nA few examples of messages to make you mimic the style:\\n```\\n{style_examples_str}\\n```\\n\\n## Context\\n```\\n{context}\\n```\\n\\n## Instruction\\n1. Act naturally, not \"as an AI/human\". And be brief.\\n2. You are equipped with some external tools:\\n{tools_str}\\nFinish each tool call with [call] suffix.\\nSo tool-using messages will look like:\\n```\\n[tool name]tool query[/tool name][call]\\ntool response\\n[/call]\\n```\\n3. Continue the \"Conversation\" and your thoughts if any.\\n4. When you\\'re ready to get the final response - wrap it this way:\\n```\\n1. your thought step 1\\n...\\nN. your thought step N\\n[response]Your response based on the steps 1-N with a brief reasoning[/response][stop]\\n```\\nDon\\'t forget the `[stop]` marker.\\n4. Mimic the style (not context) from \"Character text style\".\\n5. Think step by step.\\n\\n## Conversation\\nThink about continuation for this dialogue using the described instruction\\n```\\n{input_str}\\n```\\nIf the language used in the conversation is not English - identify it and use the same language later.'))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-4-0613\")\n",
    "agent_stringify_transform = build_stringification_chain(\n",
    "    length_function=lambda text: len(encoding.encode(text)),\n",
    "    max_messages_length=2048,\n",
    "    max_tools_length=256,\n",
    "    max_context_length=512,\n",
    "    max_username_length=10,\n",
    "    max_character_length=256,\n",
    "    max_goal_length=256,\n",
    "    max_style_examples_length=512,\n",
    "    max_style_description_length=512,\n",
    ")\n",
    "agent_preprocessing_chain = agent_stringify_transform | agent_llm_prompt\n",
    "agent_preprocessing_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4-0613\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    streaming=True,\n",
    ")\n",
    "agent_llm_encoding = tiktoken.encoding_for_model(\"gpt-4-0613\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    AGENT_INPUT_TIME: datetime.now(),\n",
    "    AGENT_INPUT_CONTEXT: \"Пост о войне России и Украины\",\n",
    "    AGENT_INPUT_FALLACIES: read_fallacies(FALLACIES_FNAME),\n",
    "    AGENT_INPUT_HISTORY: [\n",
    "        Message(\"Moonlight\", datetime.now(),\n",
    "                \"Мы скоро закончим с Украиной.\"),\n",
    "        Message(\"alex4321\", datetime.now(),\n",
    "                \"А что, случился какой-то прогресс после 6 месяцев взятия Бахмута?\\n\\n\" + \\\n",
    "          \"Ну, чтобы подозревать что это произойдёт вскоре, \" + \\\n",
    "                    \"а не затянется на годы независимо от исхода.\"),\n",
    "        Message(\"Moonlight\", datetime.now(),\n",
    "                \"Время - ресурс, у нас его дохуя.\")\n",
    "    ],\n",
    "    AGENT_INPUT_TOOLS: [\n",
    "        fallacy_tool,\n",
    "        memory_tool,\n",
    "    ],\n",
    "    AGENT_INPUT_USERNAME: \"alex4321\",\n",
    "    AGENT_INPUT_CHARACTER: \"you are a programmer, 29 y.o. male\",\n",
    "    AGENT_INPUT_GOAL: \"Convince as much people as possible that the ongoing war between Russia and Ukraine is only bringing more problem, that it do not solve any issues, and moreover - break the impression that it is widely supported. To do so - express your opinion, and debunk opponents opinions.\",\n",
    "    AGENT_INPUT_STYLE_EXAMPLES: [\n",
    "        \"В вакууме, да :Yoba:. Оба же тут существовали всё время или имели таки достигнутое соглашение, чтобы было от чего отталкиваться как опорной точки :Yoba:\",\n",
    "        \"Не особо-то может. Не привлекая население в виде не 1% принудительного мармелада и пары процентов добровольного, а в виде процентов 10.\\nА то, чтобы привлечь большое количество - неплохо бы, чтобы они понимали, нахуя это им надо. А то так численность военкомов может начать неприемлемо быстро падать, а следом их желание работать.\\nА с этим у пропаганды проблема. Вот с чем у них нет проблем, так это с стимуляцией пассивности, но это обратно нужному (для названной вами задачи).\\nДа и опять же - ну вот убедишь ты в идее не какого-нибудь Стрелкова и клуб рассерженных долбоёбов, а большое число людей. Что делать, когда (не если, а когда) идея станет неактуальной? Показательной посадкой пары человек дело не закончится же.\\n\",\n",
    "        \"Точнее не так - смену она не устраивала.\\nОна просто выстрелила себе в ногу так, что потом что-то новое приходилось строить не апгрейдом предыдущей системы, а из кусков её трупа.\",\n",
    "    ],\n",
    "    AGENT_INPUT_STYLE_DESCRIPTION: \"- Non-formal style, using mainly Russian language \" + \\\n",
    "        \"(my English is a bit screwed up)\" + \\\n",
    "        \"\\n- Brief. Most time.\\n\" + \\\n",
    "        \"- Overuse memes sometimes.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Время - ресурс, да, но тут не только время тратим. Сколько людей уже погибло, сколько всего разрушено? Это тоже ресурсы, и их не восстановить. И вот ещё, ты говоришь о скором окончании всего этого, но где доказательства? У тебя есть какие-то конкретные данные или это просто твое желание? Не надо делать поспешных выводов, ведь война - это не просто игра в стратегию, это реальные люди и их жизни.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_agent(inputs=inputs,\n",
    "          agent_input_preprocessor=agent_preprocessing_chain,\n",
    "          agent_llm=agent_llm,\n",
    "          tool_call_stop_sequence=\"[call]\",\n",
    "          tool_call_close_sequence=\"[/call]\",\n",
    "          tools=[memory_tool, fallacy_tool],\n",
    "          tool_length_function=lambda text: len(agent_llm_encoding.encode(text)),\n",
    "          tool_cut_function=lambda text, max_length: agent_llm_encoding.decode(agent_llm_encoding.encode(text)[:max_length]),\n",
    "          tool_query_max_length=32,\n",
    "          tool_response_max_length=512,\n",
    "          response_marker=\"response\",\n",
    "          max_iteration_count=5,\n",
    "          max_token_count=8 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Так, стоп. Ты серьезно считаешь, что у нас дохуя времени? Это откуда такая уверенность? Да и разве время - это основной ресурс, который тратится в войне? Забываешь про людей, которые там на передовой, про экономику, которая страдает от санкций. \\n\\nИ самое главное, не все так рады этой войне, как ты думаешь. Не надо говорить за всех. '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await arun_agent(inputs=inputs,\n",
    "                 agent_input_preprocessor=agent_preprocessing_chain,\n",
    "                 agent_llm=agent_llm,\n",
    "                 tool_call_stop_sequence=\"[call]\",\n",
    "                 tool_call_close_sequence=\"[/call]\",\n",
    "                 tools=[memory_tool, fallacy_tool],\n",
    "                 tool_length_function=lambda text: len(agent_llm_encoding.encode(text)),\n",
    "                 tool_cut_function=lambda text, max_length: agent_llm_encoding.decode(agent_llm_encoding.encode(text)[:max_length]),\n",
    "                 tool_query_max_length=32,\n",
    "                 tool_response_max_length=512,\n",
    "                 response_marker=\"response\",\n",
    "                 max_iteration_count=5,\n",
    "                 max_token_count=8 * 1024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
