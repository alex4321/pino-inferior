{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from typing import List, Tuple, Union, Dict\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, \\\n",
    "    ChatPromptTemplate\n",
    "from pino_inferior.models import aengine\n",
    "from pino_inferior.core import PROMPTS_DIR, OPENAI_API_KEY, VECTOR_DB, VECTOR_DB_PARAMS, MEMORY_PARAMS\n",
    "from pino_inferior.message import Message\n",
    "from pino_inferior.memory import Memory, INPUT_RETRIEVER_QUERY, OUTPUT_RETRIEVER_DOCUMENTS\n",
    "from langchain.schema.runnable import RunnableSequence\n",
    "from langchain.chains import TransformChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Milvus\n",
    "from dataclasses import dataclass\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import AIMessage\n",
    "from langchain.prompts.chat import ChatPromptValue\n",
    "from pino_inferior.fallacy import build_fallacy_detection_chain, read_fallacies, FALLACIES_FNAME, \\\n",
    "    INPUT_QUERY as INPUT_FALLACY_QUERY, OUTPUT_SHORT_ANSWER as OUTPUT_FALLACY_QUERY\n",
    "from datetime import datetime\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from enum import Enum\n",
    "from typing import Callable\n",
    "import asyncio\n",
    "import tiktoken\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "AGENT_PROMPTS_DIR = os.path.join(PROMPTS_DIR, \"roleplay_agent\")\n",
    "TOOLS_PROMPTS_DIR = os.path.join(AGENT_PROMPTS_DIR, \"tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _read_file(fname: str) -> str:\n",
    "    with open(fname, \"r\", encoding=\"utf-8\") as src:\n",
    "        return src.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "AGENT_INPUT_HISTORY = \"history\"\n",
    "AGENT_INPUT_TOOLS = \"tools\"\n",
    "AGENT_INPUT_CONTEXT = \"context\"\n",
    "AGENT_INPUT_FALLACIES = \"fallacies\"\n",
    "AGENT_INPUT_USERNAME = \"name\"\n",
    "AGENT_INPUT_CHARACTER = \"character\"\n",
    "AGENT_INPUT_GOAL = \"goals\"\n",
    "AGENT_INPUT_TIME = \"time\"\n",
    "AGENT_INPUT_STYLE_EXAMPLES = \"style_examples\"\n",
    "AGENT_INPUT_STYLE_DESCRIPTION = \"style_description\"\n",
    "\n",
    "AGENT_INTERMEDIATE_HISTORY_STR = \"input_str\"\n",
    "AGENT_INTERMEDIATE_TOOLS_STR = \"tools_str\"\n",
    "AGENT_INTERMEDIATE_TIME_STR = \"time_str\"\n",
    "AGENT_INTERMEDIATE_STYLE_EXAMPLES = \"style_examples_str\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class ToolDescription:\n",
    "    name: str\n",
    "    description: str\n",
    "    input_key: Union[str, None]\n",
    "    output_key: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "agent_system_prompt = SystemMessagePromptTemplate.from_template(_read_file(\n",
    "    os.path.join(AGENT_PROMPTS_DIR, \"system.txt\")\n",
    "))\n",
    "agent_instruction_prompt = HumanMessagePromptTemplate.from_template(_read_file(\n",
    "    os.path.join(AGENT_PROMPTS_DIR, \"instruction.txt\")\n",
    "))\n",
    "agent_llm_prompt = ChatPromptTemplate.from_messages([agent_system_prompt, agent_instruction_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_stringification_chain(\n",
    "        length_function: Callable[[str], int],\n",
    "        max_messages_length: int,\n",
    "        max_tools_length: int,\n",
    "        max_context_length: int,\n",
    "        max_username_length: int,\n",
    "        max_character_length: int,\n",
    "        max_goal_length: int,\n",
    "        max_style_examples_length: int,\n",
    "        max_style_description_length: int,\n",
    ") -> TransformChain:\n",
    "    def _stringify_messages(row):\n",
    "        messages: List[Message] = row[AGENT_INPUT_HISTORY]\n",
    "        while True:\n",
    "            messages_str = \"\\n\\n\".join(map(str, messages))\n",
    "            if length_function(messages_str) <= max_messages_length:\n",
    "                break\n",
    "            else:\n",
    "                messages = messages[1:]\n",
    "        assert len(messages) > 0, \\\n",
    "            f\"Only after cutting all the messages total length become less than {max_messages_length}\"\n",
    "        return messages_str\n",
    "    \n",
    "    def _stringify_tools(row):\n",
    "        tools: List[Tuple[ToolDescription, RunnableSequence]] = row[AGENT_INPUT_TOOLS]\n",
    "        tools_str = \"\\n\\n\".join([\n",
    "            f\"-- {tool.name}: {tool.description}\"\n",
    "            for tool, _ in tools\n",
    "        ])\n",
    "        assert length_function(tools_str) <= max_tools_length, \\\n",
    "            f\"Total size of tools description should be lower than {max_tools_length}\"\n",
    "        return tools_str\n",
    "        \n",
    "    def _stringify_time(row):\n",
    "        time: datetime = row[AGENT_INPUT_TIME]\n",
    "        return time.strftime(\"%d %b %Y %H:%M\")\n",
    "    \n",
    "    def _stringify_style_examples(row):\n",
    "        examples: List[str] = row[AGENT_INPUT_STYLE_EXAMPLES]\n",
    "        examples_str = \"\\n\\n\".join(examples)\n",
    "        assert length_function(examples_str) <= max_style_examples_length, \\\n",
    "            f\"Total size of style exampes should be lower than {max_style_examples_length}\"\n",
    "        return examples_str    \n",
    "\n",
    "    def agent_stringify(row):\n",
    "        messages_str = _stringify_messages(row)\n",
    "        tools_str = _stringify_tools(row)\n",
    "        time_str = _stringify_time(row)\n",
    "        style_examples_str = _stringify_style_examples(row)\n",
    "        \n",
    "        assert length_function(row[AGENT_INPUT_CONTEXT]) <= max_context_length\n",
    "        assert length_function(row[AGENT_INPUT_USERNAME]) <= max_username_length\n",
    "        assert length_function(row[AGENT_INPUT_CHARACTER]) <= max_character_length\n",
    "        assert length_function(row[AGENT_INPUT_GOAL]) <= max_goal_length\n",
    "        assert length_function(row[AGENT_INPUT_STYLE_DESCRIPTION]) <= max_style_description_length\n",
    "\n",
    "        return {\n",
    "            AGENT_INTERMEDIATE_HISTORY_STR: messages_str,\n",
    "            AGENT_INTERMEDIATE_TOOLS_STR: tools_str,\n",
    "            AGENT_INTERMEDIATE_TIME_STR: time_str,\n",
    "            AGENT_INTERMEDIATE_STYLE_EXAMPLES: style_examples_str\n",
    "        }\n",
    "    \n",
    "    async def aagent_stringify(row):\n",
    "        return agent_stringify(row)\n",
    "    \n",
    "    return TransformChain(\n",
    "        transform=agent_stringify,\n",
    "        atransform=aagent_stringify,\n",
    "        input_variables=[\n",
    "            AGENT_INPUT_HISTORY,\n",
    "            AGENT_INPUT_TOOLS,\n",
    "            AGENT_INPUT_CONTEXT,\n",
    "            AGENT_INPUT_USERNAME,\n",
    "            AGENT_INPUT_CHARACTER,\n",
    "            AGENT_INPUT_GOAL,\n",
    "            AGENT_INPUT_TIME,\n",
    "            AGENT_INPUT_STYLE_EXAMPLES,\n",
    "            AGENT_INPUT_STYLE_DESCRIPTION,\n",
    "        ],\n",
    "        output_variables=[\n",
    "            AGENT_INTERMEDIATE_HISTORY_STR,\n",
    "            AGENT_INTERMEDIATE_TOOLS_STR,\n",
    "            AGENT_INTERMEDIATE_TIME_STR,\n",
    "            AGENT_INTERMEDIATE_STYLE_EXAMPLES,\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def _arun_agent_llm(agent_prompt: ChatPromptTemplate,\n",
    "                          agent_llm: BaseChatModel,\n",
    "                          tool_call_stop_sequence: str,\n",
    "                          response_stop_sequence: str) -> str:\n",
    "    response = await agent_llm.ainvoke(\n",
    "        agent_prompt,\n",
    "        stop=[tool_call_stop_sequence, response_stop_sequence]\n",
    "    )\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _split_by_marker(text: str, open_marker: str, close_marker: str) -> List[str]:\n",
    "    blocks = text.split(open_marker)\n",
    "    before_last_open_marker = open_marker.join(blocks[:-1])\n",
    "    before_last_close_marker = blocks[-1].split(close_marker)[0]\n",
    "    return before_last_open_marker, before_last_close_marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class _NextAction:\n",
    "    TOOL = 1\n",
    "    RESPONSE = 2\n",
    "    UNKNOWN = 3\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class _ParsedResponse:\n",
    "    chain_of_thoughts: str\n",
    "    next_action: _NextAction\n",
    "    next_action_type: str\n",
    "    next_action_query: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LLMOutputParseError(ValueError):\n",
    "    def __init__(self, output: str):\n",
    "        super(LLMOutputParseError, self).__init__(\n",
    "            f\"LLM output parsing error: {output}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _parse_agent_output(response: str, tools: List[ToolDescription], response_marker: str) -> _ParsedResponse:\n",
    "    for tool in tools:\n",
    "        tool_open_marker = f\"[{tool.name}]\"\n",
    "        tool_close_marker = f\"[/{tool.name}]\"\n",
    "        if response.endswith(tool_close_marker):\n",
    "            chain_of_thoughts, query = _split_by_marker(response,\n",
    "                                                        tool_open_marker,\n",
    "                                                        tool_close_marker)\n",
    "            return _ParsedResponse(\n",
    "                chain_of_thoughts=chain_of_thoughts,\n",
    "                next_action=_NextAction.TOOL,\n",
    "                next_action_type=tool.name,\n",
    "                next_action_query=query\n",
    "            )\n",
    "    response_open_marker = f\"[{response_marker}]\"\n",
    "    response_close_marker = f\"[/{response_marker}]\"\n",
    "    if response_open_marker in response:\n",
    "        chain_of_thoughts, response = _split_by_marker(response,\n",
    "                                                       response_open_marker,\n",
    "                                                       response_close_marker)\n",
    "        return _ParsedResponse(\n",
    "            chain_of_thoughts=chain_of_thoughts,\n",
    "            next_action=_NextAction.RESPONSE,\n",
    "            next_action_type=\"\",\n",
    "            next_action_query=response,\n",
    "        )\n",
    "    raise LLMOutputParseError(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _extract_tool_representations(tools: List[Tuple[ToolDescription, RunnableSequence]]) -> Tuple[List[ToolDescription], Dict[str, Tuple[ToolDescription, RunnableSequence]]]:\n",
    "    tool_descriptions = [\n",
    "        description\n",
    "        for description, _ in tools\n",
    "    ]\n",
    "    tools_by_name = {\n",
    "        description.name: (description, tool)\n",
    "        for description, tool in tools\n",
    "    }\n",
    "    return tool_descriptions, tools_by_name\n",
    "\n",
    "\n",
    "def _process_tool(inputs: dict, response: _NextAction, tools_by_name: Dict[str, Tuple[ToolDescription, RunnableSequence]]) \\\n",
    "    -> Tuple[dict, RunnableSequence, str, str]:\n",
    "    assert response.next_action_type in tools_by_name\n",
    "    tool_description, tool_chain = tools_by_name[response.next_action_type]\n",
    "    tool_inputs = dict(inputs, **{tool_description.input_key: response.next_action_query})\n",
    "    return tool_inputs, tool_chain, tool_description.output_key, tool_description.name\n",
    "\n",
    "\n",
    "async def _process_tool_response(tool_name: str,\n",
    "                                 query: str,\n",
    "                                 output: str,\n",
    "                                 tool_call_stop_sequence: str,\n",
    "                                 tool_call_close_sequence: str,\n",
    "                                 tool_length_function: Callable[[str], int],\n",
    "                                 tool_cut_function: Callable[[str, int], str],\n",
    "                                 tool_query_max_length: int,\n",
    "                                 tool_response_max_length: int) \\\n",
    "                                    -> Tuple[str, None, bool]:\n",
    "    if tool_length_function(query) > tool_query_max_length:\n",
    "        query = tool_cut_function(query, tool_query_max_length)\n",
    "    if tool_length_function(output) > tool_response_max_length:\n",
    "        output = tool_cut_function(output, tool_response_max_length)\n",
    "    suffix = f\"[{tool_name}]{query}[/{tool_name}]\" + \\\n",
    "        f\"{tool_call_stop_sequence}\\n\" + \\\n",
    "        f\"```\\n{output}\\n```\\n\" + \\\n",
    "        f\"{tool_call_close_sequence}\"\n",
    "    final_response = None\n",
    "    continue_further = True\n",
    "    return suffix, final_response, continue_further\n",
    "\n",
    "\n",
    "async def _aprocess_agent_iteration_output(\n",
    "        inputs: dict,\n",
    "        llm_output: str,\n",
    "        tool_call_stop_sequence: str,\n",
    "        tool_call_close_sequence: str,\n",
    "        tools: List[Tuple[ToolDescription, RunnableSequence]],\n",
    "        tool_length_function: Callable[[str], int],\n",
    "        tool_cut_function: Callable[[str, int], str],\n",
    "        tool_query_max_length: int,\n",
    "        tool_response_max_length: int,\n",
    "        response_marker: str,\n",
    ") -> Tuple[AIMessage, Union[str, None], bool]:\n",
    "    tool_descriptions, tools_by_name = _extract_tool_representations(tools)\n",
    "    response = _parse_agent_output(llm_output, tool_descriptions, response_marker)\n",
    "    assert response.next_action in {_NextAction.TOOL, _NextAction.RESPONSE}\n",
    "    if response.next_action == _NextAction.TOOL:\n",
    "        tool_inputs, tool_chain, tool_output_key, tool_name = _process_tool(inputs, response, tools_by_name)\n",
    "        tool_output = (await tool_chain.ainvoke(tool_inputs))[tool_output_key]\n",
    "        suffix, final_response, continue_further = await _process_tool_response(\n",
    "            tool_name,\n",
    "            response.next_action_query,\n",
    "            tool_output,\n",
    "            tool_call_stop_sequence,\n",
    "            tool_call_close_sequence,\n",
    "            tool_length_function,\n",
    "            tool_cut_function,\n",
    "            tool_query_max_length,\n",
    "            tool_response_max_length,\n",
    "        )\n",
    "    elif response.next_action == _NextAction.RESPONSE:\n",
    "        suffix = f\"[{response_marker}]{response.next_action_query}[/{response_marker}]\"\n",
    "        final_response = response.next_action_query\n",
    "        continue_further = False\n",
    "    message = AIMessage(content=f\"{response.chain_of_thoughts}{suffix}\")\n",
    "    return message, final_response, continue_further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running one iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def _arun_agent_iteration(inputs: dict,\n",
    "                         agent_prompt: ChatPromptValue,\n",
    "                         agent_llm: BaseChatModel,\n",
    "                         tool_call_stop_sequence: str,\n",
    "                         tool_call_close_sequence: str,\n",
    "                         tools: List[Tuple[ToolDescription, RunnableSequence]],\n",
    "                         tool_length_function: Callable[[str], int],\n",
    "                         tool_cut_function: Callable[[str, int], str],\n",
    "                         tool_query_max_length: int,\n",
    "                         tool_response_max_length: int,\n",
    "                         response_marker: str) -> Tuple[AIMessage, Union[str, None], bool]:\n",
    "    llm_output = await _arun_agent_llm(\n",
    "        agent_prompt=agent_prompt,\n",
    "        agent_llm=agent_llm,\n",
    "        tool_call_stop_sequence=tool_call_stop_sequence,\n",
    "        response_stop_sequence=f\"[/{response_marker}]\",\n",
    "    )\n",
    "    return await _aprocess_agent_iteration_output(\n",
    "        inputs,\n",
    "        llm_output,\n",
    "        tool_call_stop_sequence,\n",
    "        tool_call_close_sequence,\n",
    "        tools,\n",
    "        tool_length_function,\n",
    "        tool_cut_function,\n",
    "        tool_query_max_length,\n",
    "        tool_response_max_length,\n",
    "        response_marker,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def run_agent(inputs: dict,\n",
    "              agent_input_preprocessor: RunnableSequence,\n",
    "              agent_llm: BaseChatModel,\n",
    "              tool_call_stop_sequence: str,\n",
    "              tool_call_close_sequence: str,\n",
    "              tools: List[Tuple[ToolDescription, RunnableSequence]],\n",
    "              tool_length_function: Callable[[str], int],\n",
    "              tool_cut_function: Callable[[str, int], str],\n",
    "              tool_query_max_length: int,\n",
    "              tool_response_max_length: int,\n",
    "              response_marker: str,\n",
    "              max_iteration_count: int,\n",
    "              max_token_count: int) -> str:\n",
    "    return asyncio.get_event_loop().run_until_complete(\n",
    "        arun_agent(\n",
    "            inputs,\n",
    "            agent_input_preprocessor,\n",
    "            agent_llm,\n",
    "            tool_call_stop_sequence,\n",
    "            tool_call_close_sequence,\n",
    "            tools,\n",
    "            tool_length_function,\n",
    "            tool_cut_function,\n",
    "            tool_query_max_length,\n",
    "            tool_response_max_length,\n",
    "            response_marker,\n",
    "            max_iteration_count,\n",
    "            max_token_count,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \n",
    "async def arun_agent(inputs: dict,\n",
    "                     agent_input_preprocessor: RunnableSequence,\n",
    "                     agent_llm: BaseChatModel,\n",
    "                     tool_call_stop_sequence: str,\n",
    "                     tool_call_close_sequence: str,\n",
    "                     tools: List[Tuple[ToolDescription, RunnableSequence]],\n",
    "                     tool_length_function: Callable[[str], int],\n",
    "                     tool_cut_function: Callable[[str, int], str],\n",
    "                     tool_query_max_length: int,\n",
    "                     tool_response_max_length: int,\n",
    "                     response_marker: str,\n",
    "                     max_iteration_count: int,\n",
    "                     max_token_count: int) -> str:\n",
    "    chat_inputs: ChatPromptValue = await agent_input_preprocessor.ainvoke(inputs)\n",
    "    ai_messages = []\n",
    "    for _ in range(max_iteration_count):\n",
    "        token_count_without_ai = agent_llm.get_num_tokens_from_messages(chat_inputs.messages)\n",
    "        assert token_count_without_ai <= max_token_count, \\\n",
    "            f\"Session length ({token_count_without_ai}) exceeds {max_iteration_count} limit\"\n",
    "\n",
    "        ai_message, final_response, continue_further = await _arun_agent_iteration(\n",
    "            inputs,\n",
    "            chat_inputs,\n",
    "            agent_llm,\n",
    "            tool_call_stop_sequence,\n",
    "            tool_call_close_sequence,\n",
    "            tools,\n",
    "            tool_length_function,\n",
    "            tool_cut_function,\n",
    "            tool_query_max_length,\n",
    "            tool_response_max_length,\n",
    "            response_marker\n",
    "        )\n",
    "        ai_messages.append(ai_message)\n",
    "        chat_inputs.messages.append(ai_message)\n",
    "\n",
    "        if continue_further:\n",
    "            token_count_with_ai = agent_llm.get_num_tokens_from_messages(chat_inputs.messages)\n",
    "            assert token_count_with_ai <= max_token_count, \\\n",
    "                f\"With latest AI message session length ({token_count_with_ai}) exceeds {max_token_count} while no response achieved\"\n",
    "            continue\n",
    "        assert final_response is not None, \"Did not got final response\"\n",
    "        return final_response\n",
    "    raise ValueError(f\"Did not got final LLM response after {max_iteration_count} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@dataclass\n",
    "class LengthConfig:\n",
    "    cut_function: Callable[[str, int], str]\n",
    "    length_function: Callable[[str], int]\n",
    "    max_messages_length: int = 2048\n",
    "    max_tools_length: int = 256\n",
    "    max_context_length: int = 512\n",
    "    max_username_length: int = 10\n",
    "    max_character_length: int = 256\n",
    "    max_goal_length: int = 256\n",
    "    max_style_examples_length: int = 512\n",
    "    max_style_description_length: int = 512\n",
    "    max_tool_query_length: int = 32\n",
    "    max_tool_response_length: int = 512\n",
    "    max_session_length: int = 8 * 1024\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PromptMarkupConfig:\n",
    "    tool_call_stop_sequence: str = \"[call]\"\n",
    "    tool_call_close_sequence: str = \"[/call]\"\n",
    "    response_marker = \"response\"\n",
    "\n",
    "\n",
    "class RolePlayAgent:\n",
    "    def __init__(self,\n",
    "                 tools: List[Tuple[ToolDescription, RunnableSequence]],\n",
    "                 lengths: LengthConfig,\n",
    "                 prompt_markup: PromptMarkupConfig,\n",
    "                 llm: BaseChatModel,\n",
    "                 max_iter: int = 5) -> None:\n",
    "        self.tools = tools\n",
    "        self.lengths = lengths\n",
    "        self.prompt_markup = prompt_markup\n",
    "        self.llm = llm\n",
    "        self.preprocessing_chain = self._build_preprocessing_chain()\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def _build_preprocessing_chain(self) -> RunnableSequence:\n",
    "        agent_stringify_transform = build_stringification_chain(\n",
    "            length_function=self.lengths.length_function,\n",
    "            max_messages_length=self.lengths.max_messages_length,\n",
    "            max_tools_length=self.lengths.max_tools_length,\n",
    "            max_context_length=self.lengths.max_context_length,\n",
    "            max_username_length=self.lengths.max_username_length,\n",
    "            max_character_length=self.lengths.max_character_length,\n",
    "            max_goal_length=self.lengths.max_goal_length,\n",
    "            max_style_examples_length=self.lengths.max_style_examples_length,\n",
    "            max_style_description_length=self.lengths.max_style_description_length,\n",
    "        )\n",
    "        agent_preprocessing_chain = agent_stringify_transform | agent_llm_prompt\n",
    "        return agent_preprocessing_chain\n",
    "    \n",
    "    async def arun(self, inputs: dict) -> str:\n",
    "        return await arun_agent(\n",
    "            inputs=inputs,\n",
    "            agent_input_preprocessor=self.preprocessing_chain,\n",
    "            agent_llm=self.llm,\n",
    "            tool_call_stop_sequence=self.prompt_markup.tool_call_stop_sequence,\n",
    "            tool_call_close_sequence=self.prompt_markup.tool_call_close_sequence,\n",
    "            tools=self.tools,\n",
    "            tool_length_function=self.lengths.length_function,\n",
    "            tool_cut_function=self.lengths.cut_function,\n",
    "            tool_query_max_length=self.lengths.max_tool_query_length,\n",
    "            tool_response_max_length=self.lengths.max_tool_response_length,\n",
    "            response_marker=self.prompt_markup.response_marker,\n",
    "            max_iteration_count=self.max_iter,\n",
    "            max_token_count=self.lengths.max_session_length,\n",
    "        )\n",
    "    \n",
    "    def run(self, inputs: dict) -> str:\n",
    "        return run_agent(\n",
    "            inputs=inputs,\n",
    "            agent_input_preprocessor=self.preprocessing_chain,\n",
    "            agent_llm=self.llm,\n",
    "            tool_call_stop_sequence=self.prompt_markup.tool_call_stop_sequence,\n",
    "            tool_call_close_sequence=self.prompt_markup.tool_call_close_sequence,\n",
    "            tools=self.tools,\n",
    "            tool_length_function=self.lengths.length_function,\n",
    "            tool_cut_function=self.lengths.cut_function,\n",
    "            tool_query_max_length=self.lengths.max_tool_query_length,\n",
    "            tool_response_max_length=self.lengths.max_tool_response_length,\n",
    "            response_marker=self.prompt_markup.response_marker,\n",
    "            max_iteration_count=self.max_iter,\n",
    "            max_token_count=self.lengths.max_session_length,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ToolDescription(name='memory', description='External memory tool. \\nYour training data is limited, but you can use it.\\nQuery should be exact since memory will only see it, not the surrounding context.\\nSometimes memory might return non-relevant results (like sudden associations outside the discussed context).\\nReturns retrieved documents.\\nCall it this way: [memory]%Your query[/memory][call]', input_key='query', output_key='documents_text'),\n",
       " TransformChain(input_variables=['query'], output_variables=['documents'], transform_cb=<function Memory.build_retriever_chain.<locals>._retrieve_documents>, atransform_cb=<function Memory.build_retriever_chain.<locals>._aretrieve_documents>)\n",
       " | TransformChain(input_variables=['documents'], output_variables=['documents_text'], transform_cb=<function Memory.build_retriever_chain.<locals>._stringify_documents>, atransform_cb=<function Memory.build_retriever_chain.<locals>._astringify_documents>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_encoder = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model=\"text-embedding-ada-002\",\n",
    ")\n",
    "memory_container = Memory(\n",
    "    engine=aengine,\n",
    "    vector_db=VECTOR_DB(\n",
    "        embedding_function=sentence_encoder,\n",
    "        **VECTOR_DB_PARAMS\n",
    "    ),\n",
    "    **MEMORY_PARAMS\n",
    ")\n",
    "memory_tool = (\n",
    "    ToolDescription(\n",
    "        name=\"memory\",\n",
    "        description=_read_file(os.path.join(TOOLS_PROMPTS_DIR, \"memory.txt\")),\n",
    "        input_key=INPUT_RETRIEVER_QUERY,\n",
    "        output_key=OUTPUT_RETRIEVER_DOCUMENTS,\n",
    "    ),\n",
    "    memory_container.build_retriever_chain()\n",
    ")\n",
    "memory_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fallacy_llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4-0613\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    streaming=True,\n",
    ")\n",
    "fallacy_tool = (\n",
    "    ToolDescription(\n",
    "        name=\"fallacy\",\n",
    "        description=_read_file(os.path.join(TOOLS_PROMPTS_DIR, \"fallacy.txt\")),\n",
    "        input_key=INPUT_FALLACY_QUERY,\n",
    "        output_key=OUTPUT_FALLACY_QUERY,\n",
    "    ),\n",
    "    build_fallacy_detection_chain(fallacy_llm)\n",
    ")\n",
    "fallacy_tool.__class__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4-0613\",\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    streaming=True,\n",
    ")\n",
    "agent_llm_encoding = tiktoken.encoding_for_model(agent_llm.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RolePlayAgent(\n",
    "    tools=[memory_tool, fallacy_tool],\n",
    "    lengths=LengthConfig(\n",
    "        cut_function=lambda text, length: agent_llm_encoding.decode(agent_llm_encoding.encode(text)[:length]),\n",
    "        length_function=lambda text: len(agent_llm_encoding.encode(text)),\n",
    "    ),\n",
    "    prompt_markup=PromptMarkupConfig(),\n",
    "    llm=agent_llm,\n",
    "    max_iter=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    AGENT_INPUT_TIME: datetime.now(),\n",
    "    AGENT_INPUT_CONTEXT: \"Пост о войне России и Украины\",\n",
    "    AGENT_INPUT_FALLACIES: read_fallacies(FALLACIES_FNAME),\n",
    "    AGENT_INPUT_HISTORY: [\n",
    "        Message(\"Moonlight\", datetime.now(),\n",
    "                \"Мы скоро закончим с Украиной.\"),\n",
    "        Message(\"alex4321\", datetime.now(),\n",
    "                \"А что, случился какой-то прогресс после 6 месяцев взятия Бахмута?\\n\\n\" + \\\n",
    "          \"Ну, чтобы подозревать что это произойдёт вскоре, \" + \\\n",
    "                    \"а не затянется на годы независимо от исхода.\"),\n",
    "        Message(\"Moonlight\", datetime.now(),\n",
    "                \"Время - ресурс, у нас его дохуя.\")\n",
    "    ],\n",
    "    AGENT_INPUT_TOOLS: [\n",
    "        fallacy_tool,\n",
    "        memory_tool,\n",
    "    ],\n",
    "    AGENT_INPUT_USERNAME: \"alex4321\",\n",
    "    AGENT_INPUT_CHARACTER: \"you are a programmer, 29 y.o. male\",\n",
    "    AGENT_INPUT_GOAL: \"Convince as much people as possible that the ongoing war between Russia and Ukraine is only bringing more problem, that it do not solve any issues, and moreover - break the impression that it is widely supported. To do so - express your opinion, and debunk opponents opinions.\",\n",
    "    AGENT_INPUT_STYLE_EXAMPLES: [\n",
    "        \"В вакууме, да :Yoba:. Оба же тут существовали всё время или имели таки достигнутое соглашение, чтобы было от чего отталкиваться как опорной точки :Yoba:\",\n",
    "        \"Не особо-то может. Не привлекая население в виде не 1% принудительного мармелада и пары процентов добровольного, а в виде процентов 10.\\nА то, чтобы привлечь большое количество - неплохо бы, чтобы они понимали, нахуя это им надо. А то так численность военкомов может начать неприемлемо быстро падать, а следом их желание работать.\\nА с этим у пропаганды проблема. Вот с чем у них нет проблем, так это с стимуляцией пассивности, но это обратно нужному (для названной вами задачи).\\nДа и опять же - ну вот убедишь ты в идее не какого-нибудь Стрелкова и клуб рассерженных долбоёбов, а большое число людей. Что делать, когда (не если, а когда) идея станет неактуальной? Показательной посадкой пары человек дело не закончится же.\\n\",\n",
    "        \"Точнее не так - смену она не устраивала.\\nОна просто выстрелила себе в ногу так, что потом что-то новое приходилось строить не апгрейдом предыдущей системы, а из кусков её трупа.\",\n",
    "    ],\n",
    "    AGENT_INPUT_STYLE_DESCRIPTION: \"- Non-formal style, using mainly Russian language \" + \\\n",
    "        \"(my English is a bit screwed up)\" + \\\n",
    "        \"\\n- Brief. Most time.\\n\" + \\\n",
    "        \"- Overuse memes sometimes.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время - это, конечно, ресурс, но его наличие не гарантирует успех. Если не учесть такие факторы как стратегия, доступные ресурсы или международные отношения, то можно попасть в ловушку логической ошибки ad Ignorantiam. Так что не стоит полагаться только на время, думаю, этот конфликт требует более комплексного подхода.\n"
     ]
    }
   ],
   "source": [
    "print(agent.run(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ты говоришь, что у нас время - ресурс, и его дохуя. Но ведь это ж подход как в анекдоте: \"У нас бананы растут? - Нет. - А почему? - А нихуя. - Вот и я о том же\".  Это Argumentum ad Ignorantiam, братан. Без конкретики и доказательств это просто пустой звук.\n",
      "\n",
      "Ну и второе, война - это не только ресурсы в виде времени. Это и люди, которые тратят свои жизни, здоровье, семьи. И чем дольше война, тем больше такой цены приходится платить.\n",
      "\n",
      "А уж про то, что все закончится быстро - это уже вообще Hasty Generalization. Никто не знает, как все сложится, и говорить о скором окончании - это как стрелять в темную. \n",
      "\n",
      "Не думай, что я против России, нет, просто надо понимать, что война это не решение проблем, а их усугубление. \n"
     ]
    }
   ],
   "source": [
    "print(await agent.arun(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
