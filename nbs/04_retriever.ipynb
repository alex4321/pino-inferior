{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory retriever\n",
    "\n",
    "> This module contains a tool for RAG\n",
    "> Given an input query it should:\n",
    "> 1. find corresponding sentences\n",
    "> 2. since sentences always do not have enough info - retrieve their paragraphs from DB\n",
    "> 3. than join them into a text for LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import re\n",
    "from typing import List, Set, Dict, Callable, Tuple, Union\n",
    "import hashlib\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter, Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema.embeddings import Embeddings\n",
    "from langchain.schema.vectorstore import VectorStore\n",
    "from sqlalchemy.engine import Engine\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy.ext.asyncio import AsyncEngine, AsyncSession\n",
    "from sqlalchemy import select\n",
    "from sqlalchemy.exc import IntegrityError\n",
    "from pino_inferior.core import OPENAI_API_KEY, VECTOR_DB, VECTOR_DB_PARAMS, MEMORY_PARAMS\n",
    "from pino_inferior.models import aengine, ParagraphMemoryRecord\n",
    "from datetime import datetime\n",
    "from langchain.schema.runnable import RunnableSequence\n",
    "from langchain.chains import TransformChain\n",
    "import pandas as pd\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "TEXT_HASH_COLUMN = \"ParagraphHash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def md5string(text: str) -> str:\n",
    "    return hashlib.md5(text.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ParagraphSplitter:\n",
    "    def split_text(self, text: str) -> List[Document]:\n",
    "        return [\n",
    "            Document(page_content=item, metadata={TEXT_HASH_COLUMN: f\"{md5string(item)}\"})\n",
    "            for item in text.split(\"\\n\")\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class SentenceSplitter:\n",
    "    def __init__(self):\n",
    "        self.separators=[\"\\.\\s\", \"\\?\", \"\\!\"]\n",
    "    \n",
    "    def _split_rest_separators(self, text: str, separators: List[str]) -> List[Document]:\n",
    "        if len(separators) == 0:\n",
    "            return [Document(page_content=text, metadata={})]\n",
    "        current_separator = separators[0]\n",
    "        next_separators = separators[1:]\n",
    "        result = []\n",
    "        for item in re.split(current_separator, text):\n",
    "            result += self._split_rest_separators(item, next_separators)\n",
    "        return result\n",
    "\n",
    "    def split_text(self, text: str) -> List[Document]:\n",
    "        return self._split_rest_separators(text, self.separators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SequentialSplitter:\n",
    "    def __init__(self, splitters: list) -> None:\n",
    "        self.splitters = splitters\n",
    "\n",
    "    def _split_inner(self, text: str, rest_splitters: list) -> List[Document]:\n",
    "        if len(rest_splitters) == 0:\n",
    "            return [Document(page_content=text, metadata={})]\n",
    "        current_splitter = rest_splitters[0]\n",
    "        next_splitters = rest_splitters[1:]\n",
    "        result = []\n",
    "        for item in current_splitter.split_text(text):\n",
    "            if isinstance(item, str):\n",
    "                item = Document(page_content=item, metadata={})\n",
    "            metadata = item.metadata\n",
    "            for record in self._split_inner(item.page_content, next_splitters):\n",
    "                result.append(Document(\n",
    "                    page_content=record.page_content,\n",
    "                    metadata=dict(metadata, **record.metadata)\n",
    "                ))\n",
    "        return result\n",
    "\n",
    "    def split_text(self, text: str) -> List[Document]:\n",
    "        return self._split_inner(text, self.splitters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def _remove_known_paragraphs(session: AsyncSession, paragraphs: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Filter a paragraph to remember only new ones (paragraphs might be shared between documents)\n",
    "    \"\"\"\n",
    "    # Query by paragraph MD5\n",
    "    hashes = set()\n",
    "    for document in paragraphs:\n",
    "        assert TEXT_HASH_COLUMN in document.metadata\n",
    "        hash = document.metadata[TEXT_HASH_COLUMN]\n",
    "        hashes.add(hash)\n",
    "    sql_query = select(ParagraphMemoryRecord).filter(\n",
    "        ParagraphMemoryRecord.md5.in_(hashes)\n",
    "    )\n",
    "    sql_search = await session.scalars(sql_query)\n",
    "    # Build a set of md5-text pairs to filter only the non-known combinations\n",
    "    #  (can't be just md5 because of potential collisions)\n",
    "    blacklisted_pairs = set()\n",
    "    for record in sql_search:\n",
    "        blacklisted_pairs.add((record.md5, record.text))\n",
    "    # Filter itself\n",
    "    result = []\n",
    "    for document in paragraphs:\n",
    "        pair = (document.metadata[TEXT_HASH_COLUMN], document.page_content)\n",
    "        if pair not in blacklisted_pairs:\n",
    "            result.append(document)\n",
    "            # Add to blacklist in case of duplicated paragraphs within same insert query\n",
    "            blacklisted_pairs.add(pair)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def _store_paragraphs(documents_paragraphs: List[Document],\n",
    "                            sentence_splitter: SentenceSplitter,\n",
    "                            engine: AsyncEngine,\n",
    "                            vectorstore: VectorStore):\n",
    "    \"\"\"\n",
    "    Store given paragraphs\n",
    "    \"\"\"\n",
    "    def _prepare_sentence_documents(text: str, metadata: dict) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Split paragraph to a list of Documents representing individual sentences\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for item in sentence_splitter.split_text(document.page_content):\n",
    "            item_metadata = dict(**metadata)\n",
    "            item_text_full = \"\"\n",
    "            # Join metadata to the sentence\n",
    "            for key in metadata:\n",
    "                if key == TEXT_HASH_COLUMN:\n",
    "                    continue\n",
    "                item_text_full = f\"{item_text_full} : {metadata[key]}\"\n",
    "            # Join main text\n",
    "            item_text_full = f\"{item_text_full} : {item.page_content}\"\n",
    "            item_text_full = item_text_full.strip(\" :\")\n",
    "            # Store original text in the metadata\n",
    "            item_metadata[\"_text\"] = text\n",
    "            # Create document\n",
    "            result.append(Document(\n",
    "                page_content=item_text_full,\n",
    "                metadata=item_metadata,\n",
    "            ))\n",
    "        return result\n",
    "\n",
    "    sentences_to_add = []\n",
    "    assert isinstance(engine, AsyncEngine)\n",
    "    async with AsyncSession(engine) as session:\n",
    "        async with session.begin():\n",
    "            # Remove known paragraphs\n",
    "            documents_paragraphs = await _remove_known_paragraphs(session,\n",
    "                                                                  documents_paragraphs)\n",
    "            # Prepare paragraphs to store in the ORM and sentences to store in the vector DB\n",
    "            records = []\n",
    "            for document in documents_paragraphs:\n",
    "                assert TEXT_HASH_COLUMN in document.metadata\n",
    "                hash = document.metadata[TEXT_HASH_COLUMN]\n",
    "                records.append(\n",
    "                    ParagraphMemoryRecord(\n",
    "                        text=document.page_content,\n",
    "                        meta=document.metadata,\n",
    "                        md5=hash,\n",
    "                        created_at=datetime.now()\n",
    "                    )\n",
    "                )\n",
    "                sentences_to_add += _prepare_sentence_documents(\n",
    "                    document.page_content,\n",
    "                    metadata=document.metadata,\n",
    "                )\n",
    "            # Add paragraphs to the DB\n",
    "            session.add_all(records)\n",
    "        # Add sentences to the vector DB\n",
    "        if sentences_to_add:\n",
    "            vectorstore.add_documents(sentences_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def _get_paragraphs(\n",
    "    sentence_vector_search_document_scores: List[Tuple[Document, float]],\n",
    "    engine: AsyncEngine\n",
    ") -> List[Tuple[ParagraphMemoryRecord, float]]:\n",
    "    \"\"\"\n",
    "    Extract paragraphs from the database using sentences extracted by vector DB\n",
    "    \"\"\"\n",
    "    async with AsyncSession(engine, expire_on_commit=False) as session:\n",
    "        def _get_hashes_to_search(sentence_vector_search: List[Document]) -> Set[str]:\n",
    "            \"\"\"\n",
    "            Get paragraph hashes to use in `md5 IN ...` condition\n",
    "            \"\"\"\n",
    "            parapraph_hashes = set()\n",
    "            for item in sentence_vector_search:\n",
    "                parapraph_hashes.add(item.metadata[TEXT_HASH_COLUMN])\n",
    "            return parapraph_hashes\n",
    "        \n",
    "        async def _extract_paragraphs_by_hashes(hashes: Set[str]) -> Dict[str, List[ParagraphMemoryRecord]]:\n",
    "            \"\"\"\n",
    "            Extract `ParagraphMemoryRecord` using given hashes\n",
    "            \"\"\"\n",
    "            hash_to_paragraphs = {}\n",
    "            async with session.begin():\n",
    "                sql_query = select(ParagraphMemoryRecord).filter(\n",
    "                    ParagraphMemoryRecord.md5.in_(hashes)\n",
    "                )\n",
    "                sql_search = await session.scalars(sql_query)\n",
    "                for record in sql_search:\n",
    "                    if record.md5 not in hash_to_paragraphs:\n",
    "                        hash_to_paragraphs[record.md5] = []\n",
    "                    hash_to_paragraphs[record.md5].append(record)\n",
    "            return hash_to_paragraphs\n",
    "        \n",
    "        def _filter_paragraphs_by_metadata(sentence_vector_search: List[Document],\n",
    "                                           hash2paragraph: Dict[str, List[ParagraphMemoryRecord]]) \\\n",
    "                                           -> List[List[ParagraphMemoryRecord]]:\n",
    "            \"\"\"\n",
    "            In case of md5 collision to additional filtering by common keys metadata's values being the same\n",
    "            \"\"\"\n",
    "            records_meta_found = []\n",
    "            for item in sentence_vector_search:\n",
    "                item_meta = item.metadata\n",
    "                item_meta_keys = set(item_meta)\n",
    "                potential_findings = hash2paragraph[item.metadata[TEXT_HASH_COLUMN]]\n",
    "                found = []\n",
    "                for record in potential_findings:\n",
    "                    record_meta = record.meta\n",
    "                    common_meta_keys = item_meta_keys & set(record_meta)\n",
    "                    item_common_meta = {key: item_meta[key] for key in common_meta_keys}\n",
    "                    record_common_meta = {key: record_meta[key] for key in common_meta_keys}\n",
    "                    if item_common_meta == record_common_meta:\n",
    "                        found.append(record)\n",
    "                records_meta_found.append(found)\n",
    "            return records_meta_found\n",
    "        \n",
    "        def _filter_paragraphs_by_text(sentence_vector_search: List[Document],\n",
    "                                    paragraphs: List[List[ParagraphMemoryRecord]]) \\\n",
    "            -> List[ParagraphMemoryRecord]:\n",
    "            \"\"\"\n",
    "            Finally filter paragraphs be checking if sentence text is within them\n",
    "            \"\"\"\n",
    "            result = []\n",
    "            for item, item_records_meta_found in zip(sentence_vector_search, paragraphs):\n",
    "                found = None\n",
    "                for record in item_records_meta_found:\n",
    "                    if item.metadata[\"_text\"] in record.text:\n",
    "                        found = record\n",
    "                        break\n",
    "                assert found is not None\n",
    "                result.append(found)\n",
    "            return result\n",
    "        \n",
    "        sentence_vector_search = [\n",
    "            document\n",
    "            for document, _ in sentence_vector_search_document_scores\n",
    "        ]\n",
    "        sentence_vector_scores = [\n",
    "            score\n",
    "            for _, score in sentence_vector_search_document_scores\n",
    "        ]\n",
    "        hashes = _get_hashes_to_search(sentence_vector_search)\n",
    "        hash2paragraph = await _extract_paragraphs_by_hashes(hashes)\n",
    "        paragraphs_meta_cleaned = _filter_paragraphs_by_metadata(sentence_vector_search, hash2paragraph)\n",
    "        paragraphs_text_cleaned = _filter_paragraphs_by_text(sentence_vector_search, paragraphs_meta_cleaned)\n",
    "        return [\n",
    "            (item, score)\n",
    "            for item, score in zip(paragraphs_text_cleaned, sentence_vector_scores)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _unique_documents(documents: List[Tuple[ParagraphMemoryRecord, float]],\n",
    "                      score_processor: Callable[[ParagraphMemoryRecord, float], float]) -> \\\n",
    "    List[Tuple[ParagraphMemoryRecord, float]]:\n",
    "    \"\"\"\n",
    "    Given top_sentences_k pairs (paragraph - sentence score) return pairs (unique paragraph - best sentence score)\n",
    "    \"\"\"\n",
    "    documents_by_id = {\n",
    "        document.id: document\n",
    "        for document, _ in documents\n",
    "    }\n",
    "    records = []\n",
    "    for document, score in documents:\n",
    "        records.append({\"id\": document.id, \"score\": score, \"created_at\": document.created_at})\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    df[\"score_processed\"] = [\n",
    "        score_processor(documents_by_id[row[\"id\"]], row[\"score\"])\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "    if len(df) == 0:\n",
    "        return []\n",
    "    id2max_score = df.groupby(\"id\")[\"score_processed\"].max()\n",
    "    id2max_score = id2max_score.sort_values(ascending=False)\n",
    "    return [\n",
    "        (documents_by_id[doc_id], id2max_score[doc_id])\n",
    "        for doc_id in id2max_score.index\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "INPUT_RETRIEVER_QUERY = \"query\"\n",
    "INTERMEDIATE_RETRIEVER_DOCUMENTS = \"documents\"\n",
    "OUTPUT_RETRIEVER_DOCUMENTS = \"documents_text\"\n",
    "\n",
    "BLACKLISTED_META_PROPERTIES = {TEXT_HASH_COLUMN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _no_score_processing(document: ParagraphMemoryRecord, score: float) -> float:\n",
    "    return score\n",
    "\n",
    "\n",
    "ScoreProcessing = Union[None, Callable[[ParagraphMemoryRecord, float], float]]\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, engine: AsyncEngine, vector_db: VectorStore, \\\n",
    "                 lower_score_is_better: bool,\n",
    "                 score_processing: ScoreProcessing = None,\n",
    "                 top_k_sentences: int = 50,\n",
    "                 top_k_paragraphs: int = 5) -> None:\n",
    "        \"\"\"\n",
    "        Memory wrapper\n",
    "        :param engine: SQLAlchemy engine for SQL part of database\n",
    "        :param vector_db: Vector storage for sentences\n",
    "        :param lower_score_is_bettter: Different embedder & stores operates different metrics. Like cosine distance - or similarity?\n",
    "        :param top_k_sentences: How many sentences retrieved from the DB\n",
    "        :param top_k_paragraphs: How many paragraphs retrieved from the DB by found top_k_sentences sentences.\n",
    "        \"\"\"\n",
    "        self.engine = engine\n",
    "        self.vector_db = vector_db\n",
    "        self.lower_score_is_better = lower_score_is_better\n",
    "        self.sentence_splitter = SentenceSplitter()\n",
    "        self.score_processing = score_processing\n",
    "        self.top_k_sentences = top_k_sentences\n",
    "        self.top_k_paragraphs = top_k_paragraphs\n",
    "\n",
    "    def store(self, paragraphs: List[Document]) -> None:\n",
    "        \"\"\"\n",
    "        Store given paragraphs in the satabase\n",
    "        \"\"\"\n",
    "        asyncio.get_event_loop().run_until_complete(\n",
    "            self.astore(paragraphs)\n",
    "        )\n",
    "\n",
    "    async def astore(self, paragraphs: List[Document]) -> None:\n",
    "        \"\"\"\n",
    "        Store given paragraphs in the satabase. Async version\n",
    "        \"\"\"\n",
    "        await _store_paragraphs(\n",
    "            documents_paragraphs=paragraphs,\n",
    "            sentence_splitter=self.sentence_splitter,\n",
    "            engine=self.engine,\n",
    "            vectorstore=self.vector_db\n",
    "        )\n",
    "\n",
    "    def _process_scores(self, documents: List[Tuple[Document, float]]) -> List[Tuple[Document, float]]:\n",
    "        k = 1\n",
    "        if self.lower_score_is_better:\n",
    "            k = -1\n",
    "        return [\n",
    "            (doc, score * k)\n",
    "            for doc, score in documents\n",
    "        ]\n",
    "    \n",
    "    def _get_score_processing(self):\n",
    "        if self.score_processing:\n",
    "            score_processing = self.score_processing\n",
    "        else:\n",
    "            score_processing = _no_score_processing\n",
    "        return score_processing\n",
    "    \n",
    "    def retrieve(self, query: str) -> List[Tuple[ParagraphMemoryRecord, float]]:\n",
    "        \"\"\"\n",
    "        Retrive query-relevant paragraphs from the database\n",
    "        :param query: Search query\n",
    "        :returns: Search result\n",
    "        \"\"\"\n",
    "        return asyncio.get_event_loop().run_until_complete(\n",
    "            self.aretrieve(query)\n",
    "        )\n",
    "    \n",
    "    async def aretrieve(self, query: str) -> List[Tuple[ParagraphMemoryRecord, float]]:\n",
    "        \"\"\"\n",
    "        Retrive query-relevant paragraphs from the database. Async version.\n",
    "        :param query: Search query\n",
    "        :returns: Search result\n",
    "        \"\"\"\n",
    "        score_processing = self._get_score_processing()\n",
    "        # TODO: Add proper async calls to Milvus\n",
    "        sentence_similarity_search = await asyncio.to_thread(\n",
    "            self.vector_db.similarity_search_with_score,\n",
    "            query,\n",
    "            k=self.top_k_sentences\n",
    "        )\n",
    "        sentence_similarity_search = self._process_scores(sentence_similarity_search)\n",
    "        \n",
    "        document_extraction = await _get_paragraphs(\n",
    "            sentence_similarity_search,\n",
    "            self.engine\n",
    "        )\n",
    "        documents = _unique_documents(document_extraction, score_processing)\n",
    "        documents = documents[:self.top_k_paragraphs]\n",
    "        return documents\n",
    "    \n",
    "    def build_retriever_chain(self) -> RunnableSequence:\n",
    "        \"\"\"\n",
    "        Build langchain chain from retrieving\n",
    "        :returns: LangChain chain consuming `{INPUT_RETRIEVER_QUERY: string_query}` \n",
    "            and returning `{OUTPUT_RETRIEVER_DOCUMENTS: found_paragraphs_text}`\n",
    "        \"\"\"\n",
    "        def _retrieve_documents(row):\n",
    "            return {\n",
    "                INTERMEDIATE_RETRIEVER_DOCUMENTS: self.retrieve(row[INPUT_RETRIEVER_QUERY])\n",
    "            }\n",
    "        \n",
    "        async def _aretrieve_documents(row):\n",
    "            return {\n",
    "                INTERMEDIATE_RETRIEVER_DOCUMENTS: await self.aretrieve(row[INPUT_RETRIEVER_QUERY])\n",
    "            }\n",
    "        \n",
    "        def _stringify_documents(row):\n",
    "            documents: List[Tuple[ParagraphMemoryRecord, float]] = row[INTERMEDIATE_RETRIEVER_DOCUMENTS]\n",
    "            records = []\n",
    "            for doc, _ in documents:\n",
    "                record = {}\n",
    "                record[\"text\"] = doc.text\n",
    "                record[\"meta\"] = \"\\n\\n\".join([\n",
    "                    f\"# {property_name} : {value}\"\n",
    "                    for property_name, value in doc.meta.items()\n",
    "                    if property_name not in BLACKLISTED_META_PROPERTIES\n",
    "                ])\n",
    "                records.append(record)\n",
    "            df = pd.DataFrame.from_records(records)\n",
    "            joined_paragraphs = []\n",
    "            if len(df) > 0:\n",
    "                for meta_text, sub_df in df.groupby(\"meta\"):\n",
    "                    paragraphs_joined_text = \"\\n\\n\".join(sub_df[\"text\"])\n",
    "                    meta_joined_text = f\"{meta_text}\\n\\n{paragraphs_joined_text}\"\n",
    "                    joined_paragraphs.append(meta_joined_text)\n",
    "            joined_text = \"\\n\\n\".join(joined_paragraphs)\n",
    "            return {\n",
    "                OUTPUT_RETRIEVER_DOCUMENTS: joined_text,\n",
    "            }\n",
    "        \n",
    "        async def _astringify_documents(row):\n",
    "            return _stringify_documents(row)\n",
    "        \n",
    "        retriever = TransformChain(\n",
    "            transform=_retrieve_documents,\n",
    "            atransform=_aretrieve_documents,\n",
    "            input_variables=[INPUT_RETRIEVER_QUERY],\n",
    "            output_variables=[INTERMEDIATE_RETRIEVER_DOCUMENTS],\n",
    "        )\n",
    "        stringifier = TransformChain(\n",
    "            transform=_stringify_documents,\n",
    "            atransform=_astringify_documents,\n",
    "            input_variables=[INTERMEDIATE_RETRIEVER_DOCUMENTS],\n",
    "            output_variables=[OUTPUT_RETRIEVER_DOCUMENTS]\n",
    "        )\n",
    "\n",
    "        return retriever | stringifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header1\"),\n",
    "    (\"##\", \"Header2\"),\n",
    "    (\"###\", \"Header3\"),\n",
    "]\n",
    "paragraph_splitter = SequentialSplitter(\n",
    "    [\n",
    "        MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on),\n",
    "        ParagraphSplitter(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = Memory(\n",
    "    engine=aengine,\n",
    "    vector_db=VECTOR_DB(\n",
    "        embedding_function=OpenAIEmbeddings(\n",
    "            openai_api_key=OPENAI_API_KEY,\n",
    "            model=\"text-embedding-ada-002\",\n",
    "        ),\n",
    "        **VECTOR_DB_PARAMS\n",
    "    ),\n",
    "    **MEMORY_PARAMS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = \"\"\"\n",
    "# Intro (Markdown)\n",
    "\n",
    "## History \n",
    "\n",
    " Markdown is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9] \n",
    "\n",
    " Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files. \n",
    "\n",
    " ## Rise and divergence \n",
    "\n",
    " As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks. \n",
    "\n",
    " #### Standardization \n",
    "\n",
    " From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort. \n",
    "\n",
    " ## Implementations \n",
    "\n",
    " Implementations of Markdown are available for over a dozen programming languages.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = paragraph_splitter.split_text(md)\n",
    "memory.store(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = paragraph_splitter.split_text(md)\n",
    "await memory.astore(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Intro (Markdown) : History : Markdown is a lightweight markup language for creating formatted text using a plain-text editor', metadata={'Header1': 'Intro (Markdown)', 'Header2': 'History', 'ParagraphHash': 'ad5266c9513a3189e91da32213ff39f0', '_text': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]  '}),\n",
       "  0.10909271700683265),\n",
       " (Document(page_content='Intro (Markdown) : History : Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.', metadata={'Header1': 'Intro (Markdown)', 'Header2': 'History', 'ParagraphHash': '517e576f0199fcb5aa3c445c068c2798', '_text': 'Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.'}),\n",
       "  0.12011604891440819),\n",
       " (Document(page_content='Intro (Markdown) : Implementations : Implementations of Markdown are available for over a dozen programming languages.', metadata={'Header1': 'Intro (Markdown)', 'Header2': 'Implementations', 'ParagraphHash': '537a1381db0f7f4e4faa6eca1803cbab', '_text': 'Implementations of Markdown are available for over a dozen programming languages.'}),\n",
       "  0.13282759799128296),\n",
       " (Document(page_content='Intro (Markdown) : History : John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]', metadata={'Header1': 'Intro (Markdown)', 'Header2': 'History', 'ParagraphHash': 'ad5266c9513a3189e91da32213ff39f0', '_text': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]  '}),\n",
       "  0.13911490794669268)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.vector_db.similarity_search_with_score(\n",
    "    \"What is Markdown?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Intro (Markdown) : History : Markdown is a lightweight markup language for creating formatted text using a plain-text editor', metadata={'Header1': 'Intro (Markdown)', 'Header2': 'History', 'ParagraphHash': 'ad5266c9513a3189e91da32213ff39f0', '_text': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]  '}),\n",
       "  0.8909156552858631),\n",
       " (Document(page_content='Intro (Markdown) : History : Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.', metadata={'Header1': 'Intro (Markdown)', 'Header2': 'History', 'ParagraphHash': '517e576f0199fcb5aa3c445c068c2798', '_text': 'Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.'}),\n",
       "  0.8799175855889725),\n",
       " (Document(page_content='Intro (Markdown) : Implementations : Implementations of Markdown are available for over a dozen programming languages.', metadata={'Header1': 'Intro (Markdown)', 'Header2': 'Implementations', 'ParagraphHash': '537a1381db0f7f4e4faa6eca1803cbab', '_text': 'Implementations of Markdown are available for over a dozen programming languages.'}),\n",
       "  0.8672459879791194),\n",
       " (Document(page_content='Intro (Markdown) : History : John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]', metadata={'Header1': 'Intro (Markdown)', 'Header2': 'History', 'ParagraphHash': 'ad5266c9513a3189e91da32213ff39f0', '_text': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]  '}),\n",
       "  0.8609110543687092)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.vector_db.similarity_search_with_relevance_scores(\n",
    "    \"What is Markdown?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.11598283623618355 {'Header1': 'Intro (Markdown)', 'Header2': 'History', 'ParagraphHash': 'ad5266c9513a3189e91da32213ff39f0'} Markdown is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]  \n",
      "-0.12985392763535542 {'Header1': 'Intro (Markdown)', 'Header2': 'History', 'ParagraphHash': '517e576f0199fcb5aa3c445c068c2798'} Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.\n",
      "-0.1433829039051704 {'Header1': 'Intro (Markdown)', 'Header2': 'Implementations', 'ParagraphHash': '537a1381db0f7f4e4faa6eca1803cbab'} Implementations of Markdown are available for over a dozen programming languages.\n",
      "-0.16107064626681744 {'Header1': 'Intro (Markdown)', 'Header2': 'Rise and divergence', 'ParagraphHash': '214fcbad683380c0e212dc177fc57a1a'} As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.  \n",
      "-0.19324567371083157 {'Header1': 'Intro (Markdown)', 'Header2': 'Rise and divergence', 'ParagraphHash': 'c6ab5fb2f5a7ab8c9b1659a5c221ca17'} From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.\n"
     ]
    }
   ],
   "source": [
    "for doc, score in memory.retrieve(\"What is Markdown\"):\n",
    "    print(score, doc.meta, doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.11609687857649353 {'Header1': 'Intro (Markdown)', 'Header2': 'History', 'ParagraphHash': 'ad5266c9513a3189e91da32213ff39f0'} Markdown is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]  \n",
      "-0.12995420788013445 {'Header1': 'Intro (Markdown)', 'Header2': 'History', 'ParagraphHash': '517e576f0199fcb5aa3c445c068c2798'} Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.\n",
      "-0.14352883724322718 {'Header1': 'Intro (Markdown)', 'Header2': 'Implementations', 'ParagraphHash': '537a1381db0f7f4e4faa6eca1803cbab'} Implementations of Markdown are available for over a dozen programming languages.\n",
      "-0.16112913093411518 {'Header1': 'Intro (Markdown)', 'Header2': 'Rise and divergence', 'ParagraphHash': '214fcbad683380c0e212dc177fc57a1a'} As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.  \n",
      "-0.19334200538563628 {'Header1': 'Intro (Markdown)', 'Header2': 'Rise and divergence', 'ParagraphHash': 'c6ab5fb2f5a7ab8c9b1659a5c221ca17'} From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.\n"
     ]
    }
   ],
   "source": [
    "for doc, score in (await memory.aretrieve(\"What is Markdown\")):\n",
    "    print(score, doc.meta, doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Header1 : Intro (Markdown)\n",
      "\n",
      "# Header2 : History\n",
      "\n",
      "Markdown is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]  \n",
      "\n",
      "Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.\n",
      "\n",
      "# Header1 : Intro (Markdown)\n",
      "\n",
      "# Header2 : Implementations\n",
      "\n",
      "Implementations of Markdown are available for over a dozen programming languages.\n",
      "\n",
      "# Header1 : Intro (Markdown)\n",
      "\n",
      "# Header2 : Rise and divergence\n",
      "\n",
      "As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.  \n",
      "\n",
      "From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.\n"
     ]
    }
   ],
   "source": [
    "retrieved_chain = memory.build_retriever_chain()\n",
    "\n",
    "print(retrieved_chain.invoke({INPUT_RETRIEVER_QUERY: \"What is Markdown\"})[OUTPUT_RETRIEVER_DOCUMENTS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Header1 : Intro (Markdown)\n",
      "\n",
      "# Header2 : History\n",
      "\n",
      "Markdown is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]  \n",
      "\n",
      "Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.\n",
      "\n",
      "# Header1 : Intro (Markdown)\n",
      "\n",
      "# Header2 : Implementations\n",
      "\n",
      "Implementations of Markdown are available for over a dozen programming languages.\n",
      "\n",
      "# Header1 : В учебник Мединского по истории внесут правки; ранее абзац о сталинской депортации раскритиковали в Чечне\n",
      "\n",
      "По словам ректора МГИМО, происходит «естественный процесс», так как «появляются новые данные, оценки, поэтому правки в учебник будут вноситься и сейчас, и в следующем году, и через год».  \n",
      "\n",
      "В телеграм-канале председателя парламента Чечни Магомеда Даудова в сентябре появился пост, в котором говорилось, что тираж учебника в регионе изъяли, но позднее слова об этом были удалены из публикации. Даудов отмечал, что занимается учебником по поручению Рамзана Кадырова. Вероятно, он, как и представители российского конгресса народов Кавказа, остался недоволен фразой из параграфа, посвященного тылу СССР во время Второй мировой войны.  \n"
     ]
    }
   ],
   "source": [
    "print(retrieved_chain.invoke({INPUT_RETRIEVER_QUERY: \"Что такое Markdown\"})[OUTPUT_RETRIEVER_DOCUMENTS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Header1 : Intro (Markdown)\n",
      "\n",
      "# Header2 : History\n",
      "\n",
      "Markdown is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]  \n",
      "\n",
      "Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.\n",
      "\n",
      "# Header1 : Intro (Markdown)\n",
      "\n",
      "# Header2 : Implementations\n",
      "\n",
      "Implementations of Markdown are available for over a dozen programming languages.\n",
      "\n",
      "# Header1 : Intro (Markdown)\n",
      "\n",
      "# Header2 : Rise and divergence\n",
      "\n",
      "As Markdown popularity grew rapidly, many Markdown implementations appeared, driven mostly by the need for additional features such as tables, footnotes, definition lists,[note 1] and Markdown inside HTML blocks.  \n",
      "\n",
      "From 2012, a group of people, including Jeff Atwood and John MacFarlane, launched what Atwood characterised as a standardisation effort.\n"
     ]
    }
   ],
   "source": [
    "print( (await retrieved_chain.ainvoke({INPUT_RETRIEVER_QUERY: \"What is Markdown\"}))[OUTPUT_RETRIEVER_DOCUMENTS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Header1 : Intro (Markdown)\n",
      "\n",
      "# Header2 : History\n",
      "\n",
      "Markdown is a lightweight markup language for creating formatted text using a plain-text editor. John Gruber created Markdown in 2004 as a markup language that is appealing to human readers in its source code form.[9]  \n",
      "\n",
      "Markdown is widely used in blogging, instant messaging, online forums, collaborative software, documentation pages, and readme files.\n",
      "\n",
      "# Header1 : Intro (Markdown)\n",
      "\n",
      "# Header2 : Implementations\n",
      "\n",
      "Implementations of Markdown are available for over a dozen programming languages.\n",
      "\n",
      "# Header1 : В учебник Мединского по истории внесут правки; ранее абзац о сталинской депортации раскритиковали в Чечне\n",
      "\n",
      "По словам ректора МГИМО, происходит «естественный процесс», так как «появляются новые данные, оценки, поэтому правки в учебник будут вноситься и сейчас, и в следующем году, и через год».  \n",
      "\n",
      "В телеграм-канале председателя парламента Чечни Магомеда Даудова в сентябре появился пост, в котором говорилось, что тираж учебника в регионе изъяли, но позднее слова об этом были удалены из публикации. Даудов отмечал, что занимается учебником по поручению Рамзана Кадырова. Вероятно, он, как и представители российского конгресса народов Кавказа, остался недоволен фразой из параграфа, посвященного тылу СССР во время Второй мировой войны.  \n"
     ]
    }
   ],
   "source": [
    "print( (await retrieved_chain.ainvoke({INPUT_RETRIEVER_QUERY: \"Что такое Markdown\"}))[OUTPUT_RETRIEVER_DOCUMENTS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
